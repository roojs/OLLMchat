namespace OLLMchat.Ollama
{
	public class ChatCall : BaseCall, MessageInterface
	{
		// Read-only getters that read from client (with fake setters for serialization)
		public string model { 
			get { return this.client.model; }
			set { } // Fake setter for serialization
		}
		
		public bool stream { 
			get { return this.client.stream; }
			set { } // Fake setter for serialization
		}
		
		public string? format { 
			get { return this.client.format; }
			set { } // Fake setter for serialization
		}
		
		public Json.Object? options { 
			get { return this.client.options; }
			set { } // Fake setter for serialization
		}
		
		public bool think { 
			get { return this.client.think; }
			set { } // Fake setter for serialization
		}
		
		public string? keep_alive { 
			get { return this.client.keep_alive; }
			set { } // Fake setter for serialization
		}
		
		public Gee.HashMap<string, Tool>? tools { 
			get { return this.client.tools; }
			set { } // Fake setter for serialization
		}
		public ChatResponse? streaming_response { get; set; default = null; }
		public string system_content { get; set; default = ""; }

		public Gee.ArrayList<Message> messages { get; set; default = new Gee.ArrayList<Message>(); }
		
		public ChatCall(Client client)
		{
			base(client);
			this.url_endpoint = "chat";
			this.http_method = "POST";
		}
		// this is only called by response - not by the user
		  
		public override Json.Node serialize_property(string property_name, Value value, ParamSpec pspec)
		{
			switch (property_name) {
				case "chat-content":
				case "message":
				case "streaming-response":
				case "system-content":
					// Exclude these properties from serialization
					return null;
				
				case "think":
					// Only serialize think if true, otherwise exclude
					if (!this.think) {
						return null;
					}
					return default_serialize_property(property_name, value, pspec);
				
				case "tools":
					// Serialize tools as array if not empty, otherwise exclude
					if (this.tools == null || this.tools.size == 0) {
						return null;
					}
					var tools_node = new Json.Node(Json.NodeType.ARRAY);
					tools_node.init_array(new Json.Array());
					var tools_array = tools_node.get_array();
					foreach (var tool in this.tools.values) {
						var tool_node = Json.gobject_serialize(tool);
						var tool_obj = tool_node.get_object();
						// Add "type" field for Ollama API compatibility (tool-type is excluded from serialization)
						tool_obj.set_string_member("type", tool.tool_type);
						tools_array.add_element(tool_node);
					}
					return tools_node;
				
				case "messages":
					// Serialize the message array built in exec_chat()
					var node = new Json.Node(Json.NodeType.ARRAY);
					node.init_array(new Json.Array());
					var array = node.get_array();
					foreach (var m in this.messages) {
						var msg_node = Json.gobject_serialize(m);
						array.add_element(msg_node);
					}
					return node;
				
				default:
					return base.serialize_property(property_name, value, pspec);
			}
		}

		public bool deserialize_property(string property_name, out Value value, ParamSpec pspec, Json.Node property_node)
		{
			if (property_name != "messages") {
				return default_deserialize_property(property_name, out value, pspec, property_node);
			}
			
			this.messages = new Gee.ArrayList<Message>();
			
			var array = property_node.get_array();
			for (int i = 0; i < array.get_length(); i++) {
				var element_node = array.get_element(i);
				var msg_obj = Json.gobject_deserialize(typeof(Message), element_node) as Message;
				
				// Set message_interface to this ChatCall
				msg_obj.message_interface = this;
				this.messages.add(msg_obj);
			}
			
			value = Value(typeof(Gee.ArrayList));
			value.set_object(this.messages);
			return true;
		}
 
		/**
		 * Sets up this ChatCall as a reply to a previous conversation and executes it.
		 * Appends the previous assistant response and new user message to the messages array, then calls exec_chat().
		 * 
		 * @param new_text The new user message text
		 * @param previous_response The previous ChatResponse from the assistant
		 * @return The ChatResponse from executing the chat call
		 */
		public async ChatResponse reply(string new_text, ChatResponse previous_response) throws Error
		{
			// Append the assistant's response from the previous call
			// If it had tool_calls, preserve them in the conversation history
			if (previous_response.message.tool_calls.size > 0) {
				this.messages.add(previous_response.message);
			} else {
				this.messages.add(
					new Message(this, "assistant", previous_response.message.content,
					 previous_response.message.thinking));
			}

			// Append the new user message
			this.messages.add(new Message(this, "user", new_text));

			GLib.debug("ChatCall.reply: Sending %d message(s):", this.messages.size);
			for (int i = 0; i < this.messages.size; i++) {
				var msg = this.messages[i];
				GLib.debug("  Message %d: role='%s', content='%s'%s", 
					i + 1, 
					msg.role, 
					msg.content.length > 100 ? msg.content.substring(0, 100) + "..." : msg.content,
					msg.thinking != "" ? @", thinking='$(msg.thinking.length > 50 ? msg.thinking.substring(0, 50) + "..." : msg.thinking)'" : "");
			}
			
			if (this.stream) {
				//this.streaming_response = new ChatResponse(this.client);
				return yield this.execute_streaming();
			}

			return yield this.execute_non_streaming();
		}

		/**
		 * Executes tool calls from a response and continues the conversation automatically.
		 * 
		 * This method:
		 * 1. Adds the assistant message with tool_calls to the conversation
		 * 2. Executes all tool calls from the response
		 * 3. Adds tool result messages to the conversation
		 * 4. Continues the conversation automatically until a final response is received
		 * 
		 * @param response The ChatResponse containing tool calls
		 * @return The final ChatResponse after tool execution and auto-reply
		 */
		public async ChatResponse toolsReply(ChatResponse response) throws Error
		{
			GLib.debug("ChatCall.toolsReply: Processing tool calls");
			
			// Only process tool calls if response is done and has tool_calls
			// Tool calls can be present even when content is also present
			if (!response.done || response.message.tool_calls.size == 0) {
				GLib.debug("ChatCall.toolsReply: Reply end - done=%s, tool_calls.size=%d, content.length=%zu", 
					response.done.to_string(), 
					response.message.tool_calls.size, 
					response.message.content.length);
				return response;
			}
			
			// Add the assistant message with tool_calls to the conversation
			this.messages.add(response.message);
			
			// Execute each tool call and add tool reply messages directly
			foreach (var tool_call in response.message.tool_calls) {
				GLib.debug("ChatCall.toolsReply: Executing tool '%s' with id '%s'",
					tool_call.function.name, tool_call.id);
				
				if (!this.client.tools.has_key(tool_call.function.name)) {
					GLib.warning("Tool '%s' not found in client tools", tool_call.function.name);
					this.messages.add(new Message.tool_call_invalid(this, tool_call));
					continue;
				}
				
				// Execute the tool
				try {
					this.messages.add(
						new Message.tool_reply(
							this, tool_call.id, 
							tool_call.function.name,
							this.client.tools.get(tool_call.function.name).execute(tool_call.function.arguments)
						));
					GLib.debug("ChatCall.toolsReply: Tool '%s' executed successfully", tool_call.function.name);
				} catch (Error e) {
					GLib.warning("Error executing tool '%s': %s", tool_call.function.name, e.message);
					this.messages.add(new Message.tool_call_fail(this, tool_call, e));
				}
			}
			
			// Execute the chat call
			ChatResponse next_response;
			if (this.stream) {
				next_response = yield this.execute_streaming();
			} else {
				next_response = yield this.execute_non_streaming();
			}
			
			// Recursively handle tool calls if the next response also has them, is done, and has no content
			if (next_response.done && 
				next_response.message.tool_calls.size > 0 
				&& next_response.message.content == "") {
				return yield this.toolsReply(next_response);
			}
			
			return next_response;
		}

		public async ChatResponse exec_chat() throws Error
		{
			if (this.model == "") {
				throw new OllamaError.INVALID_ARGUMENT("Model is required");
			}
			 
			// Add system message if system_content is set
			if (this.system_content != "") {
				this.messages.add(new Message(this, "system", this.system_content));
			}
			
			// Always add the user message (this ChatCall)
			this.messages.add(new Message(this, "user", this.chat_content));
			
			// Debug: output messages being sent
			GLib.debug("ChatCall.exec_chat: Sending %d message(s):", this.messages.size);
			for (int i = 0; i < this.messages.size; i++) {
				var msg = this.messages[i];
				GLib.debug("  Message %d: role='%s', content='%s'%s", 
					i + 1, 
					msg.role, 
					msg.content.length > 100 ? msg.content.substring(0, 100) + "..." : msg.content,
					msg.thinking != "" ? @", thinking='$(msg.thinking.length > 50 ? msg.thinking.substring(0, 50) + "..." : msg.thinking)'" : "");
			}
			
			if (this.stream) {
				//this.streaming_response = new ChatResponse(this.client);
				return yield this.execute_streaming();
			}

			return yield this.execute_non_streaming();
		}

		private async ChatResponse execute_non_streaming() throws Error
		{
			var bytes = yield this.send_request(true);
			var root = this.parse_response(bytes);

			if (root.get_node_type() != Json.NodeType.OBJECT) {
				throw new OllamaError.FAILED("Invalid JSON response");
			}

			var generator = new Json.Generator();
			generator.set_root(root);
			var json_str = generator.to_data(null);
			var response_obj = Json.gobject_from_data(typeof(ChatResponse), json_str, -1) as ChatResponse;
			if (response_obj == null) {
				throw new OllamaError.FAILED("Failed to parse response");
			}

			response_obj.client = this.client;
			response_obj.call = this;
			response_obj.done = true; // Non-streaming responses are always done
			
			// Check for tool calls and handle them recursively
			if (response_obj.message.tool_calls.size > 0) {
				return yield this.toolsReply(response_obj);
			}
			
			return response_obj;
		}

		private async ChatResponse execute_streaming() throws Error
		{
			// Initialize streaming_response before starting stream to ensure it's never null
			if (this.streaming_response == null) {
				this.streaming_response = new ChatResponse(this.client, this);
			}

			var url = this.build_url();
			var request_body = this.get_request_body();
			var message = this.create_streaming_message(url, request_body);

			GLib.debug("Request URL: %s", url);
			GLib.debug("Request Body: %s", request_body);

			try {
				yield this.handle_streaming_response(message, (chunk) => {
					this.process_streaming_chunk(chunk);
				});
			} catch (GLib.IOError e) {
				if (e.code == GLib.IOError.CANCELLED) {
					// User cancelled - ensure response is marked as done
					this.streaming_response.done = true;
					// Return the response even if cancelled (may be partial)
					return this.streaming_response;
				}
				// Re-throw other IO errors
				throw e;
			} catch (Error e) {
				// Mark as done and re-throw
				this.streaming_response.done = true;
				throw e;
			}

		// Check for tool calls and handle them recursively
			GLib.debug("ChatCall.execute_streaming: done=%s, tool_calls.size=%d, content='%s'", 
				this.streaming_response.done.to_string(),
				this.streaming_response.message.tool_calls.size,
				this.streaming_response.message.content);
			if (this.streaming_response.done && 
				this.streaming_response.message.tool_calls.size > 0) {
				GLib.debug("ChatCall.execute_streaming: Calling toolsReply");
				return yield this.toolsReply(this.streaming_response);
			}
			
			return this.streaming_response;
		}
		
		

		private Soup.Message create_streaming_message(string url, string request_body)
		{
			var message = new Soup.Message(this.http_method, url);

			if (this.client.api_key != null && this.client.api_key != "") {
				message.request_headers.append("Authorization", @"Bearer $(this.client.api_key)");
			}

			message.set_request_body_from_bytes("application/json", new Bytes(request_body.data));
			return message;
		}


		private void process_streaming_chunk(Json.Object chunk)
		{
			// Ensure streaming_response exists (should be initialized in execute_streaming, but double-check)
			if (this.streaming_response == null) {
				this.streaming_response = new ChatResponse(this.client, this);
			}

			// Process chunk
			this.streaming_response.addChunk(chunk);

			// Emit signal if there's new content (either regular content or thinking)
			// Also emit when done=true even if no new content, so we can finalize
			// Signal will only be delivered if handlers are connected
			if (this.streaming_response == null 
				|| this.client == null 
				|| (
					this.streaming_response.new_thinking.length == 0 && 
					this.streaming_response.new_content.length == 0 && 
					!this.streaming_response.done
				)) {
				return;
			}
			this.client.stream_chunk(
					this.streaming_response.new_thinking.length > 0 ? this.streaming_response.new_thinking : 
						(this.streaming_response.new_content.length > 0 ? this.streaming_response.new_content : ""), 
					this.streaming_response.new_thinking.length > 0 ? true : 
						(this.streaming_response.new_content.length > 0 ? false : this.streaming_response.is_thinking),
					this.streaming_response);
		}
	}
}
