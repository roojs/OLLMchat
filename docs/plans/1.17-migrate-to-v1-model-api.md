# 1.17. Migrate to v1/model API Endpoint

## Overview

Migrate from the `/api/tags` endpoint to the `/api/v1/model` endpoint for fetching the list of available models. **The primary reason for this migration is that `/api/tags` is not portable** - it's an Ollama-specific endpoint that may not be available on other OpenAI-compatible servers. The `/api/v1/model` endpoint follows the OpenAI API standard and provides better compatibility across different server implementations.

**Parent Plan**: None (standalone migration)

## Rationale

The `/api/tags` endpoint is **not portable** because:
- It's Ollama-specific and not part of the OpenAI API standard
- Other OpenAI-compatible servers (OpenRouter, Together AI, etc.) may not implement this endpoint
- The `/api/v1/model` endpoint follows the OpenAI API standard (`/v1/models`), making it portable across different server implementations
- Better compatibility ensures the application works with a wider range of LLM server backends

By migrating to `/api/v1/model`, we ensure the application can work with any OpenAI-compatible server, not just Ollama.

## Status

⏳ **PENDING** - Not yet started

## Goal

1. Create a new `Call.Tags` class that uses the `/api/tags` endpoint (move current `Call.Models` code there)
2. Update `Call.Models` to use the `/api/v1/model` endpoint with the new response format
3. Update response parsing to handle the new structure: `{"object":"list","data":[...]}`
4. Update `Response.Model` to handle new fields: `id`, `object`, `created`, `owned_by`
5. Map the `id` field from the API response to the `name` property in `Response.Model`

## Current State

### Current Implementation (`Call.Models`)

- **Endpoint**: `tags` (maps to `/api/tags`)
- **HTTP Method**: `GET`
- **Expected Response**: `{"models": [...]}`
- **Parsing**: Uses `get_models("models")` which expects a `models` field in the response

**File**: `libollmchat/Call/Models.vala`
```vala
public class Models : Base
{
    public Models(Settings.Connection connection)
    {
        base(connection);
        this.url_endpoint = "tags";
        this.http_method = "GET";
    }

    public async Gee.ArrayList<Response.Model> exec_models() throws Error
    {
        return yield this.get_models("models");
    }
}
```

### API Response Comparison

#### `/api/tags` Response (Current - Ollama-specific)

**Response Structure**:
```json
{
  "models": [
    {
      "name": "qwen3-next:80b",
      "size": 1234567890,
      "digest": "sha256:abc123...",
      "modified_at": "2025-01-12T04:45:39.123456789Z"
    },
    {
      "name": "deepseek-llm:67b",
      "size": 9876543210,
      "digest": "sha256:def456...",
      "modified_at": "2025-01-11T12:30:37.123456789Z"
    },
    ...
  ]
}
```

**Fields in `/api/tags` response**:
- `name` - Model identifier (string)
- `size` - Model size in bytes (int64)
- `digest` - Model digest/hash (string, e.g., "sha256:...")
- `modified_at` - Last modified timestamp (ISO 8601 format string)

#### `/api/v1/model` Response (New - OpenAI-compatible)

**Endpoint**: `api/v1/model` (note: singular "model", not "models")

**Response Structure**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen3-next:80b",
      "object": "model",
      "created": 1767859539,
      "owned_by": "library"
    },
    {
      "id": "deepseek-llm:67b",
      "object": "model",
      "created": 1767853437,
      "owned_by": "library"
    },
    ...
  ]
}
```

**Fields in `/api/v1/model` response**:
- `id` - Model identifier (string) - **maps to `name` in our code**
- `object` - Object type, always "model" (string)
- `created` - Creation timestamp (Unix epoch integer)
- `owned_by` - Owner identifier (string, e.g., "library", "roojs", "user")

### Key Differences Between Endpoints

| Field | `/api/tags` | `/api/v1/model` | Notes |
|-------|-------------|-----------------|-------|
| **Model identifier** | `name` | `id` | Must map `id` → `name` in parsing |
| **Size** | `size` (bytes, int64) | ❌ Not provided | Only available from `show_model()` |
| **Digest** | `digest` (string) | ❌ Not provided | Only available from `show_model()` |
| **Timestamp** | `modified_at` (ISO 8601 string) | `created` (Unix epoch int) | Different field name and format |
| **Object type** | ❌ Not provided | `object` (always "model") | New field |
| **Owner** | ❌ Not provided | `owned_by` (string) | New field |
| **Response wrapper** | `{"models": [...]}` | `{"object": "list", "data": [...]}` | Different structure |

**Important Notes**:
- `/api/tags` provides `size` and `digest` directly in the list response
- `/api/v1/model` does NOT provide `size` or `digest` - these must be fetched via `show_model()` for each model
- Timestamp formats differ: ISO 8601 string vs Unix epoch integer
- Field name differs: `name` vs `id` (must be mapped during parsing)

## Implementation Steps

### Step 1: Create `Call.Tags` Class

**File**: `libollmchat/Call/Tags.vala` (new file)

Move the current `Call.Models` implementation to `Call.Tags`:

```vala
namespace OLLMchat.Call
{
    /**
     * API call to list available models using the /api/tags endpoint.
     *
     * This is the legacy endpoint. For new code, use Call.Models which
     * uses the /api/v1/model endpoint.
     *
     * @deprecated Use Call.Models instead
     */
    public class Tags : Base
    {
        public Tags(Settings.Connection connection)
        {
            base(connection);
            this.url_endpoint = "tags";
            this.http_method = "GET";
        }

        public async Gee.ArrayList<Response.Model> exec_models() throws Error
        {
            return yield this.get_models("models");
        }
    }
}
```

**Tasks**:
- [ ] Create `libollmchat/Call/Tags.vala`
- [ ] Copy current `Call.Models` implementation to `Call.Tags`
- [ ] Add deprecation notice in documentation
- [ ] Update `libollmchat/meson.build` to include `Call/Tags.vala` in the `ollmchat_ollama_src` files list (add after `Call/Models.vala`)

### Step 2: Update `Call.Models` to Use New Endpoint

**File**: `libollmchat/Call/Models.vala`

Update to use the new endpoint and response format:

```vala
namespace OLLMchat.Call
{
    /**
     * API call to list available models on the Ollama server.
     *
     * Uses the /api/v1/model endpoint which returns a standardized
     * response format with model metadata.
     *
     * Retrieves a list of all models that are available for use.
     */
    public class Models : Base
    {
        public Models(Settings.Connection connection)
        {
            base(connection);
            this.url_endpoint = "api/v1/model";
            this.http_method = "GET";
        }

        public async Gee.ArrayList<Response.Model> exec_models() throws Error
        {
            return yield this.get_models("data");
        }
    }
}
```

**Changes**:
- [ ] Update `url_endpoint` from `"tags"` to `"api/v1/model"`
- [ ] Update `get_models()` call from `"models"` to `"data"` to match new response structure

### Step 3: Update Response.Model to Handle New Fields

**File**: `libollmchat/Response/Model.vala`

The new API response includes fields that need to be mapped:

- `id` → should map to `name` property
- `object` → new field (can be stored but may not be needed)
- `created` → timestamp (Unix epoch)
- `owned_by` → owner information (e.g., "library", "roojs", etc.)

**Tasks**:
- [ ] Add `object` property (string, default: "")
- [ ] Add `created` property (int64, default: 0) - Unix timestamp
- [ ] Add `owned_by` property (string, default: "")
- [ ] Update `parse_models_array()` in `Call.Base` to map `id` to `name` when deserializing

**Implementation in `Call.Base.parse_models_array()`**:

```vala
protected Gee.ArrayList<Response.Model> parse_models_array(Json.Array array)
{
    var items = new Gee.ArrayList<Response.Model>((a, b) => {
        return a.name == b.name;
    });

    for (int i = 0; i < array.get_length(); i++) {
        var element_node = array.get_element(i);
        var generator = new Json.Generator();
        generator.set_root(element_node);
        var json_str = generator.to_data(null);
        var item_obj = Json.gobject_from_data(typeof(Response.Model), json_str, -1) as Response.Model;
        if (item_obj == null) {
            continue;
        }
        
        // For v1/model API: map 'id' to 'name' if name is empty
        if (this is Models) {
            var element_obj = element_node.get_object();
            if (element_obj != null && element_obj.has_member("id")) {
                var model_id = element_obj.get_string_member("id");
                if (item_obj.name == "" && model_id != null) {
                    item_obj.name = model_id;
                }
            }
        }
        
        // For ps() API responses: set name from model property
        // This ensures name is always set for consistency
        // Only do this for Ps, as Models already has name set correctly
        if (this is Ps) {
            GLib.debug("PsCall: setting name='%s' from model='%s'", item_obj.model, item_obj.name);
            item_obj.name = item_obj.model;
        }
        
        items.add(item_obj);
    }

    return items;
}
```

**Tasks**:
- [ ] Add `object`, `created`, `owned_by` properties to `Response.Model`
- [ ] Update `parse_models_array()` to map `id` → `name` for `Call.Models`
- [ ] Ensure JSON deserialization handles the new fields correctly

### Step 4: Update All Usages

**Files to check**:
- `libollmchat/Client.vala` - `models()` method (already uses `Call.Models`, should work)
- `libollmchat/Settings/Connection.vala` - `load_models()` method (uses `client.models()`)
- `libollmchat/Settings/Config2.vala` - Any direct usage
- `examples/TestAppBase.vala` - Example code
- `examples/VectorAppBase.vala` - Example code

**Tasks**:
- [ ] Verify all usages of `Call.Models` still work correctly
- [ ] Test that `Client.models()` returns correct data
- [ ] Test that `Connection.load_models()` populates models correctly

## Potential Issues and Migration Concerns

### Issue 1: Missing Fields in List Response

**Problem**: The `/api/v1/model` endpoint does not return `size`, `digest`, or `modified_at` fields that `/api/tags` provides directly in the list response.

**Key Difference**:
- `/api/tags` provides `size`, `digest`, and `modified_at` directly in the list response
- `/api/v1/model` does NOT provide these fields - they must be fetched via `show_model()` for each model

**Impact**: 
- `Response.Model.size` will be 0 for models from `/api/v1/model` list API (was populated from `/api/tags`)
- `Response.Model.digest` will be empty (was populated from `/api/tags`)
- `Response.Model.modified_at` will be empty (was populated from `/api/tags`)
- More API calls needed: Must call `show_model()` for each model to get size/digest (previously available in list)

**Solution**: 
- These fields are still available from the `show_model()` API call
- The current `Connection.load_models()` already calls `show_model()` for each model to get detailed information
- The `update_from_list_model()` method in `Response.Model` already handles merging data from both APIs
- **No code changes needed** - existing code already handles this correctly
- **Performance note**: This means more API calls are required, but the code already does this, so no functional impact

### Issue 2: Field Name Mapping (`id` vs `name`)

**Problem**: The API returns `id` but our code expects `name`.

**Impact**: Models won't have names set correctly if we don't map the field.

**Solution**: 
- Update `parse_models_array()` to explicitly map `id` → `name` for `Call.Models`
- This is already handled in Step 3 above

### Issue 3: Response Structure Change (`models` vs `data`)

**Problem**: The response field name changed from `models` to `data`.

**Impact**: `get_models("models")` will fail with "Response missing 'models' field" error.

**Solution**: 
- Update `Call.Models.exec_models()` to call `get_models("data")` instead
- This is already handled in Step 2 above

### Issue 4: Endpoint Path (`tags` vs `api/v1/model`)

**Problem**: The endpoint path changed from `tags` to `api/v1/model`.

**Impact**: Requests will fail with 404 if we don't update the endpoint.

**Solution**: 
- Update `url_endpoint` in `Call.Models` constructor
- This is already handled in Step 2 above

### Issue 5: Backward Compatibility with Older Ollama Versions

**Problem**: Older Ollama server versions may not support `/api/v1/model` endpoint.

**Impact**: The application will fail to list models on older Ollama servers.

**Note**: While `/api/tags` is not portable (Ollama-specific), we still need to support older Ollama versions that may not have `/api/v1/model` yet.

**Solution Options**:
1. **Try new endpoint first, fallback to tags** (recommended):
   - Try `/api/v1/model` first (portable, standard endpoint)
   - If it fails with 404, fallback to `/api/tags` (Ollama-specific)
   - This maintains backward compatibility with older Ollama servers
   - Newer servers and OpenAI-compatible servers will use the standard endpoint

2. **Version detection**:
   - Check Ollama version using `Call.Version`
   - Use appropriate endpoint based on version
   - More complex but explicit
   - Doesn't help with non-Ollama servers

3. **Configuration option**:
   - Add a setting to choose which endpoint to use
   - Not recommended - adds complexity and user burden

**Recommended Implementation** (Option 1):

```vala
public async Gee.ArrayList<Response.Model> exec_models() throws Error
{
    // Try new endpoint first
    try {
        this.url_endpoint = "api/v1/model";
        return yield this.get_models("data");
    } catch (OllmError e) {
        // If 404, try legacy endpoint
        if (e is OllmError.FAILED && e.message.contains("404")) {
            GLib.debug("v1/model endpoint not available, falling back to tags");
            this.url_endpoint = "tags";
            return yield this.get_models("models");
        }
        // Re-throw other errors
        throw e;
    }
}
```

**Tasks**:
- [ ] Implement fallback logic in `Call.Models.exec_models()`
- [ ] Test with both new and old Ollama server versions
- [ ] Document backward compatibility behavior

### Issue 6: Timestamp Format Difference (`created` vs `modified_at`)

**Problem**: The `/api/v1/model` endpoint provides `created` as a Unix timestamp (integer), while `/api/tags` provides `modified_at` as an ISO 8601 string. These are different fields with different formats.

**Key Differences**:
- `/api/tags`: `modified_at` - ISO 8601 format string (e.g., "2025-01-12T04:45:39.123456789Z")
- `/api/v1/model`: `created` - Unix epoch integer (e.g., 1767859539)
- Different semantics: `created` is when the model was created, `modified_at` is when it was last modified

**Impact**: 
- Different field name (`created` vs `modified_at`)
- Different data type (int64 vs string)
- Different format (Unix epoch vs ISO 8601)
- May cause confusion in code that expects ISO 8601 format

**Solution**: 
- Store `created` as `int64` (Unix timestamp) in `Response.Model`
- Keep `modified_at` as `string` (ISO 8601) for backward compatibility
- If needed for display, convert `created` to ISO 8601 format in UI code
- Document the difference: `created` (Unix epoch from `/api/v1/model`) vs `modified_at` (ISO 8601 from `/api/tags`)
- Note: These represent different timestamps (creation vs modification), so both may be useful

### Issue 7: No Size/Digest in List Response

**Problem**: The list response doesn't include size/digest, so we can't show model sizes in dropdowns without calling `show_model()` for each.

**Impact**: 
- Model list dropdowns may not show sizes initially
- Need to call `show_model()` for each model to get full details

**Current Behavior**: 
- `Connection.load_models()` already calls `show_model()` for each model
- This is cached, so subsequent loads are fast
- **No change needed** - existing code already handles this

## Testing Checklist

- [ ] Test `Call.Models.exec_models()` with new endpoint
- [ ] Test `Call.Tags.exec_models()` with legacy endpoint (if kept)
- [ ] Test `Client.models()` returns correct data
- [ ] Test `Connection.load_models()` populates models correctly
- [ ] Test model names are correctly set from `id` field
- [ ] Test `created`, `owned_by`, `object` fields are populated
- [ ] Test backward compatibility with older Ollama servers (fallback to tags)
- [ ] Test that `show_model()` still works and merges data correctly
- [ ] Test model dropdowns in UI show correct model names
- [ ] Test that cached models still load correctly

## Summary of Changes

### Primary Motivation
**The `/api/tags` endpoint is not portable** - it's Ollama-specific and not part of the OpenAI API standard. Migrating to `/api/v1/model` ensures compatibility with OpenAI-compatible servers (OpenRouter, Together AI, etc.) and follows industry standards.

### Files to Create
- `libollmchat/Call/Tags.vala` - Legacy tags endpoint implementation (kept for backward compatibility fallback only)

### Files to Modify
- `libollmchat/Call/Models.vala` - Update to use `api/v1/model` endpoint (portable, standard endpoint)
- `libollmchat/Call/Base.vala` - Update `parse_models_array()` to map `id` → `name`
- `libollmchat/Response/Model.vala` - Add `object`, `created`, `owned_by` properties
- `libollmchat/meson.build` - Add `Call/Tags.vala` to `ollmchat_ollama_src` files list

### Breaking Changes
- None - The change is internal to `Call.Models`. External API (`Client.models()`) remains the same.

### Backward Compatibility
- Implement fallback to `/api/tags` endpoint if `/api/v1/model` returns 404
- This ensures compatibility with older Ollama server versions that don't support the standard endpoint yet
- **Note**: The fallback is only for backward compatibility. New implementations should use `/api/v1/model` for portability across different server backends

## Related Plans

- None currently
