# 1.23.19 Testable usage of the agent flow

**Parent:** 1.23.x (skills agent). **Source:** Request for a CLI that exercises each stage of the agent flow in isolation.

**Coding standards:** `.cursor/rules/CODING_STANDARDS.md`. Before marking ready, verify: nullable types, null checks, no `@"..."` except multi-line/help, no one-use temporaries/trivial aliases, brace style (line break for class/method, inline for if/for; no one-line if with body), `this.` for instance members, full `GLib.*` and no `using`, property defaults not constructor init, line breaking for long calls, no StringBuilder except hundreds-of-iterations loops, no string build in loops, `string[] name = {}` not `new string[0]`, loops use `continue` not else/nesting, no character looping (prefer string methods/regex), file try/catch only when existence unknown, minimal try/catch scope, no leading underscore, no `get_*()` — use properties or verb-less names (e.g. `refinement_prompt()` not `get_refinement_prompt()`).

**Plan focus:** Concrete code only. No sentences.

---

## 1. Objective

Create a new example application **oc-test-skill-agent** that extends the standard application (like other examples) and provides **testable, step-by-step usage** of the skills agent flow. Each stage can be run independently so prompts and outputs can be inspected without running the full pipeline.

---

## 2. Application shape

- **Name:** `oc-test-skill-agent`
- **Extends:** Standard application pattern — e.g. `Application` + `OLLMchat.ApplicationInterface` (like `oc-test-cli`) or `TestAppBase` (like `oc-test-gtkmd`). Prefer the same pattern as other examples that need config and CLI (e.g. TestAppBase for shared debug/url/model options).
- **Build:** Add executable in `examples/meson.build` and wire into root `meson.build` run targets if appropriate.
- **Inputs:** Global options `--project PATH`, `--prompt TEXT`, and `--model ID` set the project path, user prompt, and (optionally) the model ID used wherever the Runner or LLM calls need them. `--project` and `--prompt` are required for any mode that builds prompts; `--model` overrides the default model when present.

---

## 3. Modes (subcommands / options)

The following modes are implemented as distinct code paths, keyed off options. Exact option names can be adjusted for consistency (e.g. `--run-prompt` vs `--run-plan`).

### 3.1 Output task-creation prompt (no LLM call)

- **Trigger:** `--project PATH --prompt TEXT` (and no `--run-prompt`).
- **Behaviour:** Build the **task-creation prompt** that would be sent to the LLM (same content as built in `Runner` for `task_creation_initial.md`). Output that prompt (e.g. system + user, or the two messages in a clear format) to stdout.
- **Implementation notes:** `Runner.task_creation()` is private. Either:
  - Add a public (or test-only) method on `Runner` that builds and returns the task-creation prompt (e.g. system and user strings, or a single formatted blob), given a user prompt and optional previous_proposal / previous_proposal_issues; or
  - In the example, construct `Skill.Factory` + `Runner` with a minimal `OLLMfiles.ProjectManager` (and session) set up so that `active_project` (or equivalent) reflects `--project`, then call a new method that exposes the prompt; or
  - Build the same prompt in the example using `Skill.PromptTemplate.template("task_creation_initial.md")` and the same `fill()` arguments as in `Runner.task_creation()`, using `Runner.env()` and factory’s `skill_manager`, after setting project on the project manager.
- **Output:** The exact prompt text the LLM would receive (so it can be pasted or replayed elsewhere).

### 3.2 Run task-creation prompt against a model

- **Trigger:** `--run-prompt` (together with `--project` and `--prompt`). Optionally `--model ID` to select the model; otherwise the default model from config is used.
- **Behaviour:** Build the same task-creation prompt as in §3.1, send it to the selected **model** (from `--model` or config), and **print the raw LLM response** to stdout.
- **Implementation:** Reuse the prompt built as in §3.1; create a `Chat` (or equivalent) with the default connection/model, send the two messages (system, user), stream or wait for the response, print content.
- **Output:** Raw model output (e.g. markdown task list proposal).

### 3.3 Parse task-list output and print summary

- **Trigger:** `--parse-tasklist FILE`
- **Behaviour:** Read the **entire file** as the raw LLM task-list output (e.g. from a previous `--run-prompt` run). Construct a `ResultParser` with a Runner (or minimal runner-like context) and this response string; call **`parse_task_list()`**. Then output a **summary** based on what is in the parser:
  - If `parser.issues != ""`: print issues.
  - If `parser.task_list != null`: print a short summary of the task list (e.g. number of steps; for each step, number of tasks and for each task: skill name, “What is needed” one-liner). Optionally print `parser.proposal` or section headings.
- **Implementation notes:** `ResultParser` needs a `Runner` to build `Details` and `List`. The example must have a way to construct a minimal Runner (Factory + session + project manager with `--project` set) so that `parse_task_list()` can run. The Runner does not need to perform any LLM calls for this mode.
- **Output:** Human-readable summary of the parsed task list and any validation issues.

### 3.3.1 Parse refinement output and print summary

- **Trigger:** `--parse-refine FILE`
- **Behaviour:** Read FILE as the raw LLM refinement output (e.g. from a previous `--run-refine` run). Parse it in the same way refinement results are processed (e.g. with `ResultParser` or the logic used to extract refinement for a task). Print a summary (e.g. extracted sections, tool calls, any issues).
- **Output:** Human-readable summary of the parsed refinement and any issues.

### 3.3.2 Parse execute output and print summary

- **Trigger:** `--parse-execute FILE`
- **Behaviour:** Read FILE as the raw LLM executor response. Use `ResultParser.extract_exec()` (with a suitable task/Details context) to parse it. Print the results (e.g. `task.result`, result summary, issues).
- **Output:** Human-readable summary of the parsed execution result and any issues.

### 3.4 Output refinement prompt for a given task

- **Trigger:** `--refine-task N` where `N` is the **task index** (1-based as in “Task 1”, “Task 2”).
- **Behaviour:** Output the **refinement prompt** (system + user messages) that would be sent to the LLM for task `N`. This is the content built in `Details.refine()` using `task_refinement.md` and the task’s coarse_task_markdown(), reference_contents(), etc.
- **State:** This mode requires a **task list in memory or on disk**. Options:
  - (A) Require that `--parse-tasklist FILE` was run earlier in the same process and keep `parser.task_list` in memory; then resolve task index `N` to the corresponding `Details` and build the refinement prompt for that task.
  - (B) Accept a file argument (e.g. `--refine-task N --task-list-file FILE`) that contains the raw LLM task-list output; parse it in this mode to get the task list, then build the refinement prompt for the Nth task.
- **Implementation notes:** `Details.refine()` does not expose the filled prompt. Either add a method on `Details` (or a test helper) that returns the refinement messages (system, user) without sending, or in the example build the same template fill using the task’s public data (coarse_task_markdown is private; task_data and reference_contents are used inside refine). Prefer exposing a “refinement prompt for this task” API on `Details` or Runner for clarity and single source of truth.
- **Output:** The exact refinement prompt (system + user) for task N.

### 3.5 Run refinement for a task

- **Trigger:** `--run-refine N`
- **Behaviour:** Build the refinement prompt for task N (as in §3.4), send it to the default model, and **print the LLM response** to stdout.
- **State:** Same as §3.4 (task list available from prior `--parse-tasklist` or from a file).
- **Output:** Raw refinement output (e.g. ## Task, ## Tool Calls, etc.).

### 3.6 Execute-task tools and output executor prompt

- **Trigger:** `--execute-task [N|FILE]`
- **Behaviour:** For the given task (by index N or by loading task state from FILE):
  - Run the **execute-task tools** (tool calls) for that task — i.e. `Details.run_tools()` so that `tool_outputs` (and tool_calls) are populated.
  - Then build the **executor prompt** (the prompt that would be sent to the LLM in `Details.post_evaluate()`: `task_execution.md` filled with query, skill_definition, precursor = reference_contents + tool call/output blocks).
  - Output that executor prompt to stdout.
- **State:** The task must already be **refined** (have tools and optionally tool_calls). So either:
  - Refinement was run earlier (e.g. `--run-refine N`) and the task’s refinement result was applied (e.g. `extract_refinement`), and we run tools then build executor prompt; or
  - State is loaded from file(s): e.g. task list from one file, refined task N from another file, and we run tools (which may need a real tool implementation or stub) then output executor prompt.
- **Implementation notes:** If tools have side effects (e.g. read file, run command), the example may use the real tool implementations or a stub that returns fixed output for testing. The plan does not require changing tool semantics; only that “run tools” and “output executor prompt” are both possible.
- **Output:** The exact executor prompt (system + user) that would be sent in `post_evaluate()`.

### 3.7 Run executor and show results

- **Trigger:** `--run-execute-task [N]`
- **Behaviour:** For task N (or the single task in context): call the LLM with the **executor prompt** (as in `Details.post_evaluate()`), then parse the response with `ResultParser.extract_exec(this)` and **print the results** (e.g. `task.result`, “Result summary” content, and/or full response).
- **State:** Task must have tools run and tool_outputs set (as after §3.6). Executor prompt is built and sent; response is parsed and summarized.
- **Output:** Parsed result summary and any issues; optionally raw response.

---

## 4. Dependencies and wiring

- **Runner / Factory:** The example needs a way to obtain:
  - A `Skill.Factory` (or equivalent) with a `ProjectManager` that has `active_project` (or equivalent) set from `--project`.
  - A `Runner` instance (or a minimal stand-in) for:
    - Building task-creation prompt (§3.1, §3.2).
    - Passing to `ResultParser` for `parse_task_list()` (§3.3).
    - Building refinement and executor prompts if those are implemented via Runner/Details (§3.4–3.7).
- **Session / history:** Runner’s base expects a session. Use a minimal in-memory or no-op session for CLI.
- **Config / model:** Reuse the same config loading as other examples (e.g. `ApplicationInterface.base_load_config()`). For `--run-prompt`, `--run-refine`, and `--run-execute-task`, use the model from `--model` when provided, otherwise the default model from config (as in `oc-test-cli`).

---

## 5. Optional: state file(s)

To make `--refine-task`, `--run-refine`, `--execute-task`, and `--run-execute-task` usable across invocations (e.g. save output of `--run-prompt` to a file, then run `--parse-tasklist` on it, then `--refine-task 1`), the app may support:

- Saving the last parsed `task_list` (or a serialized form) to a state file after `--parse-tasklist`, and/or
- Loading task list and/or refined task from file(s) for later modes.

This is optional in v1; the plan can require “run in one process” (e.g. `--run-prompt` then `--parse-tasklist -` reading stdin, then `--refine-task 1`) and add file-based state in a follow-up.

---

## 6. Summary table

| Option / mode           | Purpose |
|-------------------------|--------|
| `--project PATH --prompt TEXT` | Output task-creation prompt (no LLM). |
| `--model ID`             | (Optional) Select model for LLM calls; overrides default. |
| `--run-prompt`          | Run task-creation prompt with selected model (`--model` or default); print response. |
| `--parse-tasklist FILE` | Parse FILE as task-list output; print summary from parser. |
| `--parse-refine FILE`    | Parse FILE as refinement output; print summary. |
| `--parse-execute FILE`   | Parse FILE as executor output; print summary (e.g. extract_exec). |
| `--refine-task N`       | Output refinement prompt for task N. |
| `--run-refine N`        | Run refinement for task N with LLM; print response. |
| `--execute-task [N\|FILE]` | Run execute-task tools; output executor prompt. |
| `--run-execute-task [N]`| Run executor with LLM; print parsed results. |

---

## 7. Implementation order

1. Add `oc-test-skill-agent` example skeleton (application class, `--project`, `--prompt`, `--model`, help).
2. Implement §3.1 (output task-creation prompt): add or use API to get task-creation prompt from Runner; wire to CLI.
3. Implement §3.2 (`--run-prompt`): reuse prompt from §3.1, call selected model (`--model` or default), print output.
4. Implement §3.3 (`--parse-tasklist`): minimal Runner + ProjectManager, read file, `parse_task_list()`, print summary. **Required before §3.4/§3.5** — refinement needs a parsed task list (or refinement mode parses a task-list file itself).
5. Implement §3.3.1 (`--parse-refine`) and §3.3.2 (`--parse-execute`): parse refinement/executor output from FILE, print summary.
6. Implement §3.4 (`--refine-task N`): resolve task N from in-process state (or file); add/use API to get refinement prompt; output it.
7. Implement §3.5 (`--run-refine N`): build refinement prompt, send to LLM, print response.
8. Implement §3.6 (`--execute-task`): run tools for task, build executor prompt, output it.
9. Implement §3.7 (`--run-execute-task`): send executor prompt, parse with `extract_exec`, print results.
10. Optional: state file(s) for cross-run persistence.

---

## 9. Proposed changes by bundle

Proposed code; not in codebase until approved. Each bundle lists every affected file and change for that phase so it can be done in one go.

### Bundle 1: task_creation_prompt + remove open_files

One method: `task_creation_prompt(..., skill_catalog, project_manager)`. Caller passes `OLLMcoder.Skill.Manager` and `OLLMfiles.ProjectManager`; method gets current file from project_manager (e.g. active_file, ensure buffer). No open_files. **Note:** The method is for testing; arguments are passed so the test can supply catalog and project manager without relying on runner/factory state.

**liboccoder/Skill/Runner.vala** — single method; remove private `task_creation()`:

```vala
public PromptTemplate task_creation_prompt(string user_prompt,
    string previous_proposal, string previous_proposal_issues,
    OLLMcoder.Skill.Manager skill_catalog, OLLMfiles.ProjectManager project_manager) throws GLib.Error
{
    var file = project_manager.active_file;
    if (file != null) {
        project_manager.buffer_provider.create_buffer(file);
    }
    var tpl = PromptTemplate.template("task_creation_initial.md");
    tpl.fill(
        "user_prompt", tpl.header_fenced("User Prompt", user_prompt, "text"),
        "environment", tpl.header_raw("Environment", this.env()),
        "project_description", (project_manager.active_project == null ?
            "" : project_manager.active_project.project_description()),
        "current_file", file == null ? "" : tpl.header_file("Current File - " + file.path, file),
        "previous_proposal", previous_proposal == "" ? "" :
            tpl.header_raw("Previous Proposal", previous_proposal),
        "previous_proposal_issues", previous_proposal_issues == "" ? "" :
            tpl.header_raw("Previous Proposal Issues", previous_proposal_issues),
        "skill_catalog", skill_catalog.to_markdown());
    tpl.system_fill("skill_catalog", skill_catalog.to_markdown());
    return tpl;
}
```

**liboccoder/Skill/Runner.vala** — `send_async()` loop body:

```vala
var tpl = this.task_creation_prompt(user_prompt, previous_proposal, previous_proposal_issues, this.sr_factory.skill_manager, this.sr_factory.project_manager);
this.user_request = tpl.user_to_document();
var messages = new Gee.ArrayList<OLLMchat.Message>();
messages.add(new OLLMchat.Message("system", tpl.filled_system));
messages.add(new OLLMchat.Message("user", tpl.filled_user));
// ... send, parse, etc.
```

**resources/skill-prompts/task_creation_initial.md** — remove this line:

```md
{open_files}
```

**examples/oc-test-skill-agent.vala** — build Runner then §3.1 / §3.2:

Build project_manager (DB, load projects, resolve project from `--project`, activate), then Factory + Runner:

```vala
var db_path = GLib.Path.build_filename(this.data_dir, "files.sqlite");
var db = new SQ.Database(db_path, false);
var project_manager = new OLLMfiles.ProjectManager(db);
yield project_manager.load_projects_from_db();

var project = project_manager.projects.path_map.get(opt_project);
if (project == null) {
    cl.printerr("Project not found: %s\n", opt_project);
    throw new GLib.IOError.NOT_FOUND("Project not found: " + opt_project);
}
yield project.load_files_from_db();
OLLMfiles.Folder.background_recurse = false;
yield project.read_dir(new GLib.DateTime.now_local().to_unix(), true);
project.project_files.update_from(project);
yield project_manager.activate_project(project);

var skills_dirs = new Gee.ArrayList<string>();
skills_dirs.add(GLib.Path.build_filename(GLib.Environment.get_home_dir(), "gitlive", "OLLMchat", "resources", "skills"));
var factory = new OLLMcoder.Skill.Factory(project_manager, skills_dirs, "");
var history_manager = new OLLMchat.History.Manager(this);
var session = new OLLMchat.History.EmptySession(history_manager);
var runner = (OLLMcoder.Skill.Runner) factory.create_agent(session);
```

§3.1 and §3.2 (use `runner` and `opt_prompt` from above):

```vala
// §3.1 (method gets current file from project_manager.active_file)
var tpl = runner.task_creation_prompt(opt_prompt, "", "", runner.sr_factory.skill_manager, runner.sr_factory.project_manager);
stdout.printf("=== system ===\n%s\n=== user ===\n%s\n", tpl.filled_system, tpl.filled_user);

// §3.2
var tpl = runner.task_creation_prompt(opt_prompt, "", "", runner.sr_factory.skill_manager, runner.sr_factory.project_manager);
var messages = new Gee.ArrayList<OLLMchat.Message>();
messages.add(new OLLMchat.Message("system", tpl.filled_system));
messages.add(new OLLMchat.Message("user", tpl.filled_user));
// send messages to model (--model or default), print response
```

### Bundle 2: refinement_prompt

**Prerequisite:** Refinement (§3.4, §3.5) needs a **parsed task list** (List + Details). That comes from **§3.3 (--parse-tasklist FILE)**: read the task-list output (e.g. from a prior `--run-prompt`), call `ResultParser.parse_task_list()`, which sets `runner.task_list`. So either implement §3.3 first and run it in the same process before --refine-task / --run-refine, or have refinement mode accept a task-list file and parse it in that mode (e.g. `--task-list-file FILE`). Bundle 2 does not implement §3.3; it only adds the refinement_prompt API and template.

**liboccoder/Task/Details.vala**

Refinement prompt: no current_file placeholder — reference_contents already includes current file when in the task's References.

```vala
public OLLMcoder.Skill.PromptTemplate refinement_prompt() throws GLib.Error
{
    var definition = this.skill_manager.fetch(this);
    var tpl = OLLMcoder.Skill.PromptTemplate.template("task_refinement.md");
    tpl.system_fill(); // needed to fill in filled_system
    tpl.fill(
        "issues", tpl.header_raw("Issues with the current call", this.result_parser.issues),
        "task_data", tpl.header_raw("Task", this.to_markdown(MarkdownPhase.REFINEMENT)),
        "environment", this.runner.env(),
        "project_description", (this.runner.sr_factory.project_manager.active_project == null ?
            "" : this.runner.sr_factory.project_manager.active_project.project_description()),
        "task_reference_contents", this.reference_contents(),
        "skill_details", definition.full_content);
    return tpl;
}
```

**liboccoder/Task/Details.vala** — in `refine()`, replace the fill (no current_file):

```vala
tpl.fill(
    "issues", tpl.header_raw("Issues with the current call", this.result_parser.issues),
    "task_data", tpl.header_raw("Task", this.to_markdown(MarkdownPhase.REFINEMENT)),
    "environment", this.runner.env(),
    "project_description", (this.runner.sr_factory.project_manager.active_project == null ?
        "" : this.runner.sr_factory.project_manager.active_project.project_description()),
    "task_reference_contents", this.reference_contents(),
    "skill_details", definition.full_content);
```

**resources/skill-prompts/task_creation_initial.md** — add (e.g. in Reference link types or Output format):

```md
If a task needs the current (open) document, add a reference to it in that task's References using the standard link format (e.g. [Basename](/absolute/path/to/file)).
```

**examples/oc-test-skill-agent.vala** — §3.4 and §3.5 (full code):

Assume `this.runner` is set (e.g. from `build_runner(cl)`). Task list must be in memory: either from an earlier `--parse-tasklist FILE` in the same run (then `this.runner.task_list` is set), or in this mode read the task-list file, parse, then resolve task N.

```vala
// Obtain task list: if not already in memory, read file and parse
if (this.runner.task_list == null && opt_task_list_file != null) {
    string content;
    GLib.FileUtils.get_contents(opt_task_list_file, out content);
    var parser = new OLLMcoder.Task.ResultParser(this.runner, content);
    parser.parse_task_list();
    if (parser.issues != "") {
        cl.printerr("Parse issues: %s\n", parser.issues);
        throw new GLib.IOError.INVALID_ARGUMENT(parser.issues);
    }
}

// Resolve task N (1-based) to Details
OLLMcoder.Task.Details? detail = null;
int index = 0;
foreach (var step in this.runner.task_list.steps) {
    foreach (var t in step.children) {
        index++;
        if (index == opt_refine_task) {
            detail = t;
            break;
        }
    }
    if (detail != null) {
        break;
    }
}
if (detail == null) {
    cl.printerr("Task %d not found.\n", opt_refine_task);
    throw new GLib.IOError.NOT_FOUND("Task not found: " + opt_refine_task.to_string());
}

// §3.4 --refine-task N: output refinement prompt only
var tpl = detail.refinement_prompt();
stdout.printf("=== system ===\n%s\n=== user ===\n%s\n", tpl.filled_system, tpl.filled_user);

// §3.5 --run-refine N: build same prompt, send to model, print response
var tpl = detail.refinement_prompt();
var messages = new Gee.ArrayList<OLLMchat.Message>();
messages.add(new OLLMchat.Message("system", tpl.filled_system));
messages.add(new OLLMchat.Message("user", tpl.filled_user));
var response_obj = yield this.runner.chat().send(messages, null);
stdout.printf("%s", response_obj != null && response_obj.message != null ? response_obj.message.content ?? "" : "");
```

### Bundle 3: executor_prompt

**liboccoder/Task/Details.vala**

```vala
public OLLMcoder.Skill.PromptTemplate executor_prompt() throws GLib.Error
{
    var definition = this.skill_manager.fetch(this);
    var tpl = OLLMcoder.Skill.PromptTemplate.template("task_execution.md");
    tpl.system_fill();
    tpl.fill(
        "query", this.task_data.get("What is needed").to_markdown(),
        "skill_definition", definition.full_content,
        "precursor", this.executor_precursor());
    return tpl;
}
```

**examples/oc-test-skill-agent.vala** — §3.6 and §3.7:

```vala
// §3.6 --execute-task: for task N
yield detail.run_tools();
var tpl = detail.executor_prompt();
stdout.printf("=== system ===\n%s\n=== user ===\n%s\n", tpl.filled_system, tpl.filled_user);

// §3.7 --run-execute-task: same then send to model
yield detail.run_tools();
var tpl = detail.executor_prompt();
// send (system, user) to model; parse response with result_parser.extract_exec(detail); print results
```

### ResultParser

No changes. Test app uses existing parse_task_list(), extract_refinement(), extract_exec() with minimal Runner/Details.

---

## 10. Out of scope

- Changing the design of prompts or of Runner/Details/ResultParser beyond adding small test-friendly accessors (e.g. “get prompt for this stage”).
- Full end-to-end automation of the whole agent in this example; the goal is **testable usage** of each stage, not a single “run everything” command (that remains the normal app flow).
