# 1.23.19 Testable usage of the agent flow

**Parent:** 1.23.x (skills agent). **Source:** Request for a CLI that exercises each stage of the agent flow in isolation.

**Coding standards:** `.cursor/rules/CODING_STANDARDS.md`.

---

## 1. Objective

Create a new example application **oc-test-skill-agent** that extends the standard application (like other examples) and provides **testable, step-by-step usage** of the skills agent flow. Each stage can be run independently so prompts and outputs can be inspected without running the full pipeline.

---

## 2. Application shape

- **Name:** `oc-test-skill-agent`
- **Extends:** Standard application pattern — e.g. `Application` + `OLLMchat.ApplicationInterface` (like `oc-test-cli`) or `TestAppBase` (like `oc-test-gtkmd`). Prefer the same pattern as other examples that need config and CLI (e.g. TestAppBase for shared debug/url/model options).
- **Build:** Add executable in `examples/meson.build` and wire into root `meson.build` run targets if appropriate.
- **Inputs:** Global options `--project PATH`, `--prompt TEXT`, and `--model ID` set the project path, user prompt, and (optionally) the model ID used wherever the Runner or LLM calls need them. `--project` and `--prompt` are required for any mode that builds prompts; `--model` overrides the default model when present.

---

## 3. Modes (subcommands / options)

The following modes are implemented as distinct code paths, keyed off options. Exact option names can be adjusted for consistency (e.g. `--run-prompt` vs `--run-plan`).

### 3.1 Output task-creation prompt (no LLM call)

- **Trigger:** `--project PATH --prompt TEXT` (and no `--run-prompt`).
- **Behaviour:** Build the **task-creation prompt** that would be sent to the LLM (same content as built in `Runner` for `task_creation_initial.md`). Output that prompt (e.g. system + user, or the two messages in a clear format) to stdout.
- **Implementation notes:** `Runner.task_creation()` is private. Either:
  - Add a public (or test-only) method on `Runner` that builds and returns the task-creation prompt (e.g. system and user strings, or a single formatted blob), given a user prompt and optional previous_proposal / previous_proposal_issues; or
  - In the example, construct `Skill.Factory` + `Runner` with a minimal `OLLMfiles.ProjectManager` (and session) set up so that `active_project` (or equivalent) reflects `--project`, then call a new method that exposes the prompt; or
  - Build the same prompt in the example using `Skill.PromptTemplate.template("task_creation_initial.md")` and the same `fill()` arguments as in `Runner.task_creation()`, using `Runner.env()` and factory’s `skill_manager`, after setting project on the project manager.
- **Output:** The exact prompt text the LLM would receive (so it can be pasted or replayed elsewhere).

### 3.2 Run task-creation prompt against a model

- **Trigger:** `--run-prompt` (together with `--project` and `--prompt`). Optionally `--model ID` to select the model; otherwise the default model from config is used.
- **Behaviour:** Build the same task-creation prompt as in §3.1, send it to the selected **model** (from `--model` or config), and **print the raw LLM response** to stdout.
- **Implementation:** Reuse the prompt built as in §3.1; create a `Chat` (or equivalent) with the default connection/model, send the two messages (system, user), stream or wait for the response, print content.
- **Output:** Raw model output (e.g. markdown task list proposal).

### 3.3 Parse task-list output and print summary

- **Trigger:** `--parse-tasklist FILE`
- **Behaviour:** Read the **entire file** as the raw LLM task-list output (e.g. from a previous `--run-prompt` run). Construct a `ResultParser` with a Runner (or minimal runner-like context) and this response string; call **`parse_task_list()`**. Then output a **summary** based on what is in the parser:
  - If `parser.issues != ""`: print issues.
  - If `parser.task_list != null`: print a short summary of the task list (e.g. number of steps; for each step, number of tasks and for each task: skill name, “What is needed” one-liner). Optionally print `parser.proposal` or section headings.
- **Implementation notes:** `ResultParser` needs a `Runner` to build `Details` and `List`. The example must have a way to construct a minimal Runner (Factory + session + project manager with `--project` set) so that `parse_task_list()` can run. The Runner does not need to perform any LLM calls for this mode.
- **Output:** Human-readable summary of the parsed task list and any validation issues.

### 3.3.1 Parse refinement output and print summary

- **Trigger:** `--parse-refine FILE`
- **Behaviour:** Read FILE as the raw LLM refinement output (e.g. from a previous `--run-refine` run). Parse it in the same way refinement results are processed (e.g. with `ResultParser` or the logic used to extract refinement for a task). Print a summary (e.g. extracted sections, tool calls, any issues).
- **Output:** Human-readable summary of the parsed refinement and any issues.

### 3.3.2 Parse execute output and print summary

- **Trigger:** `--parse-execute FILE`
- **Behaviour:** Read FILE as the raw LLM executor response. Use `ResultParser.extract_exec()` (with a suitable task/Details context) to parse it. Print the results (e.g. `task.result`, result summary, issues).
- **Output:** Human-readable summary of the parsed execution result and any issues.

### 3.4 Output refinement prompt for a given task

- **Trigger:** `--refine-task N` where `N` is the **task index** (1-based as in “Task 1”, “Task 2”).
- **Behaviour:** Output the **refinement prompt** (system + user messages) that would be sent to the LLM for task `N`. This is the content built in `Details.refine()` using `task_refinement.md` and the task’s coarse_task_markdown(), reference_contents(), etc.
- **State:** This mode requires a **task list in memory or on disk**. Options:
  - (A) Require that `--parse-tasklist FILE` was run earlier in the same process and keep `parser.task_list` in memory; then resolve task index `N` to the corresponding `Details` and build the refinement prompt for that task.
  - (B) Accept a file argument (e.g. `--refine-task N --task-list-file FILE`) that contains the raw LLM task-list output; parse it in this mode to get the task list, then build the refinement prompt for the Nth task.
- **Implementation notes:** `Details.refine()` does not expose the filled prompt. Either add a method on `Details` (or a test helper) that returns the refinement messages (system, user) without sending, or in the example build the same template fill using the task’s public data (coarse_task_markdown is private; task_data and reference_contents are used inside refine). Prefer exposing a “refinement prompt for this task” API on `Details` or Runner for clarity and single source of truth.
- **Output:** The exact refinement prompt (system + user) for task N.

### 3.5 Run refinement for a task

- **Trigger:** `--run-refine N`
- **Behaviour:** Build the refinement prompt for task N (as in §3.4), send it to the default model, and **print the LLM response** to stdout.
- **State:** Same as §3.4 (task list available from prior `--parse-tasklist` or from a file).
- **Output:** Raw refinement output (e.g. ## Task, ## Tool Calls, etc.).

### 3.6 Execute-task tools and output executor prompt

- **Trigger:** `--execute-task [N|FILE]`
- **Behaviour:** For the given task (by index N or by loading task state from FILE):
  - Run the **execute-task tools** (tool calls) for that task — i.e. `Details.run_tools()` so that `tool_outputs` (and tool_calls) are populated.
  - Then build the **executor prompt** (the prompt that would be sent to the LLM in `Details.post_evaluate()`: `task_execution.md` filled with query, skill_definition, precursor = reference_contents + tool call/output blocks).
  - Output that executor prompt to stdout.
- **State:** The task must already be **refined** (have tools and optionally tool_calls). So either:
  - Refinement was run earlier (e.g. `--run-refine N`) and the task’s refinement result was applied (e.g. `extract_refinement`), and we run tools then build executor prompt; or
  - State is loaded from file(s): e.g. task list from one file, refined task N from another file, and we run tools (which may need a real tool implementation or stub) then output executor prompt.
- **Implementation notes:** If tools have side effects (e.g. read file, run command), the example may use the real tool implementations or a stub that returns fixed output for testing. The plan does not require changing tool semantics; only that “run tools” and “output executor prompt” are both possible.
- **Output:** The exact executor prompt (system + user) that would be sent in `post_evaluate()`.

### 3.7 Run executor and show results

- **Trigger:** `--run-execute-task [N]`
- **Behaviour:** For task N (or the single task in context): call the LLM with the **executor prompt** (as in `Details.post_evaluate()`), then parse the response with `ResultParser.extract_exec(this)` and **print the results** (e.g. `task.result`, “Result summary” content, and/or full response).
- **State:** Task must have tools run and tool_outputs set (as after §3.6). Executor prompt is built and sent; response is parsed and summarized.
- **Output:** Parsed result summary and any issues; optionally raw response.

---

## 4. Dependencies and wiring

- **Runner / Factory:** The example needs a way to obtain:
  - A `Skill.Factory` (or equivalent) with a `ProjectManager` that has `active_project` (or equivalent) set from `--project`.
  - A `Runner` instance (or a minimal stand-in) for:
    - Building task-creation prompt (§3.1, §3.2).
    - Passing to `ResultParser` for `parse_task_list()` (§3.3).
    - Building refinement and executor prompts if those are implemented via Runner/Details (§3.4–3.7).
- **Session / history:** Runner’s base expects a session. Use a minimal in-memory or no-op session for CLI.
- **Config / model:** Reuse the same config loading as other examples (e.g. `ApplicationInterface.base_load_config()`). For `--run-prompt`, `--run-refine`, and `--run-execute-task`, use the model from `--model` when provided, otherwise the default model from config (as in `oc-test-cli`).

---

## 5. Optional: state file(s)

To make `--refine-task`, `--run-refine`, `--execute-task`, and `--run-execute-task` usable across invocations (e.g. save output of `--run-prompt` to a file, then run `--parse-tasklist` on it, then `--refine-task 1`), the app may support:

- Saving the last parsed `task_list` (or a serialized form) to a state file after `--parse-tasklist`, and/or
- Loading task list and/or refined task from file(s) for later modes.

This is optional in v1; the plan can require “run in one process” (e.g. `--run-prompt` then `--parse-tasklist -` reading stdin, then `--refine-task 1`) and add file-based state in a follow-up.

---

## 6. Summary table

| Option / mode           | Purpose |
|-------------------------|--------|
| `--project PATH --prompt TEXT` | Output task-creation prompt (no LLM). |
| `--model ID`             | (Optional) Select model for LLM calls; overrides default. |
| `--run-prompt`          | Run task-creation prompt with selected model (`--model` or default); print response. |
| `--parse-tasklist FILE` | Parse FILE as task-list output; print summary from parser. |
| `--parse-refine FILE`    | Parse FILE as refinement output; print summary. |
| `--parse-execute FILE`   | Parse FILE as executor output; print summary (e.g. extract_exec). |
| `--refine-task N`       | Output refinement prompt for task N. |
| `--run-refine N`        | Run refinement for task N with LLM; print response. |
| `--execute-task [N\|FILE]` | Run execute-task tools; output executor prompt. |
| `--run-execute-task [N]`| Run executor with LLM; print parsed results. |

---

## 7. Implementation order

1. Add `oc-test-skill-agent` example skeleton (application class, `--project`, `--prompt`, `--model`, help).
2. Implement §3.1 (output task-creation prompt): add or use API to get task-creation prompt from Runner; wire to CLI.
3. Implement §3.2 (`--run-prompt`): reuse prompt from §3.1, call selected model (`--model` or default), print output.
4. Implement §3.3 (`--parse-tasklist`): minimal Runner + ProjectManager, read file, `parse_task_list()`, print summary.
5. Implement §3.3.1 (`--parse-refine`) and §3.3.2 (`--parse-execute`): parse refinement/executor output from FILE, print summary.
6. Implement §3.4 (`--refine-task N`): resolve task N from in-process state (or file); add/use API to get refinement prompt; output it.
7. Implement §3.5 (`--run-refine N`): build refinement prompt, send to LLM, print response.
8. Implement §3.6 (`--execute-task`): run tools for task, build executor prompt, output it.
9. Implement §3.7 (`--run-execute-task`): send executor prompt, parse with `extract_exec`, print results.
10. Optional: state file(s) for cross-run persistence.

---

## 9. Proposed API changes (Runner / Details / ResultParser)

The following library changes are needed so that **oc-test-skill-agent** can implement each mode without duplicating prompt logic. Review and approve before implementation.

### 9.1 Runner (`liboccoder/Skill/Runner.vala`)

| Change | Purpose | Notes |
|--------|---------|--------|
| **Expose task-creation prompt** | §3.1, §3.2: build the same prompt as `send_async` uses, without sending. | `task_creation()` is currently private and returns a filled `PromptTemplate`. |
| **Proposal:** Add a public method that builds and returns the task-creation prompt content. For example: **`public void build_task_creation_prompt(string user_prompt, out string system, out string user, string previous_proposal = "", string previous_proposal_issues = "") throws GLib.Error`** — implementation calls the existing `task_creation()` logic (or a shared helper), fills the template, and assigns `tpl.filled_system` and `tpl.filled_user` to the out params. Alternative: make `task_creation()` public and return `PromptTemplate` so the caller can read `filled_system` / `filled_user` directly (if that fits the rest of the API). |

No other Runner API changes are required for the plan. The test app will construct a minimal Runner (Factory + session + ProjectManager with `active_project` set from `--project`) for §3.1–3.3 and for ResultParser.

### 9.2 Details (`liboccoder/Task/Details.vala`)

| Change | Purpose | Notes |
|--------|---------|--------|
| **Expose refinement prompt (system + user)** | §3.4, §3.5: output or send the refinement messages without duplicating template fill. | `refine()` builds messages internally and sends them; the filled content is not exposed. |
| **Proposal:** Add a method that builds the same system and user strings as `refine()` but does **not** send. For example: **`public void get_refinement_prompt(out string system, out string user) throws GLib.Error`**. Implementation: load `task_refinement.md` template, call `tpl.system_fill()`, then `tpl.fill(...)` with the same arguments as in `refine()` (issues, task_data via `to_markdown(MarkdownPhase.REFINEMENT)`, env, project_description, current_file, task_reference_contents, skill_details from `skill_manager.fetch(this)`). Assign `tpl.filled_system` and `tpl.filled_user` to the out params. Requires `skill_manager.fetch(this)` (definition) to be non-null; caller must ensure task list and skill context are valid. |
| **Expose executor prompt (system + user)** | §3.6, §3.7: output the executor prompt that would be sent in `post_evaluate()`. | `post_evaluate()` builds messages from `task_execution.md` and sends; precursor uses `executor_precursor()` (private). |
| **Proposal:** Add a method that builds the same system and user strings as `post_evaluate()` but does **not** send. For example: **`public void get_executor_prompt(out string system, out string user) throws GLib.Error`**. Implementation: load `task_execution.md`, call `tpl.system_fill()`, then `tpl.fill("query", ..., "skill_definition", definition.full_content, "precursor", this.executor_precursor())`. Assign filled system/user to the out params. Keeps `executor_precursor()` private (called only from within Details). Task must already have `reference_contents` and `tool_outputs` / `tool_calls` as after `run_tools()`. |

No other Details API changes are required. The test app will use a minimal or full Details as needed for parse/refine/execute modes.

### 9.3 ResultParser (`liboccoder/Task/ResultParser.vala`)

| Change | Purpose | Notes |
|--------|---------|--------|
| **None required** | §3.3, §3.3.1, §3.3.2 | Existing API is sufficient. `parse_task_list()` needs a Runner (set in constructor). `extract_refinement(Details task)` and `extract_exec(Details task)` need a Details instance; for `--parse-refine` and `--parse-execute` the test app can construct a **minimal Details** (e.g. Runner + factory + session + minimal `task_data` map) and pass it; the parser will mutate that task and set `issues`. No new methods or signatures proposed. |

### 9.4 Summary

- **Runner:** 1 new public method: build and return task-creation prompt (system + user strings).
- **Details:** 2 new public methods: get refinement prompt (system + user), get executor prompt (system + user).
- **ResultParser:** No API changes.

After approval, implement these accessors in the library, then wire **oc-test-skill-agent** to them and add the executable to `examples/meson.build`.

---

## 10. Out of scope

- Changing the design of prompts or of Runner/Details/ResultParser beyond adding small test-friendly accessors (e.g. “get prompt for this stage”).
- Full end-to-end automation of the whole agent in this example; the goal is **testable usage** of each stage, not a single “run everything” command (that remains the normal app flow).
