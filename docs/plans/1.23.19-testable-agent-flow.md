# 1.23.19 Testable usage of the agent flow

**Parent:** 1.23.x (skills agent). **Source:** Request for a CLI that exercises each stage of the agent flow in isolation.

**Coding standards:** `.cursor/rules/CODING_STANDARDS.md`.

---

## 1. Objective

Create a new example application **oc-test-skill-agent** that extends the standard application (like other examples) and provides **testable, step-by-step usage** of the skills agent flow. Each stage can be run independently so prompts and outputs can be inspected without running the full pipeline.

---

## 2. Application shape

- **Name:** `oc-test-skill-agent`
- **Extends:** Standard application pattern — e.g. `Application` + `OLLMchat.ApplicationInterface` (like `oc-test-cli`) or `TestAppBase` (like `oc-test-gtkmd`). Prefer the same pattern as other examples that need config and CLI (e.g. TestAppBase for shared debug/url/model options).
- **Build:** Add executable in `examples/meson.build` and wire into root `meson.build` run targets if appropriate.
- **Inputs:** Global options `--project PATH` and `--prompt TEXT` set the project path and user prompt used wherever the Runner (or equivalent) needs them. These are required for any mode that builds prompts.

---

## 3. Modes (subcommands / options)

The following modes are implemented as distinct code paths, keyed off options. Exact option names can be adjusted for consistency (e.g. `--run-prompt` vs `--run-plan`).

### 3.1 Output task-creation prompt (no LLM call)

- **Trigger:** `--project PATH --prompt TEXT` (and no `--run-prompt`).
- **Behaviour:** Build the **task-creation prompt** that would be sent to the LLM (same content as built in `Runner` for `task_creation_initial.md`). Output that prompt (e.g. system + user, or the two messages in a clear format) to stdout.
- **Implementation notes:** `Runner.task_creation()` is private. Either:
  - Add a public (or test-only) method on `Runner` that builds and returns the task-creation prompt (e.g. system and user strings, or a single formatted blob), given a user prompt and optional previous_proposal / previous_proposal_issues; or
  - In the example, construct `Skill.Factory` + `Runner` with a minimal `OLLMfiles.ProjectManager` (and session) set up so that `active_project` (or equivalent) reflects `--project`, then call a new method that exposes the prompt; or
  - Build the same prompt in the example using `Skill.PromptTemplate.template("task_creation_initial.md")` and the same `fill()` arguments as in `Runner.task_creation()`, using `Runner.env()` and factory’s `skill_manager`, after setting project on the project manager.
- **Output:** The exact prompt text the LLM would receive (so it can be pasted or replayed elsewhere).

### 3.2 Run task-creation prompt against the default model

- **Trigger:** `--run-prompt` (together with `--project` and `--prompt`).
- **Behaviour:** Build the same task-creation prompt as in §3.1, send it to the **default model** (from config / `default_model` usage), and **print the raw LLM response** to stdout.
- **Implementation:** Reuse the prompt built as in §3.1; create a `Chat` (or equivalent) with the default connection/model, send the two messages (system, user), stream or wait for the response, print content.
- **Output:** Raw model output (e.g. markdown task list proposal).

### 3.3 Parse LLM output and print summary

- **Trigger:** `--parse-input FILE`
- **Behaviour:** Read the **entire file** as the raw LLM output (e.g. from a previous `--run-prompt` run). Construct a `ResultParser` with a Runner (or minimal runner-like context) and this response string; call **`parse_task_list()`**. Then output a **summary** based on what is in the parser:
  - If `parser.issues != ""`: print issues.
  - If `parser.task_list != null`: print a short summary of the task list (e.g. number of steps; for each step, number of tasks and for each task: skill name, “What is needed” one-liner). Optionally print `parser.proposal` or section headings.
- **Implementation notes:** `ResultParser` needs a `Runner` to build `Details` and `List`. The example must have a way to construct a minimal Runner (Factory + session + project manager with `--project` set) so that `parse_task_list()` can run. The Runner does not need to perform any LLM calls for this mode.
- **Output:** Human-readable summary of the parsed task list and any validation issues.

### 3.4 Output refinement prompt for a given task

- **Trigger:** `--refine-task N` where `N` is the **task index** (1-based as in “Task 1”, “Task 2”).
- **Behaviour:** Output the **refinement prompt** (system + user messages) that would be sent to the LLM for task `N`. This is the content built in `Details.refine()` using `task_refinement.md` and the task’s coarse_task_markdown(), reference_contents(), etc.
- **State:** This mode requires a **task list in memory or on disk**. Options:
  - (A) Require that `--parse-input FILE` was run earlier in the same process and keep `parser.task_list` in memory; then resolve task index `N` to the corresponding `Details` and build the refinement prompt for that task.
  - (B) Accept a file argument (e.g. `--refine-task N --task-list-file FILE`) that contains the raw LLM task-list output; parse it in this mode to get the task list, then build the refinement prompt for the Nth task.
- **Implementation notes:** `Details.refine()` does not expose the filled prompt. Either add a method on `Details` (or a test helper) that returns the refinement messages (system, user) without sending, or in the example build the same template fill using the task’s public data (coarse_task_markdown is private; task_data and reference_contents are used inside refine). Prefer exposing a “refinement prompt for this task” API on `Details` or Runner for clarity and single source of truth.
- **Output:** The exact refinement prompt (system + user) for task N.

### 3.5 Run refinement for a task

- **Trigger:** `--run-refine N`
- **Behaviour:** Build the refinement prompt for task N (as in §3.4), send it to the default model, and **print the LLM response** to stdout.
- **State:** Same as §3.4 (task list available from prior `--parse-input` or from a file).
- **Output:** Raw refinement output (e.g. ## Task, ## Tool Calls, etc.).

### 3.6 Execute-task tools and output executor prompt

- **Trigger:** `--execute-task [N|FILE]`
- **Behaviour:** For the given task (by index N or by loading task state from FILE):
  - Run the **execute-task tools** (tool calls) for that task — i.e. `Details.run_tools()` so that `tool_outputs` (and tool_calls) are populated.
  - Then build the **executor prompt** (the prompt that would be sent to the LLM in `Details.post_evaluate()`: `task_execution.md` filled with query, skill_definition, precursor = reference_contents + tool call/output blocks).
  - Output that executor prompt to stdout.
- **State:** The task must already be **refined** (have tools and optionally tool_calls). So either:
  - Refinement was run earlier (e.g. `--run-refine N`) and the task’s refinement result was applied (e.g. `extract_refinement`), and we run tools then build executor prompt; or
  - State is loaded from file(s): e.g. task list from one file, refined task N from another file, and we run tools (which may need a real tool implementation or stub) then output executor prompt.
- **Implementation notes:** If tools have side effects (e.g. read file, run command), the example may use the real tool implementations or a stub that returns fixed output for testing. The plan does not require changing tool semantics; only that “run tools” and “output executor prompt” are both possible.
- **Output:** The exact executor prompt (system + user) that would be sent in `post_evaluate()`.

### 3.7 Run executor and show results

- **Trigger:** `--run-execute-task [N]`
- **Behaviour:** For task N (or the single task in context): call the LLM with the **executor prompt** (as in `Details.post_evaluate()`), then parse the response with `ResultParser.extract_exec(this)` and **print the results** (e.g. `task.result`, “Result summary” content, and/or full response).
- **State:** Task must have tools run and tool_outputs set (as after §3.6). Executor prompt is built and sent; response is parsed and summarized.
- **Output:** Parsed result summary and any issues; optionally raw response.

---

## 4. Dependencies and wiring

- **Runner / Factory:** The example needs a way to obtain:
  - A `Skill.Factory` (or equivalent) with a `ProjectManager` that has `active_project` (or equivalent) set from `--project`.
  - A `Runner` instance (or a minimal stand-in) for:
    - Building task-creation prompt (§3.1, §3.2).
    - Passing to `ResultParser` for `parse_task_list()` (§3.3).
    - Building refinement and executor prompts if those are implemented via Runner/Details (§3.4–3.7).
- **Session / history:** Runner’s base expects a session. Use a minimal in-memory or no-op session for CLI.
- **Config / default model:** Reuse the same config loading as other examples (e.g. `ApplicationInterface.base_load_config()`) and the same default model resolution as `oc-test-cli` for `--run-prompt`, `--run-refine`, and `--run-execute-task`.

---

## 5. Optional: state file(s)

To make `--refine-task`, `--run-refine`, `--execute-task`, and `--run-execute-task` usable across invocations (e.g. save output of `--run-prompt` to a file, then run `--parse-input` on it, then `--refine-task 1`), the app may support:

- Saving the last parsed `task_list` (or a serialized form) to a state file after `--parse-input`, and/or
- Loading task list and/or refined task from file(s) for later modes.

This is optional in v1; the plan can require “run in one process” (e.g. `--run-prompt` then `--parse-input -` reading stdin, then `--refine-task 1`) and add file-based state in a follow-up.

---

## 6. Summary table

| Option / mode           | Purpose |
|-------------------------|--------|
| `--project PATH --prompt TEXT` | Output task-creation prompt (no LLM). |
| `--run-prompt`          | Run task-creation prompt with default model; print response. |
| `--parse-input FILE`    | Parse FILE as task-list output; print summary from parser. |
| `--refine-task N`       | Output refinement prompt for task N. |
| `--run-refine N`        | Run refinement for task N with LLM; print response. |
| `--execute-task [N\|FILE]` | Run execute-task tools; output executor prompt. |
| `--run-execute-task [N]`| Run executor with LLM; print parsed results. |

---

## 7. Implementation order

1. Add `oc-test-skill-agent` example skeleton (application class, `--project`, `--prompt`, help).
2. Implement §3.1 (output task-creation prompt): add or use API to get task-creation prompt from Runner; wire to CLI.
3. Implement §3.2 (`--run-prompt`): reuse prompt from §3.1, call default model, print output.
4. Implement §3.3 (`--parse-input`): minimal Runner + ProjectManager, read file, `parse_task_list()`, print summary.
5. Implement §3.4 (`--refine-task N`): resolve task N from in-process state (or file); add/use API to get refinement prompt; output it.
6. Implement §3.5 (`--run-refine N`): build refinement prompt, send to LLM, print response.
7. Implement §3.6 (`--execute-task`): run tools for task, build executor prompt, output it.
8. Implement §3.7 (`--run-execute-task`): send executor prompt, parse with `extract_exec`, print results.
9. Optional: state file(s) for cross-run persistence.

---

## 8. Out of scope

- Changing the design of prompts or of Runner/Details/ResultParser beyond adding small test-friendly accessors (e.g. “get prompt for this stage”).
- Full end-to-end automation of the whole agent in this example; the goal is **testable usage** of each stage, not a single “run everything” command (that remains the normal app flow).
