# 2.20.3. Binary File Handling - Image Analysis

## Coding standards

This plan follows **`.cursor/rules/CODING_STANDARDS.md`**. All new or modified code must comply. Summary checklist:

- **Nullable types**: Prefer default objects/flags over nullable where possible.
- **Null checks**: Only where the design explicitly requires null (e.g. external API).
- **String interpolation**: Avoid `@"..."` except for multi-line usage/help text or documentation.
- **Temporary variables**: Avoid one-use temporaries; inline or access properties directly (exception: 4+ property chains).
- **Brace placement**: Line breaks for namespaces/classes/methods; inline for control structures.
- **`this.` prefix**: Use for all instance members in Vala.
- **GLib prefix**: Use fully-qualified `GLib.*`; avoid `using` imports.
- **Property initialization**: Use `get; set; default =` or field defaults, not constructors.
- **Line length**: Break long method calls and concatenations for readability.
- **StringBuilder**: Only for loops with hundreds of iterations; use `string.joinv()` or `+` otherwise.
- **ArrayList for strings**: Use `string[]` and `string.joinv()`; avoid `Gee.ArrayList<string>` when building only to join.

## Status

⏳ **PENDING**

## Purpose

Enable searching for images by their content through text descriptions generated by vision models. This makes images, screenshots, diagrams, and UI mockups discoverable via codebase search.

## Problem Statement

Binary files (images, PDFs, etc.) are currently skipped entirely. There's no way to search for images or understand their content.

## Current Status

Binary files are currently skipped:
- `Indexer.index_file()` checks `if (!file.is_text)` and skips binary files
- Images, PDFs, and other binary files are not indexed
- No way to search for images or understand their content

## Proposed Image Analysis

**Goal**: Enable searching for images by their content through text descriptions.

**Approach**:
1. Detect image files (common extensions: `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.svg`, etc.)
2. Use vision model to analyze image and generate text description
3. Vectorize the text description (not the image itself)
4. Store image metadata with description in `vector_metadata`
5. Enable codebase search to find images by their content descriptions

## Phases overview

| Phase | Scope | Sign-off |
|-------|--------|----------|
| **Phase 1** | Chat support images (Message class + serialize_images + Chat) | Message extended; Chat sends base64 only at API send; session path-only |
| **Phase 2** | Shared analysis format + ImageAnalyzer | request_analysis(messages, usage) with required usage; analysis-prompt-image.txt; caller builds messages with images; connection not tested for null |
| **Phase 3** | Integration (Indexer, config, **docs/meson**) | Indexer calls index_image_file for every !file.is_text; ImageAnalyzer checks mimetype and returns "" for non-images; **docs/meson.build** updated |

**Note:** Generate (Call.Generate) image support is **not** in this plan (see **1.22**). Image description uses the **shared analysis call format**: base method `request_analysis(messages, usage)` with required usage model; caller builds messages and adds images; connection assumed validated active (do not test for null); template **analysis-prompt-image.txt**.

**Important**: When adding new `.vala` files (e.g. `ImageAnalyzer.vala`), **update `docs/meson.build`** so the new file is in the valadoc `input:` list. Do not forget this step.

**Pre-approved code only**: Every change in this plan has concrete code blocks or exact replacement snippets below. Implementers must not invent code: use only the snippets in the plan (or the existing codebase patterns they extend). Gaps that were filled so nothing is left to invent:
- **Phase 1.1** Message: `deserialize_property` for `"images"` — full code block added; `serialize_property` insertion point clarified (add case before `default:`).
- **Phase 2** VectorBase extended (optional usage model parameter); caller builds messages with images; analysis-prompt-image.txt; ImageAnalyzer in libocvector uses template + request_analysis(messages, tool_config.vision). **Phase 3** libocvector: add ImageAnalyzer.vala to libocvector (not libollmchat); docs/meson and libocvector/meson.build updated.

---

## Phase 1 — Chat support images

**Goal**: Update chat to support images via the Message class. Messages and session store **paths only**; base64 is used **only** when building the outgoing API request. **Generate is out of scope** (see plan 1.22).

**Deliverables**:
- Message: add `images` property (array of path strings, default empty); serialize/deserialize for session; add `serialize_images(Json.Object message_obj)` that writes base64 `images` onto the passed object at send time (all logic inside the method; no helper).
- Chat: when building the messages array for the request, for each user message call `m.serialize_images(msg_obj)` so the payload has base64 `images`.
- Session save/load: persist and restore `images` as paths only (no change if Message already serializes/deserializes the paths array).

### 1.1 Message class changes

- Add property: `public Gee.ArrayList<string> images { get; set; default = new Gee.ArrayList<string>(); }` (paths only).
- In `serialize_property`: for `"images"`, if `this.images.size == 0` return `null` (so empty array is omitted from session/API); otherwise serialize as JSON array of strings (paths).
- In `deserialize_property`: for `"images"`, parse JSON array of strings into `this.images` (see code below).

**Message.serialize_property** — add the following case **before** `default:` in the existing switch (return null when empty so key is omitted):

```vala
case "images":
    if (this.images.size == 0) {
        return null;
    }
    var arr = new Json.Array();
    for (int i = 0; i < this.images.size; i++) {
        arr.add_string_element(this.images.get(i));
    }
    var n = new Json.Node(Json.NodeType.ARRAY);
    n.init_array(arr);
    return n;
```

**Note:** We must serialize `images` (paths) when non-empty so the session can persist them. So we only return `null` when the array is empty (omit key); when non-empty we include the path array. That is why the serialized message object may have an `"images"` member when passed to `serialize_images`, which then replaces it with base64 for the API.

- Add method: `public void serialize_images(Json.Object message_obj)`: remove `"images"` from `message_obj`; for each path in `this.images` validate file exists and MIME is image, read and base64-encode; add only valid base64 strings to a new array; set `message_obj.set_member("images", node)` only when the array is non-empty. Does not mutate the Message; only the passed JSON object.

**Message method (the one on the message):** All logic inside `serialize_images`; no helper. Remove `"images"` from `message_obj`, then for each path in `this.images`: validate file exists and MIME is image, read and base64-encode, add to array; set `message_obj.set_member("images", node)` only when the array is non-empty.

```vala
/**
 * Writes base64-encoded images onto the serialized message object at send time.
 * Uses this.images (paths only). Updates message_obj in place; does not mutate the Message.
 * Validates: file exists, MIME type is image. Skips invalid paths.
 */
public void serialize_images(Json.Object message_obj)
{
    if (!message_obj.has_member("images")) {
        return;
    }
    message_obj.remove_member("images");
    var arr = new Json.Array();
    foreach (var path in this.images) {
        var file = GLib.File.new_for_path(path);
        if (!file.query_exists()) {
            continue;
        }
        string? content_type = null;
        try {
            var info = file.query_info(
                GLib.FileAttribute.STANDARD_CONTENT_TYPE,
                GLib.FileQueryInfoFlags.NONE,
                null
            );
            content_type = info.get_content_type();
        } catch (GLib.Error e) {
            continue;
        }
        if (content_type == null || !content_type.has_prefix("image/")) {
            continue;
        }
        uint8[] data;
        try {
            GLib.FileUtils.get_data(path, out data);
        } catch (GLib.Error e) {
            continue;
        }
        arr.add_string_element(GLib.Base64.encode(data));
    }
    if (arr.get_length() == 0) {
        return;
    }
    var n = new Json.Node(Json.NodeType.ARRAY);
    n.init_array(arr);
    message_obj.set_member("images", n);
}
```

**Message.deserialize_property** — handle `"images"` at the start of the method (before the existing `"tool-calls"` check). Restore paths from JSON array into `this.images`:

```vala
if (property_name == "images") {
    this.images.clear();
    var json_array = property_node.get_array();
    for (uint i = 0; i < json_array.get_length(); i++) {
        this.images.add(json_array.get_string_element(i));
    }
    value = Value(typeof(Gee.ArrayList));
    value.set_object(this.images);
    return true;
}
```

So the full deserialize_property begins with the above block, then continues with the existing `if (property_name != "tool-calls")` and tool_calls handling.

### 1.2 Chat: call serialize_images when building request

In `Call.Chat.serialize_property`, for the `"messages"` case, replace the loop body so that after serializing each message we call `serialize_images` on the JSON object for user messages, then add the node to the array.

**Change in `Call.Chat.serialize_property`, case `"messages"`:**

Replace:

```vala
				case "messages":
					// Serialize the message array built in send()
					var node = new Json.Node(Json.NodeType.ARRAY);
					node.init_array(new Json.Array());
					var array = node.get_array();
					foreach (var m in this.messages) {
						var msg_node = Json.gobject_serialize(m);
						array.add_element(msg_node);
					}
					return node;
```

with:

```vala
				case "messages":
					// Serialize the message array built in send()
					var node = new Json.Node(Json.NodeType.ARRAY);
					node.init_array(new Json.Array());
					var array = node.get_array();
					foreach (var m in this.messages) {
						var msg_node = Json.gobject_serialize(m);
						m.serialize_images(msg_node.get_object());
						array.add_element(msg_node);
					}
					return node;
```

`serialize_images` no-ops when `this.images.size == 0`, so the payload gets base64 `images` only when a message has images; stored messages and session stay path-only.

### 1.3 Session save/load

- **Save**: Message serializes `images` as array of path strings; session stores paths only.
- **Load**: Message deserialize_property for `"images"` restores paths into `this.images`. Optional: validate paths on load (warn or drop if missing).

**Phase 1 sign-off**: Message class extended; Chat sends base64 only at API send time; session and in-memory messages keep paths only. Generate image support is in plan 1.22.

---

## Phase 2 — Shared analysis call format and ImageAnalyzer

**Goal**: Extend the base analysis method to accept an optional usage model (ModelUsage) so that **all** analysis (code, docs, folder, project, **image**) share the same call format. Add a simple template **analysis-prompt-image.txt** and an ImageAnalyzer that uses it via `request_analysis(messages, tool_config.vision)`.

**Deliverables**:
- **VectorBase**: Extend `request_analysis(messages, usage)` with required `usage` (ModelUsage, not null). Caller passes usage (e.g. tool_config.analysis or tool_config.vision); connection assumed already validated active, do not test for null. Existing callers (DocumentationAnalysis, Analysis, FolderAnalysis, ProjectAnalysis) must be updated to pass tool_config.analysis.
- **analysis-prompt-image.txt**: New template (same `---` format as other analysis prompts); add to gresources.
- **ImageAnalyzer**: In **libocvector** (not libollmchat), extends VectorBase; uses PromptTemplate("analysis-prompt-image.txt"), builds messages and adds the image path to the user message’s `images`, then `request_analysis(messages, tool_config.vision)` so it shares the same call format as other analysis.

### 2.1 VectorBase — extend request_analysis() to accept usage model

No change to `connection()`: when the caller passes a usage model (e.g. `tool_config.vision`), `request_analysis` gets the connection from `this.config.connections.get(usage.connection)`.

**VectorBase.request_analysis()** — extend signature and behaviour:

- Add parameter: `OLLMchat.Settings.ModelUsage usage` (required, not nullable). Caller passes the usage model (e.g. `tool_config.analysis` or `tool_config.vision`). Do not test connection or model usage — just use them (already validated active).
- **Caller builds messages**: When the caller wants to send images, it adds paths to the user message’s `images` list before calling `request_analysis`. No `image_paths` parameter; `request_analysis` just sends the messages. Chat will call Message.serialize_images when building the request.

**New signature and logic (replace existing request_analysis):**

```vala
		protected async string request_analysis(
			Gee.ArrayList<OLLMchat.Message> messages,
			OLLMchat.Settings.ModelUsage usage
		) throws GLib.Error
		{
			var conn = this.config.connections.get(usage.connection);
			var result = "";
			const int MAX_RETRIES = 2;
			for (int attempt = 0; attempt <= MAX_RETRIES; attempt++) {
				try {
					var chat = new OLLMchat.Call.Chat(conn, usage.model) { stream = false, options = usage.options };
					OLLMchat.Response.Chat? response = yield chat.send(messages, null);
					if (response == null || response.message == null) { if (attempt < MAX_RETRIES) continue; return ""; }
					result = response.message.content.strip();
					if (result != "") return result;
					if (attempt < MAX_RETRIES) continue;
					return "";
				} catch (GLib.Error e) {
					if (attempt < MAX_RETRIES) continue;
					throw e;
				}
			}
			return result;
		}
```

Caller always passes the usage model (e.g. `tool_config.analysis` or `tool_config.vision`). Connection is assumed already validated active; do not test for null. Existing callers (DocumentationAnalysis, Analysis, FolderAnalysis, ProjectAnalysis) must be updated to pass `tool_config.analysis`.

### 2.2 analysis-prompt-image.txt template

Create **resources/ocvector/analysis-prompt-image.txt** with the same `---` separator format as other analysis prompts. Expand like the other analysis prompts: system part sets role and rules; user part asks for the description.

**File: resources/ocvector/analysis-prompt-image.txt**

```
You are a helpful assistant that categorizes and describes images for semantic search indexing.

Your task is to produce a SHORT, ONE-LINE description that helps users find the image by content. Consider what kind of image it is and what it shows.

CRITICAL REQUIREMENTS:
- ONE LINE ONLY - do not write multiple sentences or paragraphs
- SHORT - keep it brief and concise (ideally under 25 words)
- NO MARKDOWN - no formatting, no code blocks, no special characters
- NO EXPLANATIONS - just the description text, nothing else

Consider likely types of images in a codebase or project:
- Icon, logo, or small graphic
- Screenshot (UI, terminal, app window, error dialog)
- Diagram (architecture, flow, UML, network)
- UI mockup or wireframe
- Photo or illustration
- Chart or graph
- Other (describe the main subject and any visible text or salient detail)

Include any visible text, labels, or key elements that would help someone find this image by search. Be specific about what is shown, not generic.

Return ONLY the one-line description - no prefix, no category label unless it helps, no markdown, no additional text.
---
Describe this image in one short line for search indexing. Say what kind of image it is (e.g. icon, screenshot, diagram) and what it shows, including any visible text or key elements.
```

**resources/gresources.xml** — In the `/ocvector` gresource, add: `<file>analysis-prompt-image.txt</file>`

### 2.3 ImageAnalyzer (libocvector) — same call format

ImageAnalyzer lives in **libocvector** (e.g. `libocvector/Indexing/ImageAnalyzer.vala`) and **extends VectorBase**. It uses PromptTemplate("analysis-prompt-image.txt"), builds messages and adds the image path to the user message’s `images`, then `request_analysis(messages, tool_config.vision)`. Constructor takes Config2 (from VectorBase). At start of `describe_image`: check `tool_config.vision.is_valid` (return "" if not); then query file info and check mimetype `image/*` (return "" if not an image). So the indexer can call `index_image_file` for every non-text file and the analyzer rejects non-images by returning "" — no need for `is_image` on File. Config code ensures codebase_search tool_config is present; do not test tool_config for null.

**Class shape (libocvector/Indexing/ImageAnalyzer.vala):**

```vala
namespace OLLMvector.Indexing
{
    public class ImageAnalyzer : VectorBase
    {
        private static PromptTemplate? cached_image_template = null;

        static construct
        {
            try {
                cached_image_template = new PromptTemplate("analysis-prompt-image.txt");
                cached_image_template.load();
            } catch (GLib.Error e) {
                GLib.critical("Failed to load image prompt template in static constructor: %s", e.message);
            }
        }

        public ImageAnalyzer(OLLMchat.Settings.Config2 config) {
            base(config);
        }

        public async string describe_image(OLLMfiles.File file) throws GLib.Error
        {
            var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
            if (!tool_config.vision.is_valid) {
                return "";
            }
            var gfile = GLib.File.new_for_path(file.path);
            if (!gfile.query_exists()) {
                return "";
            }
            var content_type = gfile.query_info(
                GLib.FileAttribute.STANDARD_CONTENT_TYPE,
                GLib.FileQueryInfoFlags.NONE,
                null
            ).get_content_type();
            if (content_type == null || !content_type.has_prefix("image/")) {
                return "";
            }
            var messages = new Gee.ArrayList<OLLMchat.Message>();
            // System message from our template (we control it); same pattern as Analysis/DocumentationAnalysis.
			messages.add(new OLLMchat.Message("system", cached_image_template.system_message));
            var user_msg = new OLLMchat.Message("user", cached_image_template.fill());
            user_msg.images.add(file.path);
            messages.add(user_msg);
            return yield this.request_analysis(messages, tool_config.vision);
        }
    }
}
```

**libocvector/meson.build** — Add `Indexing/ImageAnalyzer.vala` to the library sources. **docs/meson.build** — Add `../libocvector/Indexing/ImageAnalyzer.vala` to the valadoc input list in the libocvector section (see Phase 3).

**Phase 2 sign-off**: VectorBase extended (optional usage model parameter); analysis-prompt-image.txt added; caller builds messages with images; ImageAnalyzer in libocvector uses template + request_analysis(messages, tool_config.vision) so all analysis share the same call format.

---

## Phase 3 — Integration

**Approval before implementation:** This plan is for review and approval before any code changes. No implementation is done until the plan is approved. Sections below include **concrete code blocks** (e.g. §3.3 VectorBuilder.add_single, §3.4 index_image_file) — those are the exact changes to be made; approve the plan to proceed.

**Goal**: Wire image detection and ImageAnalyzer into the indexer; add config; **update docs/meson** for any new source files.

**Deliverables**:
- **Indexer**: In `index_file`, after the `if (!file.is_text)` branch: call `index_image_file(file, force)` for every non-text file. ImageAnalyzer.describe_image checks mimetype at start and returns "" for non-images, so no need for `is_image` on File.
- **index_image_file(file, force)**: Incremental check (mtime vs last_vector_scan); create ImageAnalyzer(config), `yield analyzer.describe_image(file)` (analyzer returns "" when vision invalid or file not an image); if description non-empty, VectorBuilder.add_single(…), update last_vector_scan. Config is set up by the application; do not test for codebase_search or vision in Indexer.
- **Config**: Add a new **model usage** for the vision model (optional; can default). Not required—when missing or when `model_usage.is_valid` is false, image analysis is skipped. Add to the appropriate tool config (e.g. CodebaseSearchToolConfig) as a `ModelUsage` (e.g. `vision_model_usage`).
- **docs/meson**: For **any** new `.vala` file added in this plan (e.g. `ImageAnalyzer.vala`, or new files under `libocvector/Indexing/`), **add that file to the `input:` list in `docs/meson.build`** in the correct order so valadoc builds. Do not forget this step.

### 3.1 CodebaseSearchToolConfig — add vision ModelUsage

Add optional vision model usage (not in `required_models()`; when missing or invalid, image analysis is skipped).

**New property and setup_defaults change:**

```vala
/**
 * Vision model configuration (connection, model, options).
 * Optional. When not set or is_valid is false, image analysis is skipped during indexing.
 */
[Description(nick = "Vision Model", blurb = "Model used for describing images during indexing (default llama3.2-vision:latest). Optional; when invalid, image analysis is skipped.")]
public OLLMchat.Settings.ModelUsage vision { get; set; default = new OLLMchat.Settings.ModelUsage(); }
```

In `setup_defaults(string connection_url)`, append (optional; can leave vision unset so it stays invalid):

```vala
this.vision = new OLLMchat.Settings.ModelUsage() {
    connection = connection_url,
    model = "llama3.2-vision:latest"
};
this.vision.options = new OLLMchat.Call.Options() {
    temperature = 0.0
};
```

### 3.2 Indexer — index_file() branch

**Replace** the block:

```vala
			// Only index text files
			if (!file.is_text) {
				GLib.debug("Skipping file '%s' (not a text file, is_text=false)", file.path);
				return false;
			}
			
			// Route to appropriate pipeline
```

**with**:

```vala
			// Only index text files or supported binary (e.g. images)
			if (!file.is_text) {
				return yield this.index_image_file(file, force);
			}
			
			// Route to appropriate pipeline
```

### 3.3 VectorBuilder — add_single() (reuse embed + metadata path)

**Element type:** Our element_type values are fixed (e.g. "image", "file", "folder", "section", "project") — no quotes in values, so no escaping needed in the WHERE clause.

**Generic use:** add_single is a generic "one description, one VectorMetadata" path. It could be reused elsewhere in VectorBuilder for file-level summaries (element_type="file"), folder summaries ("folder"), or project summaries ("project") if those code paths are refactored to call it instead of inlining embed + metadata logic.

**Why this lives in VectorBuilder, not Indexer:**

- Heavy work lives in other classes (same as `index_folder` / `index_project`).
- `index_folder` only loops and calls `index_file`; `index_project` only calls `ProjectAnalysis.analyze`.
- Code/docs size is in **index_code_file** / **index_documentation_file**; they call Tree → Analysis → **VectorBuilder.process_file()**.
- Images: one description → add a single-element path on VectorBuilder; Indexer stays thin.
- Embed + metadata logic stays in VectorBuilder (one place).

**In `libocvector/Indexing/VectorBuilder.vala`:** Add the following async method (concrete code for approval — no implementation until plan is approved):

```vala
/**
 * Embed a single description and store one VectorMetadata (e.g. for image files).
 * Deletes existing rows for file_id + element_type, then same embed path as process_file.
 */
public async void add_single(OLLMfiles.File file, string element_type, string element_name, string description) throws GLib.Error
{
	var existing = new Gee.ArrayList<VectorMetadata>();
	VectorMetadata.query(this.sql_db).select(
		"WHERE file_id = %lld AND element_type = '%s'".printf(file.id, element_type),
		existing
	);
	foreach (var r in existing) {
		VectorMetadata.query(this.sql_db).deleteId(r.id);
	}
	var documents = new Gee.ArrayList<string>();
	documents.add(description);
	var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
	var embed_conn = this.config.connections.get(tool_config.embed.connection);
	var embed_response = yield new OLLMchat.Client(embed_conn).embed_array(
		tool_config.embed.model, documents, -1, false, tool_config.embed.options
	);
	if (embed_response.embeddings.size == 0) { return; }
	var vector_batch = OLLMvector.FloatArray(this.database.dimension);
	vector_batch.add(this.embed_to_floats(embed_response.embeddings[0]));
	int64 vector_id = (int64)this.database.vector_count;
	this.database.add_vectors_batch(vector_batch);
	var meta = new VectorMetadata() {
		file_id = file.id, start_line = 0, end_line = 0,
		element_type = element_type, element_name = element_name,
		description = description, ast_path = "", vector_id = vector_id
	};
	meta.saveToDB(this.sql_db, false);
}
```

### 3.4 Indexer — index_image_file()

**Why this is larger than index_folder/index_project:**

- `index_folder` and `index_project` are **folder-level** (loop over files, or call ProjectAnalysis).
- **index_image_file** is a **file-level** pipeline like **index_code_file** and **index_documentation_file** (~95–100 lines each). So the fair comparison is those two, not index_folder.
- This method is kept shorter (~25 lines of logic) by delegating embed+metadata to **VectorBuilder.add_single()** and omitting the deleted-file check blocks used in index_code_file/index_documentation_file (add them later if needed for consistency).

**Config:** Config is set up by the application. Do not test for codebase_search or vision in Indexer. ImageAnalyzer.describe_image returns "" when vision invalid or file not an image.

**In `libocvector/Indexing/Indexer.vala`** (after `index_documentation_file`): Add the following method (concrete code for approval — no implementation until plan is approved):

```vala
		private async bool index_image_file(OLLMfiles.File file, bool force = false) throws GLib.Error
		{
			if (!force) {
				var mtime = file.mtime_on_disk();
				if (file.last_vector_scan >= mtime && mtime > 0) {
					GLib.debug("Skipping image file '%s' (not modified since last scan)", file.path);
					return false;
				}
			}
			// Config is set up by the application; do not test for codebase_search or vision here. ImageAnalyzer returns "" when vision invalid or file not an image.
			var analyzer = new OLLMvector.Indexing.ImageAnalyzer(this.config);
			var description = yield analyzer.describe_image(file);
			if (description == "") {
				file.last_vector_scan = new DateTime.now_local().to_unix();
				file.saveToDB(this.sql_db, null, false);
				return true;
			}
			var vector_builder = new VectorBuilder(this.config, this.vector_db, this.sql_db);
			yield vector_builder.add_single(file, "image", GLib.Path.get_basename(file.path), description);
			file.last_vector_scan = new DateTime.now_local().to_unix();
			file.saveToDB(this.sql_db, null, false);
			try {
				this.vector_db.save_index();
			} catch (GLib.Error e) {
				GLib.warning("Failed to save vector database after image '%s': %s", file.path, e.message);
			}
			GLib.debug("Completed indexing image file '%s'", file.path);
			return true;
		}
```

### 3.5 docs/meson.build — ImageAnalyzer.vala

Add `ImageAnalyzer.vala` to the **libocvector** section of the `input:` list (with other Indexing files). In `docs/meson.build`, in the libocvector block, add after the other Indexing files (e.g. after `../libocvector/Indexing/Indexer.vala',`):

`'../libocvector/Indexing/ImageAnalyzer.vala',`

**Phase 3 sign-off**: Indexer calls index_image_file for every non-text file; ImageAnalyzer validates mimetype and returns "" for non-images; config and docs/meson updated.

---

## Implementation Details (reference)

### Image Detection

- Check file extension against known image formats
- Could use `file.is_text == false` and check MIME type or extension
- Common formats: PNG, JPEG, GIF, WebP, SVG, BMP, TIFF

### Image Analysis Process

1. Load image file (read binary data)
2. Send image to vision model (e.g., via Ollama vision API or similar)
3. Generate text description of image content
4. Store description in `vector_metadata.description`
5. Vectorize description text (create embedding)
6. Store in FAISS with `vector_id` and metadata

### Storage

- `element_type = 'image'` (or `'binary'` for other binary files)
- `file_id` references the image file
- `start_line = 0, end_line = 0` (no line numbers for images)
- `description` contains text description of image content
- `vector_id` points to vectorized description embedding
- `ast_path = ''` (images don't have AST paths)

### Vision Model Integration

- Use Ollama vision models (e.g., `llava`, `llama3.2-vision`, `bakllava`) or similar
- Send image as base64 or file path to vision API
- Prompt: "Describe this image in detail. Include any text, objects, diagrams, screenshots, or visual elements."
- Get text response describing the image
- This text description is what gets vectorized

---

## Chat/Vision Query Support

Enable sending images in chat messages so vision models can analyze them (e.g. for codebase/project analysis). The Ollama chat API expects an `images` array of base64-encoded strings per message.

**Scope (current)**: We are only using this for **analysis** at present (e.g. analysis tools sending images to the vision model). **UI support** (user attaching images in chat, displaying images/thumbnails in the UI) is **later** and out of scope for this plan.

### Vision Query API Format (Ollama)

```bash
curl http://localhost:11434/api/chat -d '{
  "model": "llama3.2-vision",
  "messages": [
    {
      "role": "user",
      "content": "what is in this image?",
      "images": ["<base64-encoded image data>"]
    }
  ]
}'
```

- Each user message can include an `images` array: list of base64-encoded image strings.
- When sending to Ollama we must supply base64 data in `images`; file paths are not accepted by the API.

### Requirements

#### 1. Support for `images` (array of paths)

- Extend our message/chat model to carry an optional `images` field per message; **default to an empty array/list** when absent.
- **Use paths only** in our app: type is array/list of file path strings. We never store base64 in the message model or session.
- Only user messages can have images; assistant messages do not.

#### 2. Session save (paths only)

- When persisting a session (save to disk/DB), store `images` as array of file paths.
- No base64 in session; paths only. Session stays small.

#### 3. Session load (paths only)

- When loading a session, restore `images` as array of paths.
- No conversion to base64 at load time. (UI display/thumbnails are later; not in scope here.)
- Validate paths at load (optional): warn or drop if files are missing (e.g. moved/deleted).

#### 4. Serialize images only when sending the chat

- Convert paths → base64 **only** when sending the chat request to the API.
- Method on the message: `serialize_images(json_message)`:
  - **Input**: only the JSON object of the serialized message (payload for that message).
  - **Access**: message’s `images` array (paths); no separate paths argument.
  - **Does not mutate** the message; paths stay in the stored message.
- Behaviour:
  - **Remove** any existing `images` key from the serialized message JSON first.
  - For each path in the message’s `images` array:
    - Validate: file exists, **MIME type** is image.
    - Skip if not found or not an image.
    - Read and base64-encode only valid images.
  - Add `images` on the JSON only when at least one valid image.
  - **No return value needed**; method just updates the passed object in place. (If returning: **false** when no images to add, **true** when ok.)
- Path and image validation (this method does all of it):
  - Skip file not found.
  - Skip file not an image (validate **MIME type**).
  - No separate error path—just skip.
- Chat layer:
  - Calls `serialize_images(json_message)` on **all** messages before sending (passing each message’s serialized JSON).
  - Method updates the passed JSON in place; when no valid images, `images` key is simply absent.
  - In-memory and stored messages keep paths only.

#### 5. Summary

| Where              | What we use                    |
|--------------------|--------------------------------|
| Message model      | `images`: array of paths only  |
| Session (DB/disk)  | `images`: array of paths only  |
| Outgoing API call  | **Only here**: paths → base64 for `images` (at send time only) |
| Display / UI       | **Later** (out of scope for now; analysis only) |

- **Normal flow**: paths everywhere. No base64 in our data.
- **Send chat**: when building the request, run the serialize step so the payload has base64 `images`; session and in-memory messages keep paths only.

### Vectorization

- The text description is treated like any other document
- Create embedding from description text
- Store in FAISS index
- Searchable via codebase search tool

### Example Flow

```
1. Detect: image.png (binary file, extension .png)
2. Load image data
3. Call vision model: "Describe this image"
4. Get response: "Screenshot of a login form with username and password fields, submit button, and company logo in header"
5. Create VectorMetadata:
   - element_type = 'image'
   - element_name = 'image.png'
   - description = "Screenshot of a login form..."
6. Vectorize description text
7. Store in FAISS and metadata table
```

### Search Results

- When searching for "login form", image description matches
- Search result includes image file path
- User can see what images contain without opening them

## Other Binary Files

**PDFs**:
- PDF extraction using **libpoppler** is **outside scope** for this plan.
- See **5.4-pdf-extraction.md**: convert PDF to image(s) via libpoppler, then run through the same image pipeline (vision model) as standard images.

**Other Formats**:
- Consider on case-by-case basis
- Priority: Images (most common, most useful)
- Future: Office documents, etc.

## Implementation Requirements

### New Components

1. `ImageAnalyzer` class:
   - Detects image files
   - Loads image data
   - Calls vision model API
   - Returns text description

2. `Indexer.index_binary_file()` method:
   - Handles binary files (images, PDFs, etc.)
   - Routes to appropriate analyzer (image, PDF, etc.)
   - Creates VectorMetadata entries
   - Vectorizes descriptions

3. Vision model integration:
   - Add vision API support to `OLLMchat.Client`
   - Or use existing image analysis tools if available
   - Configure vision model in tool config

### Configuration

- Add `vision` (ModelUsage) to CodebaseSearchToolConfig (model usage from config)
- Specify which vision model to use for image analysis (via vision.model)
- Enable/disable binary file indexing

### Modified Flow

```
Indexer.index_file():
  if (file.is_text):
    → Normal text file indexing (existing)
  else:
    → index_image_file (ImageAnalyzer checks mimetype; returns "" for non-images, so no separate skip branch)
```

## Benefits

- Images become searchable by content
- Screenshots, diagrams, UI mockups can be found via description
- Documentation images (screenshots, diagrams) are discoverable
- Better project understanding through visual content

## Deliverables (by phase)

### Phase 1 — Chat support images

- [ ] Add `images` property to Message (array of path strings, **default empty**); serialize/deserialize for session (paths only)
- [ ] Add `serialize_images(Json.Object message_obj)` on Message: all logic inside method (no helper); updates passed object in place with base64 `images`; does not mutate Message
- [ ] Chat: when building messages array for request, call `m.serialize_images(msg_obj)` for each user message
- [ ] Session save/load: persist and restore `images` as paths only (no base64 in session)
- [ ] **Generate image support**: not in this plan; see **1.22** (consider Message as argument for Generate to simplify code like images)

### Phase 2 — Shared analysis call format and ImageAnalyzer

- [ ] **VectorBase**: Extend `request_analysis(messages, usage)` — required ModelUsage; get connection from config.connections.get(usage.connection), do not test for null (already validated active); existing callers pass tool_config.analysis
- [ ] Create **analysis-prompt-image.txt** (same `---` format); add to `resources/gresources.xml` (ocvector)
- [ ] Create `ImageAnalyzer` in **libocvector/Indexing/** (extends VectorBase): constructor takes Config2; uses PromptTemplate("analysis-prompt-image.txt"), builds messages and adds image path to user message’s images, then `request_analysis(messages, tool_config.vision)`; check tool_config.vision.is_valid at start of describe_image
- [ ] **libocvector/meson.build**: Add `Indexing/ImageAnalyzer.vala`; **docs/meson.build**: Add to libocvector section (Phase 3)

### Phase 3 — Integration

- [ ] Indexer: in `index_file`, add branch for image files → `index_image_file(file, force)`
- [ ] Indexer: when !file.is_text call index_image_file(file); ImageAnalyzer checks mimetype at start (no is_image on File)
- [ ] Add `index_image_file(file, force)`: optional incremental check; get ImageAnalyzer from config; describe → vectorize → save
- [ ] Add vision **model usage** config (optional, can default); Indexer/analyzer checks `model_usage.is_valid` and skips image analysis when not valid
- [ ] **docs/meson**: For **every** new `.vala` file in this plan, add the file to the `input:` list in `docs/meson.build` in the correct order (dependencies first). Do not forget.

### Chat/vision (covered in Phase 1 deliverables above)

- See Phase 1 checklist; serialize_images and Chat wiring are Phase 1 items.
- (Phase 1) Chat calls `serialize_images(json_message)` on all messages before sending (passing each message’s serialized JSON); method updates JSON in place (`images` absent when no valid images); session and messages keep paths only

## Related Plans

- 2.20-codebase-scanner-improvements.md - Parent plan
- 2.10-codebase-search-tool.md - Existing codebase search infrastructure
- **1.22-generate-message-argument.md** - Add image support to Generate; consider using Message as the argument for Generate to simplify code (e.g. images via Message.serialize_images)
