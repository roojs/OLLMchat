# 1.17. Migrate to v1/model API Endpoint

## Overview

Migrate from the `/api/tags` endpoint to the `/api/v1/model` endpoint for fetching the list of available models. **The primary reason for this migration is that `/api/tags` is not portable** - it's an Ollama-specific endpoint that may not be available on other OpenAI-compatible servers. The `/api/v1/model` endpoint follows the OpenAI API standard and provides better compatibility across different server implementations.

**Parent Plan**: None (standalone migration)

## Rationale

The `/api/tags` endpoint is **not portable** because:
- It's Ollama-specific and not part of the OpenAI API standard
- Other OpenAI-compatible servers (OpenRouter, Together AI, etc.) may not implement this endpoint
- The `/api/v1/model` endpoint follows the OpenAI API standard (`/v1/models`), making it portable across different server implementations
- Better compatibility ensures the application works with a wider range of LLM server backends

By migrating to `/api/v1/model`, we ensure the application can work with any OpenAI-compatible server, not just Ollama.

## Status

✅ **DONE** - Implemented

## Goal

1. Create a new `Call.Tags` class that uses the `/api/tags` endpoint (move current `Call.Models` code there)
2. Update `Call.Models` to use the `/api/v1/model` endpoint with the new response format
3. Update response parsing to handle the new structure: `{"object":"list","data":[...]}`
4. Update `Response.Model` to handle new fields: `id`, `object`, `created`, `owned_by`
5. Sync `name` from `id` in caller post-processing (`Call.Models.exec_models()`) after standard deserialize

## Current State

### Current Implementation (`Call.Models`)

- **Endpoint**: `tags` (maps to `/api/tags`)
- **HTTP Method**: `GET`
- **Expected Response**: `{"models": [...]}`
- **Parsing**: Uses `get_models("models")` which expects a `models` field in the response

**File**: `libollmchat/Call/Models.vala`
```vala
public class Models : Base
{
    public Models(Settings.Connection connection)
    {
        base(connection);
        this.url_endpoint = "tags";
        this.http_method = "GET";
    }

    public async Gee.ArrayList<Response.Model> exec_models() throws Error
    {
        return yield this.get_models("models");
    }
}
```

### API Response Comparison

#### `/api/tags` Response (Current - Ollama-specific)

**Response Structure**:
```json
{
  "models": [
    {
      "name": "qwen3-next:80b",
      "size": 1234567890,
      "digest": "sha256:abc123...",
      "modified_at": "2025-01-12T04:45:39.123456789Z"
    },
    {
      "name": "deepseek-llm:67b",
      "size": 9876543210,
      "digest": "sha256:def456...",
      "modified_at": "2025-01-11T12:30:37.123456789Z"
    },
    ...
  ]
}
```

**Fields in `/api/tags` response**:
- `name` - Model identifier (string)
- `size` - Model size in bytes (int64)
- `digest` - Model digest/hash (string, e.g., "sha256:...")
- `modified_at` - Last modified timestamp (ISO 8601 format string)

#### `/api/v1/model` Response (New - OpenAI-compatible)

**Endpoint**: `api/v1/model` (note: singular "model", not "models")

**Response Structure**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "qwen3-next:80b",
      "object": "model",
      "created": 1767859539,
      "owned_by": "library"
    },
    {
      "id": "deepseek-llm:67b",
      "object": "model",
      "created": 1767853437,
      "owned_by": "library"
    },
    ...
  ]
}
```

**Fields in `/api/v1/model` response**:
- `id` - Model identifier (string) - **caller syncs to `name` after deserialize (Models post-process)**
- `object` - Object type, always "model" (string)
- `created` - Creation timestamp (Unix epoch integer)
- `owned_by` - Owner identifier (string, e.g., "library", "roojs", "user")

### Key Differences Between Endpoints

| Field | `/api/tags` | `/api/v1/model` | Notes |
|-------|-------------|-----------------|-------|
| **Model identifier** | `name` | `id` | Caller post-process: sync `name` from `id` after deserialize |
| **Size** | `size` (bytes, int64) | ❌ Not provided | Only available from `show_model()` |
| **Digest** | `digest` (string) | ❌ Not provided | Only available from `show_model()` |
| **Timestamp** | `modified_at` (ISO 8601 string) | `created` (Unix epoch int) | Different field name and format |
| **Object type** | ❌ Not provided | `object` (always "model") | New field |
| **Owner** | ❌ Not provided | `owned_by` (string) | New field |
| **Response wrapper** | `{"models": [...]}` | `{"object": "list", "data": [...]}` | Different structure |

**Important Notes**:
- `/api/tags` provides `size` and `digest` directly in the list response
- `/api/v1/model` does NOT provide `size` or `digest` - these must be fetched via `show_model()` for each model
- Timestamp formats differ: ISO 8601 string vs Unix epoch integer
- Field name differs: `name` vs `id` (must be mapped during parsing)

## Implementation Steps

### Step 1: Create `Call.Tags` Class

**File**: `libollmchat/Call/Tags.vala` (new file)

Move the current `Call.Models` implementation to `Call.Tags`:

```vala
namespace OLLMchat.Call
{
    /**
     * API call to list available models using the /api/tags endpoint (Ollama-specific).
     *
     * This endpoint is Ollama-specific and not part of the OpenAI API standard.
     * For new code, use Call.Models which uses /api/v1/model (OpenAI-compatible).
     *
     * @deprecated Use Call.Models instead
     */
    public class Tags : Base
    {
        public Tags(Settings.Connection connection)
        {
            base(connection);
            this.url_endpoint = "tags";
            this.http_method = "GET";
        }

        public async Gee.ArrayList<Response.Model> exec_models() throws Error
        {
            return yield this.get_models("models");
        }
    }
}
```

**`libollmchat/meson.build`** — add `Call/Tags.vala` after `Call/Models.vala` in `ollmchat_ollama_src`:

```meson
  'Call/Models.vala',
  'Call/Tags.vala',
  'Call/Options.vala',
```

**Tasks**:
- [x] Create `libollmchat/Call/Tags.vala`
- [x] Copy current `Call.Models` implementation to `Call.Tags`
- [x] Add deprecation notice in documentation
- [x] Update `libollmchat/meson.build` to include `Call/Tags.vala` in the `ollmchat_ollama_src` files list (add after `Call/Models.vala`)

### Step 2: Update `Call.Models` to Use New Endpoint

**File**: `libollmchat/Call/Models.vala`

Replace the file with the following (endpoint + response field + post-process from Step 3). No code is written that isn’t in this block:

```vala
namespace OLLMchat.Call
{
    /**
     * API call to list available models (OpenAI-compatible).
     *
     * Uses the /api/v1/model endpoint, which is part of the OpenAI API standard
     * and works with Ollama and other OpenAI-compatible servers. Returns a
     * standardized response format with model metadata.
     *
     * Retrieves a list of all models that are available for use.
     */
    public class Models : Base
    {
        public Models(Settings.Connection connection)
        {
            base(connection);
            this.url_endpoint = "api/v1/model";
            this.http_method = "GET";
        }

        public async Gee.ArrayList<Response.Model> exec_models() throws Error
        {
            var list = yield this.get_models("data");
            foreach (var m in list) {
                if (m.name == "" && m.id != "") {
                    m.name = m.id;
                }
            }
            return list;
        }
    }
}
```

**Tasks**:
- [x] Replace `Call.Models` with the code above (url_endpoint, get_models("data"), post-process name from id, and OpenAI-compatible class doc)

### Step 2b: Update Client.models() documentation

**File**: `libollmchat/Client.vala`

Update the `models()` method doc comment so it describes the OpenAI-compatible endpoint instead of the old Ollama-specific one:

```vala
/**
 * Lists all available models (OpenAI-compatible).
 *
 * Retrieves a list of all models that are available for use on the server.
 * This calls the /api/v1/model endpoint (OpenAI API standard), which works
 * with Ollama and other OpenAI-compatible servers.
 *
 * @return ArrayList of Response.Model objects representing available models
 * @throws Error if the request fails or response is invalid
 * @since 1.0
 */
public async Gee.ArrayList<Response.Model> models() throws Error
```

**Tasks**:
- [x] Replace the existing `models()` doc comment and keep the method body unchanged

### Step 3: Update Response.Model and Use Standard Deserialize + Caller Post-Processing

**Approach**: Use standard JSON deserialization in `parse_models_array()` with no type-specific logic. Each **caller** post-processes the returned list after a single deserialize to sync `name` (e.g. from `id` or `model`) so data is consistent.

**File**: `libollmchat/Response/Model.vala`

The new v1 API returns fields that standard deserialization can fill if we add matching properties:

- `id` → add property so it deserializes; callers sync to `name` when needed
- `object` → new field (can be stored but may not be needed)
- `created` → timestamp (Unix epoch)
- `owned_by` → owner information (e.g., "library", "roojs", etc.)

**Tasks**:
- [ ] Add `id` property (string, default: "") so JSON `"id"` deserializes from v1/model response
- [ ] Add `object` property (string, default: "")
- [ ] Add `created` property (int64, default: 0) - Unix timestamp
- [ ] Add `owned_by` property (string, default: "")

**1. New properties in `libollmchat/Response/Model.vala`** (add after existing `name`, before `modified_at`):

```vala
/**
 * Model identifier from v1/model list API (JSON "id").
 * Callers sync to name when needed (e.g. Call.Models post-process).
 */
public string id { get; set; default = ""; }

/**
 * Object type from v1/model API (e.g. "model").
 */
public string object { get; set; default = ""; }

/**
 * Creation timestamp (Unix epoch) from v1/model API.
 */
public int64 created { get; set; default = 0; }

/**
 * Owner from v1/model API (e.g. "library", "roojs").
 */
public string owned_by { get; set; default = ""; }
```

**2. Simplified `parse_models_array()` in `libollmchat/Call/Base.vala`** (standard deserialize only; remove the `if (this is Ps)` block). Deserialize directly from the JSON node using `Json.gobject_deserialize()`, not the round-trip via Generator + string + `gobject_from_data`:

```vala
protected Gee.ArrayList<Response.Model> parse_models_array(Json.Array array)
{
    var items = new Gee.ArrayList<Response.Model>((a, b) => {
        return a.name == b.name;
    });

    for (int i = 0; i < array.get_length(); i++) {
        var item_obj = Json.gobject_deserialize(
				typeof(Response.Model), array.get_element(i)) as Response.Model;
        if (item_obj == null) {
            continue;
        }
        items.add(item_obj);
    }

    return items;
}
```

**3. Call.Models** — post-process is already in the Step 2 code block (exec_models() with get_models("data") and name-from-id loop).

**4. Caller post-processing in `Call.Ps.exec_models()`** (`libollmchat/Call/Ps.vala`):

```vala
public async Gee.ArrayList<Response.Model> exec_models() throws Error
{
    var list = yield this.get_models("models");
    foreach (var m in list) {
        m.name = m.model;
    }
    return list;
}
```

**Tasks**:
- [x] Add `id`, `object`, `created`, `owned_by` to `Response.Model` (insert block 1 after `name`, before `modified_at`). No changes to `serialize_property`/`deserialize_property` — default handler covers the new properties.
- [x] In `Call.Base`: replace `parse_models_array()` with block 2 (remove `if (this is Ps)` block).
- [x] In `Call.Ps.exec_models()`: replace with block 4 (get list then post-process to set `name = model`).

**Concrete code coverage**: Every line of code that would be written is in this plan. Step 1: full `Call/Tags.vala` and meson snippet. Step 2: full `Call/Models.vala` (including post-process). Step 3: four properties to insert in `Response.Model`, full `parse_models_array()` and full `Call.Ps.exec_models()`. No other code is required.

### Step 4: Update All Usages

**No call-site changes required.** All callers already use `Call.Models` and `exec_models()`; the public API (constructor, `exec_models()` → `Gee.ArrayList<Response.Model>`) is unchanged. Call sites in `Client.vala`, `Config2.vala`, `ConnectionsPage.vala`, `ConnectionAdd.vala`, `MainDialog.vala`, `ollmchat-cli.vala`, `oc-test-cli.vala`, and the examples continue to work as-is.

**Files to check** (verify only; no code changes):
- `libollmchat/Client.vala` - `models()` method (uses `Call.Models`)
- `libollmchat/Settings/Connection.vala` - `load_models()` (uses `client.models()`)
- `libollmchat/Settings/Config2.vala` - direct `Call.Models` usage
- `examples/TestAppBase.vala`, `examples/VectorAppBase.vala`, `examples/oc-test-cli.vala`
- `ollmapp/SettingsDialog/ConnectionsPage.vala`, `ConnectionAdd.vala`, `MainDialog.vala`, `ollmchat-cli.vala`

**Tasks**:
- [x] Verify all usages of `Call.Models` still work correctly
- [ ] Test that `Client.models()` returns correct data
- [ ] Test that `Connection.load_models()` populates models correctly

## Testing Checklist

- [ ] Test `Call.Models.exec_models()` with new endpoint
- [ ] Test `Call.Tags.exec_models()` with legacy endpoint (if kept)
- [ ] Test `Client.models()` returns correct data
- [ ] Test `Connection.load_models()` populates models correctly
- [ ] Test model names are correctly set from `id` field
- [ ] Test `created`, `owned_by`, `object` fields are populated
- [ ] Test that `show_model()` still works and merges data correctly
- [ ] Test model dropdowns in UI show correct model names
- [ ] Test that cached models still load correctly

## Summary of Changes

### Primary Motivation
**The `/api/tags` endpoint is not portable** - it's Ollama-specific and not part of the OpenAI API standard. Migrating to `/api/v1/model` ensures compatibility with OpenAI-compatible servers (OpenRouter, Together AI, etc.) and follows industry standards.

### Files to Create
- `libollmchat/Call/Tags.vala` - Legacy tags endpoint implementation (kept for backward compatibility fallback only)

### Files to Modify
- `libollmchat/Call/Models.vala` - Update to use `api/v1/model` endpoint; class doc to say OpenAI-compatible (Step 2)
- `libollmchat/Client.vala` - Update `models()` doc comment to say `/api/v1/model` (OpenAI-compatible) instead of `/api/tags` (Step 2b)
- `libollmchat/Call/Base.vala` - Keep `parse_models_array()` as standard deserialize only; remove type-specific logic (e.g. Ps name-from-model). Callers post-process.
- `libollmchat/Response/Model.vala` - Add `id`, `object`, `created`, `owned_by` properties (see Step 3 code blocks)
- `libollmchat/meson.build` - Add `Call/Tags.vala` to `ollmchat_ollama_src` files list

### Breaking Changes
- None - The change is internal to `Call.Models`. External API (`Client.models()`) remains the same.

## Related Plans

- None currently
