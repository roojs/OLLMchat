# 1.3.8. Temporary Model Creation and Deletion

## Overview

Implementation of temporary model creation and deletion system for handling custom `num_ctx` settings. When users configure a model with a custom context window size, we need to create temporary model variants using Ollama's `create` API since `num_ctx` cannot be set via API options alone.

## Status

✅ **COMPLETE** - All phases implemented.

## Related Plans

- **1.3.6** - Models Page (where model options including num_ctx are configured)
- **1.3.4** - Pulling Models (uses Ollama API for model operations)
- **1.5** - Model List Management

## Purpose

When users try to run a chat with a model that has a custom `num_ctx` setting, we need to create temporary model variants since `num_ctx` requires using Ollama's `create` API rather than just API options.

## Template placeholder (in Options)

In **Options** (e.g. `Call.Options` or model options), add a **placeholder for a `template` (string) property**, commented out in code for now. It refers to the ability for the user to edit a template in the future. Not implemented yet; we might use it later.

**Not related to** the Modelfile TEMPLATE used when creating temp models: for create we always use the existing template from the model we are using from (the base model).

## Implementation Phases

### Phase 1: Update the options ✅ COMPLETE

- ✅ Add to `Call.Options`: typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty.
- ✅ Align Description blurbs and `default_value` with Ollama defaults; use "(default: X)" and the Start value rules.
- ✅ num_ctx: display in K (64K, 128K, 256K, 512K, …) in the UI; keep stored/API value in tokens.
- ✅ Option ranges: apply the non‑0–1 ranges (num_ctx, num_predict, top_k, seed, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty).
- ✅ Options UI: new rows for the added options; `load_defaults` / `fill_from_model` and serialization for new fields.

### Phase 2: Add create support to libollmchat ✅ COMPLETE

**Call.Create** (or equivalent) — **dumb**, does what it's told: given connection, target model name, and Modelfile (e.g. `FROM {base}`, `PARAMETER num_ctx {value}`), it performs `POST /api/create`. No logic about when to create or how to name; caller provides name and Modelfile.

**Response/Model.customize(connection, options)** — holds the logic: returns the model name to use (based on `options.num_ctx`). If `num_ctx` is set (not auto): temp name `ollmchat-temp/{base}-ctx{N}K` (N = num_ctx/1024); if that model does not exist, builds the Modelfile (`FROM` this model, `PARAMETER num_ctx`), calls **Call.Create**, then returns that name. If `num_ctx` is auto, returns `this.name`. We always use the existing template from the base model; no custom template in the Modelfile.

### Phase 3: Hide all ollmchat-temp from model lists

- `Client.models()` and every model list / dropdown: **exclude** any model whose name starts with or is under `ollmchat-temp/`.
- ollmchat-temp models must not appear in the model selection UI.

### Phase 4: Create on chat when num_ctx is not auto (only for now)

Before running a chat, the entry point calls **`model.customize(connection, options)`** to get the model name to use. That method implements the create-if-missing logic (temp name, existence check, Call.Create when needed) and returns the name. For this phase, whenever `num_ctx` is set (not auto), the temp model is ensured before the chat; no other triggers for now.

## Todo Checklist

### Phase 1: Update the options ✅
- [x] Add typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty to `Call.Options`
- [x] Add placeholder for `template` property (commented out) in Options
- [x] Update `has_values`, `fill_from_model`, `serialize_property`/`deserialize_property` for new fields
- [x] Align Description blurbs with Ollama defaults using "(default: X)" format
- [x] Update `default_value` for all options to match Ollama defaults
- [x] Implement num_ctx display in K (64K, 128K, etc.) in UI while storing tokens
- [x] Apply option ranges for num_ctx, num_predict, top_k, seed, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty
- [x] Add new option rows in `ollmapp/SettingsDialog/Rows/Options.vala`
- [x] Update `ollmapp/SettingsDialog/ModelRow.vala` with `load_defaults` switch for new options
- [x] Update num_ctx control (IntRow or dedicated) to show/edit in K

### Phase 2: Add create support to libollmchat ✅
- [x] Create `libollmchat/Call/Create.vala` (or equivalent)
  - [x] Implement `POST /api/create` endpoint call
  - [x] Accept connection, target model name, and Modelfile content as parameters
  - [x] No policy logic - just performs the API call as instructed
- [x] Create `libollmchat/Response/Create.vala` for create response handling
- [x] Update `libollmchat/Response/Model.vala`
  - [x] Add `customize(connection, options)` method
  - [x] Implement temp model name generation: `ollmchat-temp/{base}-ctx{N}K` (N = num_ctx/1024)
  - [x] Add logic to check if temp model exists
  - [x] Build Modelfile with `FROM {this.name}` and `PARAMETER num_ctx {value}`
  - [x] Call `Call.Create` when temp model doesn't exist
  - [x] Return temp model name when num_ctx is set (not auto)
  - [x] Return `this.name` when num_ctx is auto
  - [x] Use existing template from base model (no custom template)
- [x] Add new files to `meson.build` build system

### Phase 3: Hide all ollmchat-temp from model lists ✅
- [x] Update `Client.models()` to filter out models starting with `ollmchat-temp/`
- [x] Update all model list sources to exclude `ollmchat-temp/` models
- [x] Update all model dropdown/selection UI components to exclude temp models
- [x] Verify temp models never appear in model selection UI

### Phase 4: Create on chat when num_ctx is not auto ✅ COMPLETE
- [x] Identify chat entry point(s)
  - **Primary entry point**: `Agent.Base.send_async()` (line 341 in `libollmchat/Agent/Base.vala`)
    - Called when agent sends messages
    - Has access to: `this.connection`, `this.chat_call.options`, `this.session.model_usage.model_obj`
    - Calls `this.chat_call.send(messages, cancellable)` at line 341
    - **Action needed**: Call `model_obj.customize(connection, options)` before `chat_call.send()`, then update `chat_call.model` with returned name
  - **Note**: `Agent.Base` constructor (line 121) creates Chat with model name, but model customization should happen at execution time, not construction time
- [x] Add call to `model.customize(connection, options)` before chat execution
- [x] Use returned model name for chat execution
- [x] Test temp model creation when num_ctx is set (not auto)
- [x] Test reuse of existing temp models
- [x] Implement error handling: fall back to base model if create fails
- [x] Add logging for temp model creation and errors
- [x] Add `--ctx-num` option to `oc-test-cli` for testing

---

## Temp Model Creation (ref)

**When**: Chat is run with num_ctx set (not auto).

**Process**:
1. Name = `ollmchat-temp/{base_name}-ctx{N}K` (N = num_ctx in K, e.g. 64, 128, 256, 512).
2. If that model exists → use it.
3. If not → create via `POST /api/create` with Modelfile `FROM {base}`, `PARAMETER num_ctx {value}` (existing template from the base model); then use it.

**Example**:
- Base model: `llama3`, num_ctx: 65536 (64K)
- Temp model name: `ollmchat-temp/llama3-ctx64`
- If `ollmchat-temp/llama3-ctx64` exists → use it. Otherwise create it, then use it.

## Temp Model Management

### Model Listings (Phase 3)
- All model listings must **always ignore** models in `ollmchat-temp` (or under `ollmchat-temp/`).
- Temp models are transparent to the user; they never appear in model selection.

### Startup Cleanup (later)
- On startup: list `ollmchat-temp` models, detect which are in use (e.g. via `ps`), delete those not in use.
- Deferred to a later phase; not required for Phase 4.

### Lifecycle (Phase 4, for now)
- **Create**: When chat is run with num_ctx not auto and the temp model does not exist.
- **Use**: The temp model `ollmchat-temp/{base}-ctx{N}K` for the chat.
- **Reuse**: If it exists, never create again; just use it.

## Implementation Details (by phase)

### Phase 1
- `libollmchat/Call/Options.vala`: add typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty; `has_values`, `fill_from_model`, `serialize_property`/`deserialize_property`. Placeholder for `template` (string), commented out for now—for possible future user-editable template.
- `ollmapp/SettingsDialog/Rows/Options.vala`: new rows; `default_value` and ranges from the Option ranges table.
- `ollmapp/SettingsDialog/ModelRow.vala`: `load_defaults` switch for new options.
- num_ctx: IntRow or dedicated control to show/edit in K while storing tokens.

### Phase 2
- **`libollmchat/Call/Create.vala`** (or equivalent): dumb. Given connection, target model name, and Modelfile content, performs `POST /api/create`. No policy logic.
- **`libollmchat/Response/Model.vala`**: `customize(connection, options)` — returns the model name to use; if `num_ctx` is set, uses temp name `ollmchat-temp/{this.name}-ctx{N}K` (N = num_ctx/1024), checks if it exists, calls **Call.Create** with Modelfile `FROM {this.name}`, `PARAMETER num_ctx {value}` if not, then returns that name. If `num_ctx` is auto, returns `this.name`.

### Phase 3 ✅
- `ConnectionModels.refresh_connection()`: filters out models whose name starts with `ollmchat-temp/` when adding to the model list
- `ChatInput.setup_model_dropdown()`: uses `Gtk.CustomFilter` to exclude `ollmchat-temp/` models from the dropdown
- `SettingsDialog/Rows/Model.vala`: uses `Gtk.CustomFilter` to exclude `ollmchat-temp/` models from the model selection dropdown
- All UI components automatically exclude temp models since they use `ConnectionModels` which filters temp models, and UI filters provide additional protection

### Phase 4
- Chat entry point: call **`model.customize(connection, options)`** to get the model name, then run the chat with that name. `customize` handles create-if-missing via Call.Create.

### API Integration (Phase 2)
- **Call.Create** performs Ollama `POST /api/create` with the Modelfile it is given. **Model.customize** builds the Modelfile (`FROM {this.name}`, `PARAMETER num_ctx {value}`) and passes it to Create when it needs to create the temp model.

### Error Handling
- If create fails: fall back to base model, log, and optional user-facing message.

## Integration Points

- **Config2.model_options**: Source of custom num_ctx values
- **Response/Model.customize(connection, options)**: Returns model name; create-if-missing for temp model when num_ctx is set; calls Call.Create
- **Call.Create**: Dumb `POST /api/create`; given name and Modelfile, does what it’s told
- **Chat entry point**: Calls `model.customize(connection, options)` to get the model name, then runs the chat
- **Ollama API**: Model creation endpoint (used by Call.Create)
- **Process Management**: (later) Track active temp models via process monitoring

## Expanding Runtime Options

Ollama’s full option set is split into **predict/runtime** options (per-request) and **Runner** options (set when the model is loaded). We only use **num_ctx** from Runner at create time (via temp models); the rest of Runner is ignored.

### API reference: Ollama options

**Predict options** (runtime, sent in `options`):

| JSON key            | Type    | We have |
|---------------------|---------|---------|
| `num_keep`          | int     | no (irrelevant for us) |
| `seed`              | int     | yes     |
| `num_predict`       | int     | yes     |
| `top_k`             | int     | yes     |
| `top_p`             | float32 | yes     |
| `min_p`             | float32 | yes     |
| `typical_p`         | float32 | no      |
| `repeat_last_n`     | int     | no      |
| `temperature`       | float32 | yes     |
| `repeat_penalty`    | float32 | no      |
| `presence_penalty`  | float32 | no      |
| `frequency_penalty` | float32 | no      |
| `stop`              | []string| —       (ignored; not needed for us) |

**Runner options** (model load; we only use `num_ctx` at create, others ignored):

| JSON key   | Type | Use at create | Notes                    |
|------------|------|----------------|--------------------------|
| `num_ctx`  | int  | yes            | Context size; temp model |
| `num_batch`| int  | no             | —                        |
| `num_gpu`  | int  | no             | —                        |
| `main_gpu` | int  | no             | —                        |
| `use_mmap` | *bool| no             | —                        |
| `num_thread`| int | no             | —                        |

### Ollama runtime defaults (when the model doesn’t set them)

When we don’t send an option, Ollama uses these. Use them for the **auto** value in the options UI (i.e. show that “auto” means this default).

| JSON key           | Default | Notes |
|--------------------|---------|-------|
| `temperature`      | 0.8     |       |
| `top_k`            | 40      |       |
| `top_p`            | 0.9     |       |
| `typical_p`        | 1.0     |       |
| `repeat_last_n`    | 64      |       |
| `repeat_penalty`   | 1.1     |       |
| `presence_penalty` | 0.0     |       |
| `frequency_penalty`| 0.0     |       |
| `seed`             | -1      |       |

(Options not listed — e.g. `num_keep`, `num_predict`, `min_p`, `num_ctx` — have no single Ollama-side default documented here; model or context may override.)

### Start value and “(default: X)” in descriptions

- **Start value** comes from (1) **model parameters** first (from Ollama `show` → `model.parameters` → `model.options` via `fill_from_model`). If the model sets an option, that is the initial value in the UI when the user hasn’t overridden it.
- When the **model doesn’t set** an option, use the **Ollama default** as the starting point: put **(default: X)** at the end of the option’s text description and use that same value for the UI `default_value` (and for the “auto” display when the user hasn’t set a value).

So: model overrides when present; otherwise Ollama default. Existing options (temperature, top_k, top_p, etc.) should have their `default_value` and Description blurbs aligned with the Ollama defaults table where we have one.

### Options to add (excluding `stop`, `num_keep`)

**num_keep** — Irrelevant for us; not adding.

**To add:** typical_p, repeat_last_n, repeat_penalty, presence_penalty, frequency_penalty. Descriptions use Ollama defaults at the end.

| Option | Type | What it does | Default |
|--------|------|--------------|---------|
| **typical_p** | float | Locally typical sampling: minimum likelihood for a token relative to the top token before it is considered. Range 0.0–1.0; 1.0 = off. Lower values = more conservative. (default: 1.0) | 1.0 |
| **repeat_last_n** | int | Sliding window (tokens) for repetition penalties. 0 = no look‑back; -1 = full context. Works with `repeat_penalty`. (default: 64) | 64 |
| **repeat_penalty** | float | Reduces probability of tokens in the `repeat_last_n` window. ≥1.0 discourages repetition. (default: 1.1) | 1.1 |
| **presence_penalty** | float | Penalty if a token has appeared (binary). Encourages new tokens; 0 = off. (default: 0.0) | 0.0 |
| **frequency_penalty** | float | Penalty by how often a token appeared. Reduces overuse; 0 = off. (default: 0.0) | 0.0 |

Use the Default column for `default_value` in the options UI and for “auto” when the user hasn’t set a value.

### Option ranges (non‑0–1)

Ranges for the options that are not 0–1 bounded. (0–1 options like temperature, top_p, min_p, typical_p are omitted.)

| Option | Min | Max | Step | Display / notes |
|--------|-----|-----|------|-----------------|
| **num_ctx** | 1 | 1_000_000 | 1024 | **Display in K** (1K = 1024 tokens): e.g. 64K, 128K, 256K, 512K. Stored and sent to API as token count. We currently show raw tokens; switch to K for the UI. |
| **num_predict** | 1 | 1_000_000 | 1 | |
| **top_k** | 1 | 1000 | 1 | |
| **seed** | -1 | 2_147_483_647 | 1 | -1 = random (Ollama default) |
| **repeat_last_n** | 0 | 65536 | 1 | 0 = off. Typical 64. -1 = unset (we do not support Ollama’s -1 = full context). |
| **repeat_penalty** | 1.0 | 2.0 | 0.1 | Typical 1.1. |
| **presence_penalty** | 0.0 | 2.0 | 0.1 | |
| **frequency_penalty** | 0.0 | 2.0 | 0.1 | |

**num_ctx display:** We currently use a plain int spin (tokens). Prefer showing K in the UI (64K, 128K, 256K, 512K, … up to 1024K for 1M) while keeping the stored and API value in tokens.

---

## Future Considerations

- **Model Variant Caching**: Cache frequently used temp models
- **Automatic Cleanup**: Periodic cleanup of unused temp models (not just on startup)
- **User Notification**: Optionally notify user when temp model is created
- **Temp Model Limits**: Limit number of temp models to prevent disk space issues

