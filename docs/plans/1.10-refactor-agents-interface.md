# 1.10. Refactor Agents as Interface Between UI and Client

## Overview

Refactor the agent architecture so that agents act as the interface layer between the UI and the client, rather than being embedded within the client. This separation will make agents more flexible, allow for better UI integration, and enable dynamic agent creation and modification.

## Status

⏳ **TODO** - To be implemented.

## Related Plans

- **1.7** - Agent Management System (current agent implementation)
- **1.11** - libgjs for Agents (dynamic agent creation)
- **1.3.9** - Agent Configuration (agent configuration UI)

## Current Architecture

**Current State:**
- Agents are embedded within the client via `client.prompt_assistant`
- Client directly uses agent's `generate_system_prompt()` and `generate_user_prompt()` methods
- Manager stores agents and sets active agent on client
- UI selects agent, which updates `manager.active_agent_name`, which then sets `client.prompt_assistant`

**Problems:**
- Tight coupling between client and agents
- Agents are passive prompt generators
- Difficult to add agent-specific UI or behavior
- Client must know about agent structure

## Proposed Architecture

**New State:**
- Agents become the primary interface between UI and client
- Agents wrap or own the client instance
- UI interacts with agents, agents interact with client
- Agents can provide UI widgets, handle tool calls, manage state
- Client becomes a lower-level API that agents use

**Benefits:**
- Clear separation of concerns
- Agents can have their own state and behavior
- Easier to add agent-specific features
- Better support for dynamic agent creation
- Agents can manage their own tools and configurations

## Architecture Changes

### Agent Interface

**New Agent Responsibilities:**
- Own or wrap a client instance
- Handle chat requests from UI
- Relay replies from client back to UI
- Manage agent-specific tools
- Provide UI widgets (already implemented via `get_widget()`)
- Handle agent-specific state and configuration
- Process messages before/after client calls
- Handle streaming responses from client to UI

**Agent Methods:**
- `send_message(string user_input)` - Send message through agent to client, returns response
- `send_message_async(string user_input, Callback callback)` - Async version with callback for streaming
- `handle_reply(string reply)` - Process and relay reply from client to UI
- `handle_stream_chunk(string chunk)` - Process and relay streaming chunks from client to UI
- `get_client()` - Get the underlying client instance
- `get_tools()` - Get agent-specific tools
- `configure_tools()` - Configure tools for this agent
- `get_widget()` - Provide UI widget (already exists)

### Client Changes

**Client Simplification:**
- Remove `prompt_assistant` property from Client
- Client becomes a lower-level API
- Client focuses on API communication, not prompt generation
- Prompt generation moves to agent layer

**Client Methods:**
- Keep existing API methods (chat, stream, etc.)
- Remove agent-specific logic
- Tools remain at client level (or move to agent level?)

### Manager Changes

**Manager Updates:**
- Keep agent registration and management
- Remove `active_agent_name` (agents manage their own state)
- Manager provides agent selection to UI
- Manager coordinates between agents and sessions

**Manager Methods:**
- `get_active_agent()` - Get currently active agent
- `set_active_agent(BaseAgent agent)` - Set active agent
- `agent_activated` signal - Emit when agent changes (already exists)

### UI Changes

**UI Updates:**
- UI interacts with agents, not directly with client
- Chat input sends messages to active agent
- Agent handles routing to client
- Agent relays replies back to UI (full round-trip: UI → Agent → Client → Agent → UI)
- Agent handles streaming responses to UI
- Agent provides UI widgets (already implemented)

**ChatInput Changes:**
- Send messages to `manager.get_active_agent().send_message_async()`
- Register callback to receive replies from agent
- Agent handles client interaction internally
- Agent processes and relays replies/streams back to UI callback
- Remove direct client access from UI

### Session Changes

**Session Updates:**
- Sessions may need to store agent reference
- Or sessions use manager's active agent
- Agent state may need to be session-specific

## Implementation Details

### Phase 1: Agent Interface Refactoring

**1. Update BaseAgent**
- Add `client` property (owned or wrapped Client instance)
- Add `send_message(string user_input)` method - synchronous, returns full response
- Add `send_message_async(string user_input, Callback callback)` method - async with streaming support
- Add `handle_reply(string reply)` method - process and relay reply to UI
- Add `handle_stream_chunk(string chunk)` method - process and relay streaming chunks to UI
- Add reply callback/signal for UI to receive responses
- Add `get_client()` method
- Remove dependency on client's `prompt_assistant`
- Agent generates prompts internally before calling client
- Agent receives responses from client and relays to UI

**2. Update CodeAssistant**
- Initialize with client instance
- Override `send_message()` and `send_message_async()` to add system prompts
- Override `handle_reply()` and `handle_stream_chunk()` to process agent-specific responses
- Handle tool calls through agent layer
- Relay tool results and final responses back to UI
- Maintain existing behavior but through new interface

**3. Update JustAsk**
- Initialize with client instance
- Override `send_message()` and `send_message_async()` to pass through directly
- Override `handle_reply()` and `handle_stream_chunk()` to pass through directly (no processing)
- Minimal changes needed, mostly pass-through behavior

### Phase 2: Client Simplification

**1. Remove prompt_assistant from Client**
- Remove `prompt_assistant` property
- Remove prompt generation logic from client
- Client becomes pure API interface

**2. Update Client Methods**
- `chat()` method accepts pre-generated prompts
- Or client methods accept agent context
- Tools remain at client level (or move to agent?)

### Phase 3: Manager Updates

**1. Update Manager**
- Keep agent registration
- Update `get_active_agent()` to return agent with client
- Remove `active_agent_name` if not needed
- Ensure agents have client instances

**2. Update new_client()**
- Create client instance
- Create agent with client instance
- Register agent
- Return agent (or keep client separate?)

### Phase 4: UI Integration

**1. Update ChatInput**
- Send messages to `manager.get_active_agent().send_message_async()`
- Register callback to receive replies from agent
- Handle streaming chunks from agent callback
- Remove direct client access
- Agent handles all client interaction and reply relaying

**2. Update Window**
- Ensure agents are created with client instances
- UI interacts with agents, not clients directly
- UI receives replies through agent callbacks/signals

## Files to Modify

### Core Agent Files
- `libollmchat/Prompt/BaseAgent.vala` - Add client property, send_message method
- `libollmchat/Prompt/CodeAssistant.vala` - Update to use new interface
- `libollmchat/Prompt/JustAsk.vala` - Update to use new interface

### Client Files
- `libollmchat/Client.vala` - Remove prompt_assistant, simplify API
- `libollmchat/Call/Chat.vala` - Update to work without prompt_assistant

### Manager Files
- `libollmchat/History/Manager.vala` - Update agent management, ensure agents have clients

### UI Files
- `libollmchatgtk/ChatInput.vala` - Send messages to agent instead of client
- `ollmchat/Window.vala` - Update agent initialization with client instances

## Migration Strategy

1. **Add new methods alongside old ones** - Keep backward compatibility initially
2. **Gradually migrate UI to use new interface** - Test each component
3. **Remove old code** - Once migration is complete
4. **Update tests** - Ensure all tests pass with new architecture

## Design Considerations

### Client Ownership
- **Option A**: Agent owns client instance (agent creates/manages client)
- **Option B**: Agent wraps client instance (client created externally, passed to agent)
- **Option C**: Agent references client (client managed separately, agent uses reference)

**Recommendation**: Option B - Agent wraps client, allows for client reuse and better separation

### Tool Management
- **Option A**: Tools remain at client level (shared across agents)
- **Option B**: Tools move to agent level (agent-specific tools)
- **Option C**: Hybrid (base tools at client, agent-specific at agent)

**Recommendation**: Option C - Base tools at client, agent-specific tools at agent level

### Session-Agent Relationship
- **Option A**: Session stores agent reference
- **Option B**: Session uses manager's active agent
- **Option C**: Each session has its own agent instance

**Recommendation**: Option B - Sessions use manager's active agent, simpler state management

### Reply Handling
- **Option A**: Agent emits signals for replies (UI connects to signals)
- **Option B**: Agent uses callback functions (UI passes callbacks to agent)
- **Option C**: Hybrid (signals for events, callbacks for streaming)

**Recommendation**: Option C - Signals for reply events, callbacks for streaming chunks. Provides flexibility for both synchronous and asynchronous reply handling.

## Message Flow

**Current Flow (Before Refactoring):**
```
UI → Client → LLM API
LLM API → Client → UI
```

**New Flow (After Refactoring):**
```
UI → Agent → Client → LLM API
LLM API → Client → Agent → UI
```

**Detailed Flow:**
1. UI calls `agent.send_message_async(user_input, callback)`
2. Agent generates system/user prompts (if needed)
3. Agent calls `client.chat()` or `client.stream()` with prompts
4. Client sends to LLM API
5. LLM API responds (streaming or complete)
6. Client receives response
7. Agent receives response from client via callback/signal
8. Agent processes response (agent-specific logic)
9. Agent calls UI callback/signal with processed response
10. UI receives and displays response

**Streaming Flow:**
1. UI calls `agent.send_message_async(user_input, stream_callback)`
2. Agent sets up streaming with client
3. Client streams chunks from LLM API
4. Each chunk: Client → Agent → UI (via stream_callback)
5. Final response: Client → Agent → UI (via completion callback)

## Future Enhancements

- Agent-specific tool configurations
- Agent state persistence
- Agent-to-agent communication
- Agent composition (agents that use other agents)
- Agent plugins/extensions
- Agent reply transformation (formatting, filtering, etc.)
- Agent reply caching

