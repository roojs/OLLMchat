# 2.4.1 URGENT - Fix CodeEdit After Refactor

## Status: PLANNING

## Problem Statement

After the refactor from signals to direct method calls, the edit mode tool no longer receives streaming content from the LLM. The tool activates successfully and sends "start" message to LLM, but subsequent streaming chunks are not processed, so code blocks are not captured.

## How It Worked Before (Signal-Based Architecture)

### Old Data Flow

```
LLM API → Client (Soup HTTP) 
  → Chat.process_streaming_chunk() 
    → Emits stream_content signal (for content chunks)
    → Emits stream_chunk signal (for all chunks)
  → Multiple listeners:
    1. SessionBase.on_stream_chunk() - persistence
    2. Manager.stream_chunk() - UI relay  
    3. Tools (EditMode) - code block capture
```

### Old EditMode Streaming Implementation

1. **Signal Connection**: EditMode tool connected to `client.stream_content` signal
   - Connected in EditMode constructor or Request.connect_signals()
   - Signal emitted by `Chat.process_streaming_chunk()` for each content chunk
   - Only emitted for non-thinking content (`new_content.length > 0`)

2. **Real-Time Processing**: 
   - Each chunk arrived via signal callback: `on_stream_content(string new_text, Response.Chat response)`
   - EditMode processed chunks **DURING streaming** (not at the end)
   - Called `process_streaming_content(new_text)` for each chunk
   - Parsed code blocks incrementally as they arrived

3. **Message Completion**:
   - Connected to `client.message_created` signal
   - When message was done, `on_message_created()` was called
   - Processed any remaining content and applied file changes

### Key Points from Old Implementation

- **Streaming was real-time**: Chunks processed as they arrived, not buffered
- **Signal-based**: Tools connected directly to Client/Chat signals
- **Multiple listeners**: Same signal went to Session (persistence), Manager (UI), and Tools (processing)
- **Tool-specific**: Each tool managed its own signal connections

## Current State After Refactor

### New Data Flow (Method Calls)

```
LLM API → Chat.process_streaming_chunk()
  → Calls agent.handle_stream_chunk(new_text, is_thinking, response) [DIRECT METHOD CALL]
    → Agent.Base.handle_stream_chunk()
      → Calls session.handle_stream_chunk(new_text, is_thinking, response) [DIRECT METHOD CALL]
        → Session.handle_stream_chunk()
          → Creates/updates stream messages for persistence
          → Calls base.handle_stream_chunk() [EMITS SIGNAL]
            → Manager.stream_chunk() signal [FOR UI ONLY]
              → ChatWidget.on_stream_chunk_handler()
```

### What Changed

1. **Signals Removed**: Client/Chat no longer emit signals for agent usage
2. **Direct Method Calls**: Agent → Session → Manager (method calls)
3. **UI Still Uses Signals**: Manager still emits signals for UI components
4. **Tools Lost Access**: No mechanism for tools to receive streaming chunks

### Current EditMode State

- `connect_signals()`: Empty, just logs (signals removed)
- `process_streaming_content()`: Still exists, but never called
- `active_requests`: Static list maintained, but no way to notify them
- `on_message_created()`: Still works via Session's message_added signal

## The Problem

**EditMode needs streaming chunks but has no way to receive them:**

1. Tool executes and activates edit mode → sends "start" to LLM ✅
2. LLM starts streaming response → chunks arrive at Chat ✅
3. Chat calls `agent.handle_stream_chunk()` → goes to Session → Manager → UI ✅
4. **EditMode never receives chunks** ❌
5. When message is done → `on_message_created()` called, but no changes captured ❌

## Solution Plan

### Approach: Agent Signals for Tool Monitoring

Create a generic mechanism using signals where:
- Agent emits signals for streaming and message events
- Agent maintains registry of active tool requests (keyed by request_id/tool_call.id)
- Tool requests register themselves to receive signals
- Agent connects tool request callbacks to signals
- Tool requests unregister when done

**Key Design Decisions:**
- Use signals on Agent (not direct method calls)
- Tool requests implement callback interface methods
- Agent manages signal connections/disconnections
- Request already has agent reference (set in Tool.execute())

### Implementation Steps

#### Step 1: Add tool_call_id to RequestBase

**File**: `libollmchat/Tool/RequestBase.vala`

- Add property: `public string tool_call_id { get; set; default = ""; }`
- Exclude from JSON deserialization (set after deserialization, like `tool` and `agent`)
- This identifies which tool call this request belongs to

#### Step 2: Add Active Request Registry to Agent

**File**: `libollmchat/Agent/Base.vala`

- Add property: `private Gee.HashMap<string, RequestBase> active_tool_requests`
- Initialize in constructor: `active_tool_requests = new Gee.HashMap<string, RequestBase>();`

**Methods to add:**
- `public void register_active_request(string tool_call_id, RequestBase request)`
  - Stores request in map: `active_tool_requests[tool_call_id] = request`
  - Called by Request after tool_call_id is set

- `public void unregister_active_request(string tool_call_id)`
  - Removes from map: `active_tool_requests.unset(tool_call_id)`
  - Called by Request when cleaning up

#### Step 3: Set tool_call_id in Agent.execute_tools()

**File**: `libollmchat/Agent/Base.vala`

In `execute_tools()` method, after creating request (inside `tool.execute()`):
- Need to get the request object back from `tool.execute()`
- **Option A**: Change `tool.execute()` to return `(string result, RequestBase? request)` tuple
- **Option B**: Have tool.execute() call a callback: `on_request_created(RequestBase request)`
- **Option C**: Request registers itself in `RequestBase.execute()` if `tool_call_id` is set

**Recommended: Option C** (Request registers itself)
- In `Agent.execute_tools()`, before calling `tool.execute()`:
  - Create a wrapper that captures the request
  - Or, modify `tool.execute()` to accept optional callback
- Actually, simpler: Set `request.tool_call_id = tool_call.id` **before** calling `request.execute()`
- In `RequestBase.execute()`, if `tool_call_id != ""`, call `agent.register_active_request(tool_call_id, this)`

Wait, problem: `tool.execute()` creates the request internally. We don't have access to it.

**Better approach**: 
- Modify `Tool.BaseTool.execute()` to accept optional `on_request_created` callback
- After creating request, call callback: `on_request_created(request, tool_call.id)`
- Agent provides callback that registers the request

#### Step 4: Add handle_stream_chunk() to RequestBase

**File**: `libollmchat/Tool/RequestBase.vala`

- Add virtual method: `public virtual void handle_stream_chunk(string new_text, bool is_thinking)`
- Default implementation: empty (does nothing)
- EditMode.Request overrides it to call `process_streaming_content(new_text)` (only if not thinking)

#### Step 5: Relay Streaming to Active Requests in Agent

**File**: `libollmchat/Agent/Base.vala`

In `handle_stream_chunk()` method:
```vala
public virtual void handle_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Relay to active tool requests (for tools that need streaming, like EditMode)
    foreach (var request in this.active_tool_requests.values) {
        if (request.tool.active) {
            request.handle_stream_chunk(new_text, is_thinking);
        }
    }
    
    // Relay to session (existing code)
    this.session.handle_stream_chunk(new_text, is_thinking, response);
}
```

#### Step 6: Implement handle_stream_chunk() in EditMode.Request

**File**: `liboctools/EditMode/Request.vala`

- Override `handle_stream_chunk(string new_text, bool is_thinking)`
- Only process non-thinking content: `if (!is_thinking && new_text.length > 0)`
- Call `process_streaming_content(new_text)`

#### Step 7: Register/Unregister in EditMode Request

**File**: `liboctools/EditMode/Request.vala`

- In `execute_request()`: After adding to `active_requests`, also register with agent if `tool_call_id` is set
- In `disconnect_signals()` or `on_message_created()`: Unregister from agent

Actually, registration should happen in `RequestBase.execute()` if `tool_call_id` is set.

#### Step 8: Unregister When Done

**File**: `liboctools/EditMode/Request.vala`

- In `on_message_created()`: After processing, call `agent.unregister_active_request(this.tool_call_id)`
- In `reply_with_errors()`: Before cleanup, call `agent.unregister_active_request(this.tool_call_id)`

Also need to unregister in `RequestBase` cleanup, but that's tricky. Better to have Request call it explicitly.

## Detailed Implementation

### 1. RequestBase Changes

```vala
// Add property
public string tool_call_id { get; set; default = ""; }

// Exclude from deserialization
public virtual bool deserialize_property(string property_name, out Value value, ParamSpec pspec, Json.Node property_node)
{
    switch (property_name) {
        case "tool":
        case "agent":
        case "tool_call_id":  // NEW
            value = Value(pspec.value_type);
            return true;
        // ...
    }
}

// Add virtual method for streaming
public virtual void handle_stream_chunk(string new_text, bool is_thinking)
{
    // Default: do nothing
    // Tools that need streaming override this
}

// Register with agent in execute() if tool_call_id is set
public virtual async string execute()
{
    // ... existing permission check code ...
    
    // Register with agent if tool_call_id is set
    if (this.tool_call_id != "" && this.agent != null) {
        // Agent needs register_active_request method
        // But Agent.Interface doesn't have it yet - need to add
        // For now, cast to Agent.Base
        var agent_base = this.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.register_active_request(this.tool_call_id, this);
        }
    }
    
    // ... existing execute_request() call ...
}
```

### 2. Agent.Base Changes

```vala
// Add registry
private Gee.HashMap<string, RequestBase> active_tool_requests;

// In constructor
this.active_tool_requests = new Gee.HashMap<string, RequestBase>();

// Add registration methods
public void register_active_request(string tool_call_id, RequestBase request)
{
    this.active_tool_requests[tool_call_id] = request;
    GLib.debug("Agent.register_active_request: Registered request for tool_call_id='%s'", tool_call_id);
}

public void unregister_active_request(string tool_call_id)
{
    this.active_tool_requests.unset(tool_call_id);
    GLib.debug("Agent.unregister_active_request: Unregistered request for tool_call_id='%s'", tool_call_id);
}

// Modify handle_stream_chunk
public virtual void handle_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Relay to active tool requests FIRST (before session)
    foreach (var request in this.active_tool_requests.values) {
        if (request.tool.active) {
            request.handle_stream_chunk(new_text, is_thinking);
        }
    }
    
    // Then relay to session (existing code)
    this.session.handle_stream_chunk(new_text, is_thinking, response);
}
```

### 3. Agent.execute_tools() Changes

```vala
// In execute_tools(), before calling tool.execute():
// We need to set tool_call_id on the request
// But request is created inside tool.execute()

// Solution: Modify Tool.BaseTool.execute() to accept callback
// Or: Have tool.execute() return the request somehow

// Actually, simpler: Pass tool_call_id to tool.execute()
// Tool can set it on request before calling request.execute()

// Modify Tool.BaseTool.execute() signature:
public virtual async string execute(Call.Chat chat_call, Json.Object parameters, string? tool_call_id = null)
{
    // ... create request ...
    request.tool = this;
    request.agent = chat_call.agent;
    
    // Set tool_call_id if provided
    if (tool_call_id != null) {
        request.tool_call_id = tool_call_id;
    }
    
    return yield request.execute();
}

// In Agent.execute_tools():
var result = yield tool.execute(this.chat_call, tool_call.function.arguments, tool_call.id);
```

### 4. EditMode.Request Changes

```vala
// Override handle_stream_chunk
public override void handle_stream_chunk(string new_text, bool is_thinking)
{
    // Only process non-thinking content (actual code blocks)
    if (!is_thinking && new_text.length > 0) {
        this.process_streaming_content(new_text);
    }
}

// Unregister in cleanup methods
private async void on_message_created(OLLMchat.Message message)
{
    // ... existing code ...
    
    // Unregister from agent
    if (this.tool_call_id != "" && this.agent != null) {
        var agent_base = this.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.unregister_active_request(this.tool_call_id);
        }
    }
    
    // ... rest of cleanup ...
}

private void reply_with_errors(OLLMchat.Response.Chat response, string message = "")
{
    // Unregister from agent
    if (this.tool_call_id != "" && this.agent != null) {
        var agent_base = this.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.unregister_active_request(this.tool_call_id);
        }
    }
    
    // ... rest of code ...
}
```

## Testing Plan

1. **Basic Flow Test**:
   - Activate edit mode for a file
   - Send message to LLM
   - Verify streaming chunks are received by EditMode
   - Verify code blocks are captured
   - Verify file changes are applied

2. **Multiple Tools Test**:
   - Activate edit mode
   - Call another tool
   - Verify only edit mode receives streaming (other tools don't override handle_stream_chunk)

3. **Cleanup Test**:
   - Activate edit mode
   - Complete message
   - Verify request is unregistered
   - Verify no memory leaks

4. **Error Handling Test**:
   - Activate edit mode
   - Cause error
   - Verify cleanup still happens

## Files to Modify

1. `libollmchat/Tool/RequestBase.vala` - Add tool_call_id, handle_stream_chunk()
2. `libollmchat/Tool/Tool.vala` - Modify execute() to accept tool_call_id
3. `libollmchat/Agent/Base.vala` - Add registry, modify handle_stream_chunk(), execute_tools()
4. `liboctools/EditMode/Request.vala` - Override handle_stream_chunk(), unregister on cleanup

## Risks and Considerations

1. **Agent.Interface**: Need to decide if registration methods should be in interface or only Base
   - For now, cast to Agent.Base (only Base implements it)
   - Future: Could add to interface if needed

2. **Multiple Active Requests**: What if same tool_call_id is used twice?
   - Shouldn't happen (tool_call.id is unique per call)
   - But handle gracefully: overwrite old registration

3. **Request Lifecycle**: Ensure unregistration happens in all cleanup paths
   - on_message_created() - success path
   - reply_with_errors() - error path
   - disconnect_signals() - cleanup path

4. **Performance**: Iterating over active_tool_requests on every chunk
   - Should be fine (usually 0-1 active requests)
   - Can optimize later if needed

## Success Criteria

- ✅ EditMode receives streaming chunks during LLM response
- ✅ Code blocks are captured in real-time
- ✅ File changes are applied when message is done
- ✅ No memory leaks (requests properly unregistered)
- ✅ Generic solution works for other tools that need streaming
- ✅ No regression in existing functionality
