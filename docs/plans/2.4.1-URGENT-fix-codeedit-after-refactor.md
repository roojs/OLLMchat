# 2.4.1 URGENT - Fix CodeEdit After Refactor

## Status: PLANNING

## Problem Statement

After the refactor from signals to direct method calls, the edit mode tool no longer receives streaming content from the LLM. The tool activates successfully and sends "start" message to LLM, but subsequent streaming chunks are not processed, so code blocks are not captured.

## How It Worked Before (Signal-Based Architecture)

### Old Data Flow

```
LLM API → Client (Soup HTTP) 
  → Chat.process_streaming_chunk() 
    → Emits stream_content signal (for content chunks)
    → Emits stream_chunk signal (for all chunks)
  → Multiple listeners:
    1. SessionBase.on_stream_chunk() - persistence
    2. Manager.stream_chunk() - UI relay  
    3. Tools (EditMode) - code block capture
```

### Old EditMode Streaming Implementation

1. **Signal Connection**: EditMode tool connected to `client.stream_content` signal
   - Connected in EditMode constructor or Request.connect_signals()
   - Signal emitted by `Chat.process_streaming_chunk()` for each content chunk
   - Only emitted for non-thinking content (`new_content.length > 0`)

2. **Real-Time Processing**: 
   - Each chunk arrived via signal callback: `on_stream_content(string new_text, Response.Chat response)`
   - EditMode processed chunks **DURING streaming** (not at the end)
   - Called `process_streaming_content(new_text)` for each chunk
   - Parsed code blocks incrementally as they arrived

3. **Message Completion**:
   - Connected to `client.message_created` signal
   - When message was done, `on_message_created()` was called
   - Processed any remaining content and applied file changes

### Key Points from Old Implementation

- **Streaming was real-time**: Chunks processed as they arrived, not buffered
- **Signal-based**: Tools connected directly to Client/Chat signals
- **Multiple listeners**: Same signal went to Session (persistence), Manager (UI), and Tools (processing)
- **Tool-specific**: Each tool managed its own signal connections

## Current State After Refactor

### New Data Flow (Method Calls)

```
LLM API → Chat.process_streaming_chunk()
  → Calls agent.handle_stream_chunk(new_text, is_thinking, response) [DIRECT METHOD CALL]
    → Agent.Base.handle_stream_chunk()
      → Calls session.handle_stream_chunk(new_text, is_thinking, response) [DIRECT METHOD CALL]
        → Session.handle_stream_chunk()
          → Creates/updates stream messages for persistence
          → Calls base.handle_stream_chunk() [EMITS SIGNAL]
            → Manager.stream_chunk() signal [FOR UI ONLY]
              → ChatWidget.on_stream_chunk_handler()
```

### What Changed

1. **Signals Removed**: Client/Chat no longer emit signals for agent usage
2. **Direct Method Calls**: Agent → Session → Manager (method calls)
3. **UI Still Uses Signals**: Manager still emits signals for UI components
4. **Tools Lost Access**: No mechanism for tools to receive streaming chunks

### Current EditMode State

- `connect_signals()`: Empty, just logs (signals removed)
- `process_streaming_content()`: Still exists, but never called
- `active_requests`: Static list maintained, but no way to notify them
- `on_message_created()`: Still works via Session's message_added signal

## The Problem

**EditMode needs streaming chunks but has no way to receive them:**

1. Tool executes and activates edit mode → sends "start" to LLM ✅
2. LLM starts streaming response → chunks arrive at Chat ✅
3. Chat calls `agent.handle_stream_chunk()` → goes to Session → Manager → UI ✅
4. **EditMode never receives chunks** ❌
5. When message is done → `on_message_created()` called, but no changes captured ❌

## Solution Plan

### Approach: Agent Signals for Tool Monitoring

Create a generic mechanism using signals where:
- Agent emits signals for streaming and message events
- Agent maintains registry of active tool requests (keyed by request_id/tool_call.id)
- Tool requests register themselves to receive signals
- Agent connects tool request callbacks to signals
- Tool requests unregister when done

**Key Design Decisions:**
- Use signals on Agent (not direct method calls)
- Tool requests implement callback interface methods
- Agent manages signal connections/disconnections
- Request already has agent reference (set in Tool.execute())

### Implementation Steps

#### Step 1: Add Signals to Agent.Base

**File**: `libollmchat/Agent/Base.vala`

**Existing signals** (already defined):
- `public signal void stream_chunk(string new_text, bool is_thinking, Response.Chat response)` ✅ EXISTS
- `public signal void stream_content(string new_text, Response.Chat response)` ✅ EXISTS (content only, not thinking)

**New signals to add**:
- `public signal void message_completed(Message message)` ❌ NEED TO ADD

**Note**: `message_failed` is NOT needed - there's no existing error handling mechanism for failed messages in the streaming flow. Errors propagate as exceptions through `send_async()` which throws Error. Tools can handle errors in their own try/catch blocks if needed.

**Note**: We can use the existing `stream_chunk` signal instead of creating `stream_chunk_received`. The existing signal already has the right signature and is emitted for all chunks (both thinking and content).

#### Step 2: Add Active Tools Registry to Agent

**File**: `libollmchat/Agent/Base.vala`

- Add property: `private Gee.HashMap<string, RequestBase> active_tools`
- Initialize in constructor: `active_tools = new Gee.HashMap<string, RequestBase>();`

**Methods to add:**
- `public void register_tool_monitoring(string request_id, RequestBase request)`
  - Stores request in map: `active_tools[request_id] = request`
  - Sets `request.agent = this` (already set, but ensures it's correct)
  - Connects request callbacks to agent signals:
    - `this.stream_chunk.connect(request.on_stream_chunk)` (use existing signal)
    - `this.message_completed.connect(request.on_message_completed)`

- `private void unregister_tool_monitoring(string request_id)`
  - Gets request from map
  - Disconnects signals:
    - `this.stream_chunk.disconnect(request.on_stream_chunk)` (use existing signal)
    - `this.message_completed.disconnect(request.on_message_completed)`
  - Clears agent reference: `request.agent = null` (optional, for cleanup)
  - Removes from map: `active_tools.unset(request_id)`

#### Step 3: Add Callback Interface to RequestBase

**File**: `libollmchat/Tool/RequestBase.vala`

Add virtual callback methods that tools can override:
- `public virtual void on_stream_chunk(string new_text, bool is_thinking, Response.Chat response)`
  - Default: empty (does nothing)
  - Called when streaming chunk arrives

- `public virtual void on_message_completed(Message message)`
  - Default: empty (does nothing)
  - Called when message is completed successfully

**Note**: `on_message_failed()` is NOT needed - errors propagate as exceptions through `send_async()` which throws Error. Tools can handle errors in their own try/catch blocks if needed.

#### Step 4: Emit Signals in Agent.handle_stream_chunk()

**File**: `libollmchat/Agent/Base.vala`

Modify `handle_stream_chunk()` to emit signals BEFORE relaying to session:
```vala
public virtual void handle_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Emit signal for active tool requests (they're connected via register_tool_monitoring)
    // This allows tools like EditMode to receive streaming chunks in real-time
    // Use existing stream_chunk signal (already defined in Agent.Base)
    this.stream_chunk(new_text, is_thinking, response);
    
    // If response is done, emit message_completed signal for active tool requests
    // This allows tools to know when the message is complete and they can process/finalize
    if (response.done && response.message != null) {
        this.message_completed(response.message);
    }
    
    // Relay to session (existing code - for persistence and UI updates)
    this.session.handle_stream_chunk(new_text, is_thinking, response);
}
```

**Important**: 
- Use existing `stream_chunk` signal (already defined, no need to create new one)
- Only need to add `message_completed` signal
- Signals must be emitted BEFORE relaying to session, so tools receive chunks before they're persisted/displayed

#### Step 5: Emit Signals on Message Completion

**File**: `libollmchat/Agent/Base.vala`

Need to emit `message_completed` signal when messages finish.

**For message_completed:**
- Emit in `handle_stream_chunk()` when `response.done == true`
- At that point, `response.message` contains the completed message
- Emit: `this.message_completed(response.message)`

**Note**: `message_failed` is NOT needed - there's no existing error handling mechanism for failed messages. Errors in the streaming flow propagate as exceptions through `send_async()` which throws Error. Tools can handle errors in their own try/catch blocks if needed.

**Implementation:**
```vala
public virtual void handle_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Emit signal for active tool requests
    this.stream_chunk_received(new_text, is_thinking, response);
    
    // If response is done, emit message_completed
    if (response.done && response.message != null) {
        this.message_completed(response.message);
    }
    
    // Relay to session (existing code)
    this.session.handle_stream_chunk(new_text, is_thinking, response);
}
```

#### Step 6: Set request_id and Register in Tool.execute()

**File**: `libollmchat/Tool/Tool.vala`

Modify `execute()` to accept optional `request_id` parameter:
```vala
public virtual async string execute(Call.Chat chat_call, Json.Object parameters, string? request_id = null)
{
    // ... create request ...
    request.tool = this;
    request.agent = chat_call.agent;
    
    // If request_id provided, register for monitoring
    if (request_id != null && request.agent != null) {
        var agent_base = request.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.register_tool_monitoring(request_id, request);
        }
    }
    
    return yield request.execute();
}
```

#### Step 7: Pass request_id from Agent.execute_tools()

**File**: `libollmchat/Agent/Base.vala`

In `execute_tools()`, pass `tool_call.id` as request_id:
```vala
var result = yield tool.execute(this.chat_call, tool_call.function.arguments, tool_call.id);
```

#### Step 8: Implement Callbacks in EditMode.Request

**File**: `liboctools/EditMode/Request.vala`

Override the callback methods:
- `public override void on_stream_chunk(string new_text, bool is_thinking, Response.Chat response)`
  - Only process non-thinking content: `if (!is_thinking && new_text.length > 0)`
  - Call `process_streaming_content(new_text)`

- `public override void on_message_completed(Message message)`
  - Call `on_message_created(message)` (existing method)
  - Unregister: `agent.unregister_tool_monitoring(request_id)` - but we need request_id stored

Actually, need to store request_id in RequestBase so it can unregister itself.

#### Step 9: Add request_id to RequestBase

**File**: `libollmchat/Tool/RequestBase.vala`

- Add property: `public string request_id { get; set; default = ""; }`
- Exclude from JSON deserialization
- Set in `Tool.execute()` before registering

#### Step 10: Unregister in EditMode Cleanup

**File**: `liboctools/EditMode/Request.vala`

In cleanup methods (`on_message_created()`, `reply_with_errors()`):
```vala
if (this.request_id != "" && this.agent != null) {
    var agent_base = this.agent as Agent.Base;
    if (agent_base != null) {
        agent_base.unregister_tool_monitoring(this.request_id);
    }
}
```

## Detailed Implementation

### 1. Agent.Base Changes

```vala
// Add signal for tool monitoring (stream_chunk already exists, just need message_completed)
public signal void message_completed(Message message);

// Add registry
private Gee.HashMap<string, RequestBase> active_tools;

// In constructor
this.active_tools = new Gee.HashMap<string, RequestBase>();

// Add registration method
public void register_tool_monitoring(string request_id, RequestBase request)
{
    // Store tool reference to prevent garbage collection
    this.active_tools[request_id] = request;
    
    // Connect tool to agent signals
    request.agent = this;  // Ensure agent reference is set
    this.stream_chunk.connect(request.on_stream_chunk);  // Use existing stream_chunk signal
    this.message_completed.connect(request.on_message_completed);
    // Note: message_failed not needed - errors propagate as exceptions
    
    GLib.debug("Agent.register_tool_monitoring: Registered request '%s' for monitoring", request_id);
}

// Add unregistration method
private void unregister_tool_monitoring(string request_id)
{
    if (this.active_tools.has_key(request_id)) {
        var request = this.active_tools[request_id];
        
        // Disconnect signals
        this.stream_chunk.disconnect(request.on_stream_chunk);  // Use existing stream_chunk signal
        this.message_completed.disconnect(request.on_message_completed);
        // Note: message_failed not needed - errors propagate as exceptions
        
        // Clear reference (optional, for cleanup)
        request.agent = null;
        
        // Remove from registry
        this.active_tools.unset(request_id);
        
        GLib.debug("Agent.unregister_tool_monitoring: Unregistered request '%s'", request_id);
    }
}

// Make unregister public so requests can call it
public void unregister_tool(string request_id)
{
    this.unregister_tool_monitoring(request_id);
}

// Modify handle_stream_chunk to emit signals BEFORE relaying to session
// This is where the agent relays to sessions, so we emit signals here
public virtual void handle_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Emit signal for active tool requests (they're connected via register_tool_monitoring)
    // This allows tools like EditMode to receive streaming chunks in real-time
    // Use existing stream_chunk signal (already defined in Agent.Base)
    this.stream_chunk(new_text, is_thinking, response);
    
    // If response is done, emit message_completed signal for active tool requests
    // This allows tools to know when the message is complete and they can process/finalize
    if (response.done && response.message != null) {
        this.message_completed(response.message);
    }
    
    // Relay to session (existing code - for persistence and UI updates)
    this.session.handle_stream_chunk(new_text, is_thinking, response);
}
```

### 2. RequestBase Changes

```vala
// Add property for request_id
public string request_id { get; set; default = ""; }

// Exclude from deserialization
public virtual bool deserialize_property(string property_name, out Value value, ParamSpec pspec, Json.Node property_node)
{
    switch (property_name) {
        case "tool":
        case "agent":
        case "request_id":  // NEW
            value = Value(pspec.value_type);
            return true;
        // ...
    }
}

// Add virtual callback methods
public virtual void on_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Default: do nothing
    // Tools that need streaming override this
}

public virtual void on_message_completed(Message message)
{
    // Default: do nothing
    // Tools that need message completion override this
}

// Note: on_message_failed() not needed - errors propagate as exceptions through send_async()
// Tools can handle errors in their own try/catch blocks if needed
```

### 3. Tool.BaseTool.execute() Changes

```vala
// Modify execute() to accept optional request_id
public virtual async string execute(Call.Chat chat_call, Json.Object parameters, string? request_id = null)
{
    // Convert parameters Json.Object to Json.Node for deserialization
    var parameters_node = new Json.Node(Json.NodeType.OBJECT);
    parameters_node.set_object(parameters);
    
    // Deserialize parameters JSON into Request object
    var request = this.deserialize(parameters_node);
    if (request == null) {
        return "ERROR: Failed to create request object";
    }
    
    // Set tool and agent (not in JSON, set after deserialization)
    request.tool = this;
    request.agent = chat_call.agent;
    
    // Set request_id if provided
    if (request_id != null) {
        request.request_id = request_id;
    }
    
    // Register for monitoring if request_id is set
    if (request_id != null && request.agent != null) {
        var agent_base = request.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.register_tool_monitoring(request_id, request);
        }
    }
    
    return yield request.execute();
}
```

### 4. Agent.execute_tools() Changes

```vala
// In execute_tools(), pass tool_call.id as request_id
var result = yield tool.execute(this.chat_call, tool_call.function.arguments, tool_call.id);
```

### 5. EditMode.Request Changes

```vala
// Override on_stream_chunk callback
public override void on_stream_chunk(string new_text, bool is_thinking, Response.Chat response)
{
    // Only process non-thinking content (actual code blocks)
    if (!is_thinking && new_text.length > 0) {
        this.process_streaming_content(new_text);
    }
}

// Override on_message_completed callback
public override void on_message_completed(Message message)
{
    // Call existing on_message_created method
    this.on_message_created.begin(message);
}

// Unregister in cleanup methods
private async void on_message_created(OLLMchat.Message message)
{
    // ... existing code ...
    
    // Unregister from agent
    if (this.request_id != "" && this.agent != null) {
        var agent_base = this.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.unregister_tool(this.request_id);
        }
    }
    
    // ... rest of cleanup ...
}

private void reply_with_errors(OLLMchat.Response.Chat response, string message = "")
{
    // Unregister from agent
    if (this.request_id != "" && this.agent != null) {
        var agent_base = this.agent as Agent.Base;
        if (agent_base != null) {
            agent_base.unregister_tool(this.request_id);
        }
    }
    
    // ... rest of code ...
}
```

## Testing Plan

1. **Basic Flow Test**:
   - Activate edit mode for a file
   - Send message to LLM
   - Verify streaming chunks are received by EditMode
   - Verify code blocks are captured
   - Verify file changes are applied

2. **Multiple Tools Test**:
   - Activate edit mode
   - Call another tool
   - Verify only edit mode receives streaming (other tools don't override handle_stream_chunk)

3. **Cleanup Test**:
   - Activate edit mode
   - Complete message
   - Verify request is unregistered
   - Verify no memory leaks

4. **Error Handling Test**:
   - Activate edit mode
   - Cause error
   - Verify cleanup still happens

## Files to Modify

1. `libollmchat/Tool/RequestBase.vala` - Add request_id, on_stream_chunk(), on_message_completed()
2. `libollmchat/Tool/Tool.vala` - Modify execute() to accept tool_call_id
3. `libollmchat/Agent/Base.vala` - Add registry, modify handle_stream_chunk(), execute_tools()
4. `liboctools/EditMode/Request.vala` - Override on_stream_chunk(), on_message_completed(), unregister on cleanup

## Risks and Considerations

1. **Agent.Interface**: Need to decide if registration methods should be in interface or only Base
   - For now, cast to Agent.Base (only Base implements it)
   - Future: Could add to interface if needed

2. **Multiple Active Requests**: What if same tool_call_id is used twice?
   - Shouldn't happen (tool_call.id is unique per call)
   - But handle gracefully: overwrite old registration

3. **Request Lifecycle**: Ensure unregistration happens in all cleanup paths
   - on_message_created() - success path
   - reply_with_errors() - error path
   - disconnect_signals() - cleanup path

4. **Performance**: Iterating over active_tool_requests on every chunk
   - Should be fine (usually 0-1 active requests)
   - Can optimize later if needed

## Success Criteria

- ✅ EditMode receives streaming chunks during LLM response
- ✅ Code blocks are captured in real-time
- ✅ File changes are applied when message is done
- ✅ No memory leaks (requests properly unregistered)
- ✅ Generic solution works for other tools that need streaming
- ✅ No regression in existing functionality
