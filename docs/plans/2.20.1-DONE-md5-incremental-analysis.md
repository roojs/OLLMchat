# 2.20.1. MD5-Based Incremental Analysis

## Status

✅ **COMPLETED**

## Coding Standards Checklist

Before implementing, ensure all code follows these standards:

- **Nullable types**: Avoid nullable types where possible (using default objects/flags instead)
- **Null checks**: Avoid generic null checks, only use them where the design explicitly requires null
- **String interpolation**: Avoid `@"..."` string interpolation except for multi-line usage/help text or documentation
- **Temporary variables**: Avoid one-use temporaries or trivial aliases when describing new code
- **Brace placement**: Line breaks for namespaces/classes/methods, inline for control structures
- **`this.` prefix**: Always use `this.` for instance members in new/modified Vala code
- **GLib prefix & using statements**: Use fully-qualified `GLib.*` and avoid `using` imports
- **Property initialization**: Initialize properties with defaults (`get; set; default =` or field defaults) instead of constructors
- **Line length & breaking**: Break long lines (method calls, concatenations) for readability
- **StringBuilder usage**: Avoid `GLib.StringBuilder` unless building strings in loops with hundreds of iterations. Use `string.joinv()` for joining arrays and `+` for simple concatenation
- **ArrayList for strings**: Avoid `Gee.ArrayList<string>` when building arrays just to join them. Use `string[]` arrays instead
- **HashMap/ArrayList access**: Use `.get()` and `.set()` methods instead of array-style accessors
- **Early returns**: Use early returns to reduce nesting and avoid else clauses when possible

## Purpose

Add element-level caching using MD5 checksums to avoid redundant LLM analysis calls for unchanged code sections. This dramatically reduces LLM API calls and embedding API calls when only parts of a file have changed.

## Problem Statement

Currently, even if only one method in a file changed, the entire file is re-analyzed and re-vectorized, wasting LLM calls and embedding API calls.

## Current Implementation

The current indexing pipeline works as follows:

1. **File-Level Check** (`Indexer.index_file()`):
   - Checks `file.last_vector_scan >= file.mtime_on_disk()` to skip entire files if not modified
   - If file is modified, proceeds to full re-indexing

2. **Full Re-Indexing Process**:
   - Parses entire file with `Tree.parse()` to extract all elements
   - Analyzes ALL elements with `Analysis.analyze_tree()` (calls LLM for each element)
   - **Deletes ALL existing metadata** for the file: `DELETE FROM vector_metadata WHERE file_id = ...` (VectorBuilder line 74)
   - Re-vectorizes ALL elements (creates new embeddings for everything)
   - Creates new VectorMetadata entries for all elements

**Problem**: Even if only one method changed, the entire file is re-analyzed and re-vectorized, wasting LLM calls and embedding API calls.

## Proposed MD5-Based Approach

The plan proposes element-level caching using MD5 checksums:

1. **Before Analysis - Load Existing Metadata**:
   - Query existing `vector_metadata` entries for the file
   - Create a cache/map of existing metadata indexed by element identifier (e.g., `ast_path` or `element_type:element_name`)

2. **During Tree Parsing**:
   - For each element found in the file:
     - Calculate MD5 hash of the element's code content (`tree.lines_to_string(start_line, end_line)`)
     - Look up existing metadata for this element (by `ast_path` or similar)
     - If existing metadata found:
       - Compare stored MD5 with calculated MD5
       - If MD5 matches: content unchanged, skip LLM analysis
       - If MD5 differs: content changed, need to re-analyze
     - If no existing metadata: new element, need to analyze

3. **Analysis Phase**:
   - Only call LLM for elements where:
     - MD5 differs (content changed)
     - No existing metadata (new element)
     - Element name doesn't match (likely renamed/changed, even if ast_path matches)
   - For unchanged elements (MD5 matches):
     - Reuse existing `description` from metadata
     - Still update `start_line` and `end_line` if they changed (line numbers may shift)
     - Skip LLM call entirely
   - For legacy data (empty MD5, but element name matches):
     - Calculate MD5 for current content
     - Update metadata with calculated MD5
     - Reuse existing `description` (don't re-analyze)
     - Update line numbers if they changed
     - Skip LLM call (migration without re-analysis)

4. **Vectorization Phase**:
   - Only create new embeddings for:
     - Changed elements (MD5 differs)
     - New elements (no existing metadata)
   - For unchanged elements:
     - Reuse existing vector_id and embedding in FAISS
     - Just update metadata (line numbers, etc.) if needed

5. **Cleanup**:
   - Remove metadata entries for elements that no longer exist in the file
   - Update metadata for elements that moved (line numbers changed but MD5 same)

## Required Modifications

### 1. Database Schema Changes

**Add `md5_hash` column to `vector_metadata` table:**

```sql
ALTER TABLE vector_metadata ADD COLUMN md5_hash TEXT NOT NULL DEFAULT '';
```

**Update `VectorMetadata.vala`:**
- Add `md5_hash` property (string, stored in database)
- Update `initDB()` to include migration for existing databases

### 2. Tree Modifications

**Cache lives in Tree** (Tree is created per file and passed around):
- Tree has a property: `cached_metadata` (hashmap)
- Key: `ast_path` (or `element_type:element_name` if ast_path empty)
- Value: `VectorMetadata` object from database
- Tree loads cache after construction (or after parsing)

**Tree does matching and MD5 calculation**:
- After parsing, Tree calculates MD5 for each element
- Tree matches elements with cached metadata
- Tree pre-populates `Tree.elements` with descriptions for unchanged elements

**Key changes**:
1. Add `cached_metadata` property to Tree (hashmap keyed by ast_path)
2. Add method to load cache from database (after construction, before parsing)
3. Modify `extract_element_metadata()` to calculate MD5 and match with cache when creating each element
4. Pre-populate descriptions in elements as they're created (no separate matching pass needed)

### 3. Analysis Layer Modifications

**Current behavior** (`Analysis.analyze_tree()`):
- Always calls LLM for each element (unless `should_skip_llm()` returns true)

**New behavior needed**:
- **Tree.elements already have descriptions pre-populated by Tree** (for unchanged elements)
- Skip LLM call if element already has description (Tree already populated it)
- Only analyze elements with empty descriptions (changed/new elements)
- Still update line numbers if they changed

**Key changes**:
1. No need to pass cache to Analysis - descriptions are already in Tree.elements
2. Check if element.description is empty before calling LLM
3. Only call `analyze_element()` for elements that need analysis (empty description)

### 4. VectorBuilder Modifications

**Current behavior** (`VectorBuilder.process_file()`):
- Line 74: Deletes ALL metadata: `DELETE FROM vector_metadata WHERE file_id = ...`
- Always processes all elements as new

**New behavior needed**:
- **VectorBuilder receives Tree with pre-populated descriptions**:
  - Unchanged elements already have descriptions (from cache)
  - Changed/new elements have empty descriptions (need analysis)
  - VectorBuilder checks if element needs new embedding (no vector_id or changed)
- **During processing**: 
  - Skip analysis for unchanged elements (already have descriptions)
  - Only create new embeddings for changed/new elements
  - Update line numbers for unchanged elements if they shifted
  - Remove metadata for deleted elements

**Key changes**:
1. Check which elements need new embeddings (access Tree.cached_metadata to check vector_id)
2. Only delete/update specific metadata entries (not all)
3. Only create new embeddings for changed elements

### 5. Indexer Modifications

**Current flow** (`Indexer.index_file()`):
```
1. Check file mtime → skip if unchanged
2. Parse file (Tree.parse())
3. Analyze all elements (Analysis.analyze_tree())
4. Analyze file summary (Analysis.analyze_file())
5. Process file (VectorBuilder.process_file()) → deletes all, re-vectorizes all
```

**New flow needed**:
```
1. Check file mtime → skip if unchanged
2. Create Tree(file)
3. Tree loads existing metadata into cached_metadata property:
   - Query: SELECT * FROM vector_metadata WHERE file_id = ?
   - Key cache by ast_path (or element_type:element_name)
4. Parse file (Tree.parse())
   - During parsing, `extract_element_metadata()` calculates MD5 and matches with cache:
     - Calculate MD5 using tree.lines_to_string(element.start_line, element.end_line)
     - Look up in cached_metadata by ast_path (or fallback key)
     - If found:
       - If MD5 exists and matches: unchanged → copy description to element
       - If MD5 exists and differs: changed → leave description empty
       - If MD5 is empty (legacy) and element name matches: legacy → copy description, calculate MD5
       - If MD5 is empty (legacy) and element name differs: changed → leave description empty
     - If not found: new element → leave description empty
5. Pass Tree to Analysis (descriptions already pre-populated for unchanged elements)
6. Analyze only elements with empty descriptions (Analysis.analyze_tree())
7. Analyze file summary (Analysis.analyze_file())
8. Pass Tree to VectorBuilder (all elements have descriptions now, cache still available)
9. Process file incrementally (VectorBuilder.process_file()):
    - For each element, check if it needs new embedding (no vector_id in cache or changed)
    - Update unchanged elements (line numbers only, reuse vector_id)
    - Create new embeddings for changed/new elements
    - Remove metadata for deleted elements (not in tree.elements)
```

### 6. MD5 Calculation

**Where to calculate MD5**:
- In `Tree.extract_element_metadata()`: Calculate MD5 when creating each element
- Use `tree.lines_to_string(element.start_line, element.end_line)` to get code content
- Use `GLib.Checksum` with `ChecksumType.MD5` to compute hash
- Cache lives in Tree, so MD5 calculation happens in Tree during element extraction
- Tree can access its own cached_metadata property for matching

**What to hash**:
- Element's code content: `tree.lines_to_string(element.start_line, element.end_line)`
- This ensures we detect any code changes, not just line number shifts

### 7. Element Matching Strategy

**How to match parsed elements with cached metadata**:

1. **Primary key: `ast_path`** (most reliable)
   - Format: `namespace-class-method` or `namespace-function`
   - Unique identifier for element location in code
   - Works even if line numbers change

2. **Fallback: `element_type:element_name`** (if ast_path not available)
   - Less reliable (duplicate names possible)
   - Use only if ast_path is empty

3. **Handle edge cases**:
   - Elements that moved (same ast_path, different line numbers)
   - Elements that were renamed (different name, same location)
   - Elements that were deleted (in cache but not in parsed tree)

4. **Legacy data handling (no MD5 hash)**:
   - If existing metadata has empty `md5_hash` (legacy data from before Phase 1)
   - And element matches by `ast_path` (or `element_type:element_name` if ast_path unavailable)
   - Then:
     - Calculate MD5 for current element content
     - Update metadata with calculated MD5
     - **Reuse existing description** (don't re-analyze)
     - Update line numbers if they changed
     - This allows migration of old data without triggering unnecessary LLM calls
   - Only re-analyze if element name doesn't match (likely renamed/changed)

### 8. FAISS Vector Updates

**Current behavior**:
- Always creates new vectors for all elements
- Old vectors remain in FAISS (orphaned, but not harmful)

**New behavior needed**:
- For unchanged elements: Keep existing vector_id, don't create new embedding
- For changed elements: 
  - Create new vector with new embedding
  - Update metadata with new vector_id
  - **Leave old vector in FAISS** (orphaned, but harmless - FAISS doesn't support deletion)
- For deleted elements: 
  - Remove metadata entry from database
  - **Leave vector in FAISS** (orphaned, but harmless - FAISS doesn't support deletion)

**Note**: 
- FAISS doesn't support vector deletion or in-place updates
- Orphaned vectors remain in FAISS but are harmless (not referenced by metadata)
- Only metadata entries are deleted/updated
- New vectors are appended to FAISS index

## Deliverables

- [x] Add `md5_hash` column to `vector_metadata` table
- [x] Update `VectorMetadata` class with `md5_hash` property
- [x] Implement MD5 calculation for element code content
- [x] Modify `Tree` class to:
  - Add `cached_metadata` property (hashmap keyed by ast_path)
  - Add method to load cache from database (after construction, before parsing)
  - Modify `extract_element_metadata()` to calculate MD5 and match with cache when creating each element
  - Pre-populate descriptions in elements as they're created (no separate matching pass needed)
- [x] Modify `Analysis` layer to skip elements that already have descriptions
- [x] Modify `VectorBuilder` to check which elements need new embeddings (access Tree.cached_metadata)
- [x] Modify `Indexer` to implement incremental update flow
- [x] Implement element matching by `ast_path`
- [x] Implement legacy data handling (empty MD5, reuse description if element name matches)
- [x] Handle FAISS vector updates for changed elements
- [ ] Test incremental updates with file modifications
- [ ] Test legacy data migration (existing data without MD5)

## Code Changes

### 1. VectorMetadata.vala - Add md5_hash Property and Migration

**Add property to class:**
```vala
/**
 * MD5 hash of element's code content for change detection.
 * Empty string for legacy data (before MD5 support was added).
 * Stored in database for incremental analysis.
 */
public string md5_hash { get; set; default = ""; }
```

**Update initDB() method to add migration:**
```vala
public static void initDB(SQ.Database db)
{
	// ... existing CREATE TABLE code ...
	
	// Migrate existing databases: add ast_path column if it doesn't exist
	string errmsg;
	if (Sqlite.OK != db.db.exec("ALTER TABLE vector_metadata ADD COLUMN ast_path TEXT NOT NULL DEFAULT ''", null, out errmsg)) {
		// Column might already exist, which is fine
		if (!errmsg.contains("duplicate column name")) {
			GLib.debug("Migration note (may be expected): %s", errmsg);
		}
	}
	
	// NEW: Migrate existing databases: add md5_hash column if it doesn't exist
	if (Sqlite.OK != db.db.exec("ALTER TABLE vector_metadata ADD COLUMN md5_hash TEXT NOT NULL DEFAULT ''", null, out errmsg)) {
		// Column might already exist, which is fine
		if (!errmsg.contains("duplicate column name")) {
			GLib.debug("Migration note (may be expected): %s", errmsg);
		}
	}
	
	// ... existing index creation code ...
}
```

**Update CREATE TABLE statement (for new databases):**
```vala
var query = "CREATE TABLE IF NOT EXISTS vector_metadata (" +
	"id INTEGER PRIMARY KEY AUTOINCREMENT, " +
	"vector_id INTEGER NOT NULL, " +
	"file_id INTEGER NOT NULL, " +
	"start_line INTEGER NOT NULL, " +
	"end_line INTEGER NOT NULL, " +
	"element_type TEXT NOT NULL, " +
	"element_name TEXT NOT NULL, " +
	"description TEXT NOT NULL DEFAULT '', " +
	"ast_path TEXT NOT NULL DEFAULT '', " +
	"md5_hash TEXT NOT NULL DEFAULT ''" +  // NEW
	");";
```

### 2. Tree.vala - Add Cache Loading and MD5 Matching

**Add property to Tree class:**
```vala
/**
 * Cached metadata from database, keyed by ast_path (or element_type:element_name fallback).
 * Loaded before parsing to enable incremental analysis.
 */
public Gee.HashMap<string, VectorMetadata> cached_metadata { get; private set; default = new Gee.HashMap<string, VectorMetadata>(); }
```

**Add method to load cached metadata:**
```vala
/**
 * Loads existing metadata from database into cached_metadata hashmap.
 * 
 * Should be called after Tree construction but before parse().
 * Keys cache by ast_path (or element_type:element_name if ast_path is empty).
 * 
 * @param sql_db The SQLite database instance for querying metadata
 * @throws GLib.Error if database query fails
 */
public async void load_cached_metadata(SQ.Database sql_db) throws GLib.Error
{
	this.cached_metadata.clear();
	
	var results = new Gee.ArrayList<VectorMetadata>();
	yield VectorMetadata.query(sql_db).select_async("WHERE file_id = " + this.file.id.to_string(), results);
	
	foreach (var metadata in results) {
		// Use ast_path as key, fallback to element_type:element_name if ast_path is empty
		var cache_key = metadata.ast_path == "" ? 
			metadata.element_type + ":" + metadata.element_name : 
			metadata.ast_path;
		
		// If key already exists (duplicate ast_path or fallback), keep the one with non-empty md5_hash
		if (this.cached_metadata.has_key(cache_key)) {
			// Prefer entry with non-empty md5_hash (more recent)
			if (this.cached_metadata.get(cache_key).md5_hash == "" && metadata.md5_hash != "") {
				this.cached_metadata.set(cache_key, metadata);
			}
		} else {
			this.cached_metadata.set(cache_key, metadata);
		}
	}
	
	GLib.debug("Loaded %d cached metadata entries for file %s", 
	           this.cached_metadata.size, this.file.path);
}
```


**Add method to match element with cache and populate description:**
```vala
/**
 * Matches element with cached metadata and pre-populates description if unchanged.
 * 
 * Calculates MD5 hash for element's code content, looks up in cached_metadata,
 * and if found with matching MD5, copies description from cache.
 * 
 * @param metadata The VectorMetadata element to match
 */
private void match_element_with_cache(VectorMetadata metadata)
{
	// Calculate MD5 for current element content
	var element_code = this.lines_to_string(metadata.start_line, metadata.end_line);
	if (element_code == "") {
		return;
	}
	
	var checksum = new GLib.Checksum(GLib.ChecksumType.MD5);
	checksum.update((uint8[])element_code.to_utf8(), -1);
	metadata.md5_hash = checksum.get_string();
	
	// Determine cache key (ast_path or fallback)
	var cache_key = metadata.ast_path == "" ? 
		metadata.element_type + ":" + metadata.element_name : 
		metadata.ast_path;
	
	// Look up in cache
	if (!this.cached_metadata.has_key(cache_key)) {
		return;
	}
	
	var cached = this.cached_metadata.get(cache_key);
	
	// Handle legacy data (empty MD5 in cache)
	if (cached.md5_hash == "") {
		// Legacy data: check if element name matches
		if (cached.element_name == metadata.element_name && cached.element_type == metadata.element_type) {
			// Element name matches - reuse description, calculate and store MD5
			metadata.description = cached.description;
			// Update cached metadata with calculated MD5 (for next time)
			cached.md5_hash = metadata.md5_hash;
			// Update line numbers if they changed
			if (cached.start_line != metadata.start_line || cached.end_line != metadata.end_line) {
				cached.start_line = metadata.start_line;
				cached.end_line = metadata.end_line;
			}
			// Store vector_id from cache for VectorBuilder to reuse
			metadata.vector_id = cached.vector_id;
			GLib.debug("Legacy element %s (%s) - reused description, calculated MD5", 
			           metadata.element_name, metadata.element_type);
		}
		return;
	}
	
	// Modern data: compare MD5 hashes
	if (cached.md5_hash != metadata.md5_hash) {
		return;
	}
	
	// MD5 matches - element unchanged, reuse description
	metadata.description = cached.description;
	// Update line numbers if they changed (element moved but content same)
	if (cached.start_line != metadata.start_line || cached.end_line != metadata.end_line) {
		cached.start_line = metadata.start_line;
		cached.end_line = metadata.end_line;
	}
	// Store vector_id from cache for VectorBuilder to reuse
	metadata.vector_id = cached.vector_id;
	GLib.debug("Unchanged element %s (%s) - reused description", 
	           metadata.element_name, metadata.element_type);
}
```

**Modify extract_element_metadata() to call match_element_with_cache():**
```vala
private VectorMetadata? extract_element_metadata(TreeSitter.Node node, 
	string code_content,
	string? parent_enum_name = null, 
	string? current_namespace = null, 
	string? parent_class_name = null)
{
	// ... existing code to create metadata and set properties ...
	
	// Build AST path from node by traversing up the AST using base class method
	metadata.ast_path = this.ast_path(node, code_content);
	GLib.debug("extract_element_metadata: set ast_path='%s' for %s %s (lines %d-%d)", 
		metadata.ast_path, element_type, element_name, metadata.start_line, metadata.end_line);
	
	// NEW: Match with cache and pre-populate description if unchanged
	this.match_element_with_cache(metadata);
	
	return metadata;
}
```

### 3. Analysis.vala - Skip Elements with Pre-populated Descriptions

**Modify analyze_tree() method:**
```vala
public async Tree analyze_tree(Tree tree) throws GLib.Error
{
	// Ensure prompt template is loaded (cached after first load)
	this.get_prompt_template();
	
	// Process elements sequentially
	int success_count = 0;
	int failure_count = 0;
	int skipped_count = 0;  // NEW: Track skipped elements
	int total_elements = tree.elements.size;
	int element_number = 0;
	
	foreach (var element in tree.elements) {
		element_number++;
		
		// NEW: Skip if element already has description (pre-populated from cache)
		if (element.description != "" && element.description != null) {
			skipped_count++;
			// Emit signal for skipped elements too
			this.element_analyzed(element.element_name, element_number, total_elements);
			continue;
		}
		
		if (this.should_skip_llm(element)) {
			element.description = "";
			// Emit signal for skipped elements too
			this.element_analyzed(element.element_name, element_number, total_elements);
			continue;
		}
		
		try {
			GLib.debug("Analyzing: %s (%s)", element.element_name, element.element_type);
			yield this.analyze_element(element, tree);
			
			// Emit signal after element is analyzed
			this.element_analyzed(element.element_name, element_number, total_elements);
			
			if (element.description != "" && element.description != null) {
				success_count++;
				continue;
			}
			failure_count++;
		} catch (GLib.Error e) {
			GLib.warning("Failed to analyze element %s (%s) in file %s: %s", 
							element.element_name, element.element_type, tree.file.path, e.message);
			element.description = "";
			// Emit signal even if analysis failed
			this.element_analyzed(element.element_name, element_number, total_elements);
			failure_count++;
		}
	}
	
	GLib.debug("Processing file %s - %d elements processed, %d skipped (cached), %d succeeded, %d failed", 
				tree.file.path, total_elements, skipped_count, success_count, failure_count);
	
	// ... rest of method ...
}
```

### 4. VectorBuilder.vala - Incremental Vector Processing

**Modify process_file() method:**
```vala
public async void process_file(Tree tree) throws GLib.Error
{
	if (tree.elements.size == 0) {
		GLib.debug("No elements to process in file: %s", tree.file.path);
		return;
	}
	
	// Separate elements into unchanged (reuse vector) and changed/new (create new vector)
	var unchanged_elements = new Gee.ArrayList<VectorMetadata>();
	var changed_elements = new Gee.ArrayList<VectorMetadata>();
	var elements_to_delete = new Gee.HashSet<int64>();  // Track metadata IDs to delete
	
	// Build set of current element keys (ast_path or fallback) for deletion detection
	var current_keys = new Gee.HashSet<string>();
	foreach (var element in tree.elements) {
		current_keys.add(element.ast_path == "" ? 
			element.element_type + ":" + element.element_name : 
			element.ast_path);
	}
	
	// Check each element
	foreach (var element in tree.elements) {
		var cache_key = element.ast_path == "" ? 
			element.element_type + ":" + element.element_name : 
			element.ast_path;
		
		if (!tree.cached_metadata.has_key(cache_key)) {
			// New element - needs new embedding
			changed_elements.add(element);
			continue;
		}
		
		var cached = tree.cached_metadata.get(cache_key);
		
		// Check if element is unchanged (can reuse existing vector)
		// Element is unchanged if:
		// 1. MD5 hashes match (or both are empty for legacy data that matched by name)
		// 2. Element has vector_id set (from Tree.match_elements_with_cache())
		bool is_unchanged = false;
		
		if (element.vector_id > 0 && element.vector_id == cached.vector_id) {
			// Vector ID matches - check MD5
			if (element.md5_hash != "" &&
				cached.md5_hash != "" &&
				element.md5_hash == cached.md5_hash) {
				// MD5 matches - definitely unchanged
				is_unchanged = true;
			} else if (element.md5_hash == "" &&
						cached.md5_hash == "" &&
						element.element_name == cached.element_name &&
						element.element_type == cached.element_type) {
				// Both MD5 empty (legacy) but names match - unchanged (Tree already matched)
				is_unchanged = true;
			}
		}
		
		if (is_unchanged) {
			// Unchanged element - reuse existing vector
			unchanged_elements.add(element);
			// Line number updates handled below in unchanged_elements processing
		} else {
			// Changed element - needs new embedding
			changed_elements.add(element);
			// Mark old metadata for deletion (will be replaced with new entry)
			if (cached.id > 0) {
				elements_to_delete.add(cached.id);
			}
		}
	}

	// Delete metadata for elements that no longer exist in file
	foreach (var entry in tree.cached_metadata.entries) {
		if (current_keys.contains(entry.key)) {
			continue;
		}
		
		// Element was deleted from file
		if (entry.value.id > 0) {
			elements_to_delete.add(entry.value.id);
		}
	}

	// Delete old metadata entries
	foreach (var id in elements_to_delete) {
		VectorMetadata.query(this.sql_db).deleteId(id);
	}
	
	GLib.debug("VectorBuilder.process_file: %d unchanged (reuse vector), %d changed/new (create vector), %d deleted", 
	           unchanged_elements.size, changed_elements.size, elements_to_delete.size);
	
	// Process unchanged elements (just update metadata if needed)
	foreach (var element in unchanged_elements) {
		var cache_key = element.ast_path == "" ? 
			element.element_type + ":" + element.element_name : 
			element.ast_path;
		
		if (!tree.cached_metadata.has_key(cache_key)) {
			continue;
		}
		
		var cached = tree.cached_metadata.get(cache_key);
		// Check if metadata needs updating (line numbers or MD5 hash)
		bool needs_update = false;
		
		if (cached.start_line != element.start_line || cached.end_line != element.end_line) {
			cached.start_line = element.start_line;
			cached.end_line = element.end_line;
			needs_update = true;
		}
		
		// Update MD5 if it was just calculated (legacy data migration)
		if (element.md5_hash != "" && cached.md5_hash == "") {
			cached.md5_hash = element.md5_hash;
			needs_update = true;
		}
		
		// Update description if it changed (shouldn't happen for unchanged, but be safe)
		if (cached.description != element.description) {
			cached.description = element.description;
			needs_update = true;
		}
		
		if (needs_update) {
			cached.saveToDB(this.sql_db, false);
		}
	}

	// Process changed/new elements (create new embeddings)
	if (changed_elements.size == 0) {
		GLib.debug("No changed elements to vectorize for file: %s", tree.file.path);
		return;
	}
	
	// Format changed elements into documents
	var documents = new Gee.ArrayList<string>();
	
	foreach (var element in changed_elements) {
		documents.add(this.format_element_document(element, tree));
	}
	
	// ... existing embedding code ...
	
	// Get embed model from codebase_search tool config
	if (!this.config.tools.has_key("codebase_search")) {
		throw new GLib.IOError.FAILED("Codebase search tool config not found");
	}
	
	var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
	if (!tool_config.enabled) {
		throw new GLib.IOError.FAILED("Codebase search tool is disabled");
	}
	
	if (tool_config.embed.model == "" || tool_config.embed.connection == "" || 
	    !this.config.connections.has_key(tool_config.embed.connection)) {
		throw new GLib.IOError.FAILED("Invalid embed_model configuration");
	}
	
	// Create client from connection
	var embed_response = yield new OLLMchat.Client(
		this.config.connections.get(tool_config.embed.connection)
	).embed_array(
		tool_config.embed.model, 
		documents,
		-1,
		false,
		tool_config.embed.options
	);
	
	if (embed_response.embeddings.size == 0) {
		throw new GLib.IOError.FAILED("Failed to get embeddings for file");
	}
	
	if (embed_response.embeddings.size != documents.size) {
		throw new GLib.IOError.FAILED(
			"Embedding count mismatch: expected " +
			documents.size.to_string() +
			", got " +
			embed_response.embeddings.size.to_string());
	}
	
	// Convert embeddings to float arrays and store in FAISS
	var vector_batch = OLLMvector.FloatArray(this.database.dimension);
	
	// Get current vector count to determine starting vector_id
	int64 start_vector_id = (int64)this.database.vector_count;
	
	foreach (var embedding in embed_response.embeddings) {
		vector_batch.add(this.embed_to_floats(embedding));
	}
	
	// Add vectors to FAISS index
	this.database.add_vectors_batch(vector_batch);
	
	// Store metadata in SQL database for changed elements
	for (int j = 0; j < changed_elements.size; j++) {
		var element = changed_elements.get(j);
		element.vector_id = start_vector_id + j;
		element.saveToDB(this.sql_db, false);
	}
	
	GLib.debug("Completed processing %d changed elements from file: %s", changed_elements.size, tree.file.path);
}
```

### 5. Indexer.vala - Update Flow to Load Cache Before Parsing

**Modify index_file() method:**
```vala
private async bool index_file(OLLMfiles.File file, bool force = false) throws GLib.Error
{
	// ... existing skip checks ...
	
	GLib.debug("Processing file '%s'", file.path);
	
	// Create Tree
	var tree = new Tree(file);
	
	// NEW: Load existing metadata into cache before parsing
	yield tree.load_cached_metadata(this.sql_db);
	
	// Parse file (this will also call match_elements_with_cache() after parsing)
	yield tree.parse();
	
	if (tree.elements.size == 0) {
		// ... existing empty elements handling ...
	}
	
	var analysis = new Analysis(this.config, this.sql_db);
	
	// Connect to element_analyzed signal and forward as element_scanned
	analysis.element_analyzed.connect((element_name, element_number, total_elements) => {
		this.element_scanned(element_name, element_number, total_elements);
	});
	
	// Analyze only elements that need it (empty descriptions)
	tree = yield analysis.analyze_tree(tree);
	
	// Analyze file and create file-level summary
	tree = yield analysis.analyze_file(tree);
	
	// Process file incrementally (only changed/new elements get new embeddings)
	yield new VectorBuilder(this.config, this.vector_db, this.sql_db).process_file(tree);
	
	// ... existing file save code ...
}
```

### 6. VectorMetadata.vala - Add md5_hash to saveToDB()

**Update saveToDB() method (if needed):**
The `SQ.Query` system should automatically handle the `md5_hash` property if it's defined as a property with get/set. No changes needed to `saveToDB()` method itself, but ensure the property is properly defined (see section 1 above).

## Related Plans

- 2.20-codebase-scanner-improvements.md - Parent plan
- 2.10-codebase-search-tool.md - Existing codebase search infrastructure
