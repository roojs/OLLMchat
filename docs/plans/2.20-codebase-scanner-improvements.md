# 2.20. Codebase Scanner Improvements

## Status

⏳ **PENDING**

## Purpose

Improve the codebase scanner to provide folder-level and project-level summaries, optimize indexing with MD5-based caching, ensure documentation is properly indexed, and add support for binary file analysis (images). This would be a huge advantage to the LLM to have information about the project it's working on.

## Implementation Phases

This plan is broken down into 6 distinct phases that can be implemented incrementally:

1. **Phase 1: MD5-Based Incremental Analysis** - Add element-level caching to avoid redundant LLM calls
2. **Phase 2: Documentation File Verification** - Ensure all documentation files are properly indexed
3. **Phase 3: Binary File Handling - Image Analysis** - Add vision model support for image indexing
4. **Phase 4: Folder-Level Summaries** - Generate one-line descriptions for each folder
5. **Phase 5: Project-Level Summaries** - Generate detailed project descriptions with coding standards detection
6. **Phase 6: Integration & Testing** - End-to-end testing and optimization

## Overview of Enhancements

This plan includes several enhancements to the codebase scanner:

- **MD5-Based Incremental Analysis**: Element-level caching to avoid redundant LLM calls for unchanged code
- **Documentation File Verification**: Ensure all documentation files are properly indexed and searchable
- **Binary File Handling**: Image analysis using vision models to make images searchable by content
- **Folder-Level Summaries**: One-line descriptions for each directory/folder
- **Project-Level Summaries**: Detailed project descriptions with coding standards detection

## Benefits

- LLM gets better context about the project structure
- Faster access to project information
- Better understanding of code organization
- Can provide project overview to agents automatically

---

## Phase 1: MD5-Based Incremental Analysis

**Status**: ⏳ PENDING

**Goal**: Add element-level caching using MD5 checksums to avoid redundant LLM analysis calls for unchanged code sections.

### Problem Statement

Currently, even if only one method in a file changed, the entire file is re-analyzed and re-vectorized, wasting LLM calls and embedding API calls.

### MD5-Based Incremental Analysis

### Current Implementation

The current indexing pipeline works as follows:

1. **File-Level Check** (`Indexer.index_file()`):
   - Checks `file.last_vector_scan >= file.mtime_on_disk()` to skip entire files if not modified
   - If file is modified, proceeds to full re-indexing

2. **Full Re-Indexing Process**:
   - Parses entire file with `Tree.parse()` to extract all elements
   - Analyzes ALL elements with `Analysis.analyze_tree()` (calls LLM for each element)
   - **Deletes ALL existing metadata** for the file: `DELETE FROM vector_metadata WHERE file_id = ...` (VectorBuilder line 74)
   - Re-vectorizes ALL elements (creates new embeddings for everything)
   - Creates new VectorMetadata entries for all elements

**Problem**: Even if only one method changed, the entire file is re-analyzed and re-vectorized, wasting LLM calls and embedding API calls.

### Proposed MD5-Based Approach

The plan proposes element-level caching using MD5 checksums:

1. **Before Analysis - Load Existing Metadata**:
   - Query existing `vector_metadata` entries for the file
   - Create a cache/map of existing metadata indexed by element identifier (e.g., `ast_path` or `element_type:element_name`)

2. **During Tree Parsing**:
   - For each element found in the file:
     - Calculate MD5 hash of the element's code content (`tree.lines_to_string(start_line, end_line)`)
     - Look up existing metadata for this element (by `ast_path` or similar)
     - If existing metadata found:
       - Compare stored MD5 with calculated MD5
       - If MD5 matches: content unchanged, skip LLM analysis
       - If MD5 differs: content changed, need to re-analyze
     - If no existing metadata: new element, need to analyze

3. **Analysis Phase**:
   - Only call LLM for elements where:
     - MD5 differs (content changed)
     - No existing metadata (new element)
   - For unchanged elements (MD5 matches):
     - Reuse existing `description` from metadata
     - Still update `start_line` and `end_line` if they changed (line numbers may shift)
     - Skip LLM call entirely

4. **Vectorization Phase**:
   - Only create new embeddings for:
     - Changed elements (MD5 differs)
     - New elements (no existing metadata)
   - For unchanged elements:
     - Reuse existing vector_id and embedding in FAISS
     - Just update metadata (line numbers, etc.) if needed

5. **Cleanup**:
   - Remove metadata entries for elements that no longer exist in the file
   - Update metadata for elements that moved (line numbers changed but MD5 same)

## Required Modifications

### 1. Database Schema Changes

**Add `md5_hash` column to `vector_metadata` table:**

```sql
ALTER TABLE vector_metadata ADD COLUMN md5_hash TEXT NOT NULL DEFAULT '';
```

**Update `VectorMetadata.vala`:**
- Add `md5_hash` property (string, stored in database)
- Update `initDB()` to include migration for existing databases

### 2. VectorBuilder Modifications

**Current behavior** (`VectorBuilder.process_file()`):
- Line 74: Deletes ALL metadata: `DELETE FROM vector_metadata WHERE file_id = ...`
- Always processes all elements as new

**New behavior needed**:
- **Before deleting**: Load existing metadata for the file into a map
- **Before analysis**: Calculate MD5 for each element and check against cache
- **During processing**: 
  - Skip analysis for unchanged elements (MD5 matches)
  - Only create new embeddings for changed/new elements
  - Update line numbers for unchanged elements if they shifted
  - Remove metadata for deleted elements

**Key changes**:
1. Load existing metadata before analysis (not after deletion)
2. Match elements by `ast_path` (most reliable identifier)
3. Calculate MD5 hash for element code content
4. Compare MD5 before calling Analysis layer
5. Pass "skip analysis" flag to Analysis layer for unchanged elements
6. Only delete/update specific metadata entries (not all)
7. Only create new embeddings for changed elements

### 3. Analysis Layer Modifications

**Current behavior** (`Analysis.analyze_tree()`):
- Always calls LLM for each element (unless `should_skip_llm()` returns true)

**New behavior needed**:
- Accept pre-populated descriptions from cache (for unchanged elements)
- Skip LLM call if description already exists and MD5 matches
- Still update line numbers if they changed

**Key changes**:
1. Add parameter to `analyze_tree()` to accept cached descriptions
2. Check if element already has description from cache before calling LLM
3. Only call `analyze_element()` for elements that need analysis

### 4. Indexer Modifications

**Current flow** (`Indexer.index_file()`):
```
1. Check file mtime → skip if unchanged
2. Parse file (Tree.parse())
3. Analyze all elements (Analysis.analyze_tree())
4. Analyze file summary (Analysis.analyze_file())
5. Process file (VectorBuilder.process_file()) → deletes all, re-vectorizes all
```

**New flow needed**:
```
1. Check file mtime → skip if unchanged
2. Load existing metadata for file (cache)
3. Parse file (Tree.parse())
4. Calculate MD5 for each element
5. Match elements with cached metadata (by ast_path)
6. Mark elements as: unchanged (MD5 match), changed (MD5 differs), new (no cache)
7. Analyze only changed/new elements (Analysis.analyze_tree() with cache)
8. Analyze file summary (Analysis.analyze_file())
9. Process file incrementally (VectorBuilder.process_file() with cache)
   - Update unchanged elements (line numbers only)
   - Re-vectorize changed/new elements
   - Remove deleted elements
```

### 5. MD5 Calculation

**Where to calculate MD5**:
- In `Tree` layer: After parsing, calculate MD5 for each element's code content
- Or in `VectorBuilder`: Before checking cache, calculate MD5
- Use `GLib.Checksum.compute_for_string()` with `ChecksumType.MD5`

**What to hash**:
- Element's code content: `tree.lines_to_string(element.start_line, element.end_line)`
- Include signature if available (for methods/functions)
- This ensures we detect any code changes, not just line number shifts

### 6. Element Matching Strategy

**How to match parsed elements with cached metadata**:

1. **Primary key: `ast_path`** (most reliable)
   - Format: `namespace-class-method` or `namespace-function`
   - Unique identifier for element location in code
   - Works even if line numbers change

2. **Fallback: `element_type:element_name`** (if ast_path not available)
   - Less reliable (duplicate names possible)
   - Use only if ast_path is empty

3. **Handle edge cases**:
   - Elements that moved (same ast_path, different line numbers)
   - Elements that were renamed (different name, same location)
   - Elements that were deleted (in cache but not in parsed tree)

### 7. FAISS Vector Updates

**Current behavior**:
- Always creates new vectors for all elements
- Old vectors remain in FAISS (orphaned, but not harmful)

**New behavior needed**:
- For unchanged elements: Keep existing vector_id, don't create new embedding
- For changed elements: 
  - Option A: Update existing vector in-place (if FAISS supports it)
  - Option B: Remove old vector, create new one (simpler)
- For deleted elements: Remove vector from FAISS (if possible)

**Note**: FAISS may not support in-place updates. May need to:
- Remove old vector_id from FAISS
- Create new vector
- Update metadata with new vector_id

### Phase 1 Deliverables

- [ ] Add `md5_hash` column to `vector_metadata` table
- [ ] Update `VectorMetadata` class with `md5_hash` property
- [ ] Implement MD5 calculation for element code content
- [ ] Modify `VectorBuilder` to load and use cached metadata
- [ ] Modify `Analysis` layer to accept cached descriptions
- [ ] Modify `Indexer` to implement incremental update flow
- [ ] Implement element matching by `ast_path`
- [ ] Handle FAISS vector updates for changed elements
- [ ] Test incremental updates with file modifications

---

## Phase 2: Documentation File Verification

**Status**: ⏳ PENDING

**Goal**: Ensure all documentation files are properly indexed and searchable.

### Problem Statement

Documentation files should be vectorized like code files, but we need to verify they're being indexed correctly and are searchable.

## Documentation File Vectorization

### Scanning Order

The indexing process follows a three-phase approach:

1. **Phase 1: File Scanning** (existing)
   - Scan all files in the project
   - Extract code elements, generate descriptions, create vector embeddings
   - Store file-level metadata in `vector_metadata` table

2. **Phase 2: Folder Scanning** (new)
   - After all files are scanned
   - Iterate through each folder in the project
   - For each folder:
     - Query existing file metadata for files in that folder
     - Collect file descriptions from `vector_metadata` table
     - Send to Analysis layer with folder context
     - Generate one-line folder description
     - Store folder metadata in `vector_metadata` table (NOT vectorized)

3. **Phase 3: Project Analysis** (new)
   - After all folders are scanned
   - Collect all folder descriptions and project structure
   - Send to Analysis layer with project context
   - Generate detailed project description
   - Store project metadata in `vector_metadata` table (NOT vectorized)

### Folder Metadata

**Storage**:
- Stored in `vector_metadata` table
- `element_type = 'folder'`
- `file_id` references the folder's `FileBase.id` (folders are also in filebase table)
- `start_line = 0` and `end_line = 0` (no line numbers for folders)
- `vector_id = 0` (NOT stored in FAISS, not vectorized)
- `description` contains one-line folder description
- `ast_path = ''` (folders don't have AST paths)

**Analysis Process**:
1. Get child folders directly from folder structure:
   - Iterate through `folder.children.items`
   - Filter for `Folder` objects (subfolders)
   - For each child folder, query its description from `vector_metadata`:
     ```sql
     SELECT description FROM vector_metadata 
     WHERE file_id = ? AND element_type = 'folder'
     ```
2. Query `vector_metadata` for all files in the folder:
   ```sql
   SELECT element_name, description FROM vector_metadata 
   WHERE file_id IN (SELECT id FROM filebase WHERE parent_id = ? AND base_type = 'f')
   AND element_type = 'file'
   ```
3. Build context: folder path + list of files with their descriptions + list of child folders with their descriptions
4. Call Analysis layer with folder analysis prompt template (includes child folder summaries)
5. Generate one-line description (similar to element descriptions, but can reference child folders)
6. Store in `vector_metadata` with `element_type='folder'`

**Folder Processing Order**:
- **Bottom-up traversal**: Process child folders before parent folders
- Recursively collect all folders in the project
- Sort folders by depth (deepest folders first)
- Process folders in sorted order (deepest → shallowest)
- This ensures child folder descriptions are available when analyzing parent folders
- Parent folder analysis can include summaries of what child folders contain

**Prompt Template**:
- New resource file: `resources/ocvector/folder-prompt.txt`
- Similar structure to element analysis prompt
- Input includes:
  - Folder path
  - List of files with their descriptions
  - List of child folders with their descriptions (if any)
- Output: one-line description of folder contents (can reference child folders)

### Project Metadata

**Storage**:
- Stored in `vector_metadata` table
- `element_type = 'project'`
- `file_id` references the project root folder's `FileBase.id`
- `start_line = 0` and `end_line = 0` (no line numbers)
- `vector_id = 0` (NOT stored in FAISS, not vectorized)
- `description` contains detailed project description (multi-line, not one-liner)
- `ast_path = ''` (projects don't have AST paths)

**Analysis Process**:
1. Query all folder descriptions from `vector_metadata`:
   ```sql
   SELECT element_name, description FROM vector_metadata 
   WHERE element_type = 'folder' AND file_id IN (SELECT id FROM filebase WHERE project_id = ?)
   ```
2. Collect project structure information:
   - Languages used (from file extensions or file metadata)
   - Build system (meson.build, CMakeLists.txt, package.json, etc.)
   - Documentation locations (docs/, README.md, etc.)
   - Plans location (if applicable)
   - Coding standards/rules locations (`.cursor/rules/`, `docs/standards/`, `CONTRIBUTING.md`, etc.)
   - Folder structure with descriptions
3. Call Analysis layer with project analysis prompt template
4. Generate detailed project description
5. Store in `vector_metadata` with `element_type='project'`

**Prompt Template**:
- New resource file: `resources/ocvector/project-prompt.txt`
- More detailed than folder prompt
- Input includes:
  - Project path
  - Languages detected
  - Build system detected
  - Documentation locations
  - Coding standards/rules locations (if found)
  - List of folders with descriptions
  - Overview of file types
- Output: detailed project description including:
  - What the project is about
  - Languages used
  - Build system
  - Where documentation is located
  - Where plans are located (if applicable)
  - Coding standards/rules that should be applied and where they are located (if found)
  - Overview of folder structure and what each does

### Implementation Details

**New Analysis Methods**:

1. `Analysis.analyze_folder()`:
   - Takes folder `Folder` object
   - Gets child folders directly from `folder.children.items` (filter for Folder objects)
   - For each child folder, queries its description from `vector_metadata` (if already analyzed)
   - Queries file descriptions for files in folder from `vector_metadata`
   - Builds prompt context (includes files and child folders)
   - Calls LLM with folder prompt template
   - Returns folder description
   - Stores in `vector_metadata`

2. `Analysis.analyze_project()`:
   - Takes project root `FileBase` object
   - Queries all folder descriptions
   - Detects languages, build system, documentation locations
   - Detects coding standards/rules files (see detection logic below)
   - Builds comprehensive prompt context
   - Calls LLM with project prompt template
   - Returns detailed project description
   - Stores in `vector_metadata`

**Indexer Modifications**:

Add new methods to `Indexer`:

1. `index_folders()`:
   - Called after `index_folder()` completes file scanning
   - Recursively collects all folders in the project
   - Sorts folders by depth (deepest folders first) for bottom-up processing
   - Iterates through folders in sorted order (deepest → shallowest)
   - For each folder, calls `Analysis.analyze_folder()`
   - Stores folder metadata
   - Ensures child folders are analyzed before parent folders

2. `index_project()`:
   - Called after `index_folders()` completes
   - Gets project root folder
   - Calls `Analysis.analyze_project()`
   - Stores project metadata

**Updated Indexing Flow**:

```
index_filebase(folder, recurse, force):
  1. index_folder() → scans all files
  2. index_folders() → analyzes all folders (NEW)
  3. index_project() → analyzes project (NEW)
```

**Coding Standards/Rules Detection**:

The project analysis should detect common locations for coding standards and rules:

1. **Common locations to check**:
   - `.cursor/rules/` directory (Cursor IDE rules)
   - `docs/standards/` or `docs/coding-standards/` directory
   - `CONTRIBUTING.md` file (often contains coding guidelines)
   - `.github/CONTRIBUTING.md` (GitHub contributing guidelines)
   - `docs/CONTRIBUTING.md`
   - `STYLE.md` or `STYLE_GUIDE.md`
   - `docs/STYLE.md` or `docs/STYLE_GUIDE.md`
   - Project-specific rules files (e.g., `docs/rules.md`, `RULES.md`)

2. **Detection logic**:
   - Scan project root and common documentation directories
   - Look for files matching patterns: `*STANDARD*`, `*RULES*`, `*STYLE*`, `CONTRIBUTING*`
   - Check for `.cursor/rules/` directory and list files within
   - Collect file paths where coding standards/rules are located

3. **Include in project analysis**:
   - List of files/directories containing coding standards
   - Brief description of what standards exist (if file names indicate)
   - Location paths for easy reference

**Resource Files**:

1. `resources/ocvector/folder-prompt.txt`:
   - System message: Instructions for generating folder descriptions
   - User template: Placeholders for {folder_path}, {files_list}
   - Output: One-line description

2. `resources/ocvector/project-prompt.txt`:
   - System message: Instructions for generating project descriptions
   - User template: Placeholders for:
     - {project_path}
     - {languages}
     - {build_system}
     - {documentation_locations}
     - {plans_location}
     - {coding_standards_locations} (NEW)
     - {folders_list}
   - Output: Detailed multi-line project description including coding standards/rules information

### Phase 5 Deliverables

- [ ] Create `resources/ocvector/project-prompt.txt` prompt template
- [ ] Implement project structure detection (languages, build system, documentation locations)
- [ ] Implement coding standards/rules detection logic
- [ ] Implement `Analysis.analyze_project()` method
- [ ] Add `Indexer.index_project()` method
- [ ] Integrate project analysis into indexing flow (after folder scanning)
- [ ] Store project metadata with `element_type='project'`
- [ ] Test project description generation
- [ ] Verify project metadata includes all required information
- [ ] Test coding standards detection

---

## Phase 6: Integration & Testing

**Status**: ⏳ PENDING

**Goal**: End-to-end testing, optimization, and integration of all phases.

### Testing Requirements

1. **MD5 Incremental Analysis**:
   - Test file modification scenarios (single element changed, multiple elements changed)
   - Verify unchanged elements reuse cached descriptions
   - Verify changed elements get re-analyzed
   - Test line number updates for moved code
   - Verify FAISS vector updates work correctly

2. **Documentation Files**:
   - Test various documentation file types (README.md, CONTRIBUTING.md, docs/*.md)
   - Verify documentation appears in search results
   - Test file-level summaries for documentation

3. **Image Analysis**:
   - Test various image formats (PNG, JPEG, GIF, WebP, SVG)
   - Verify image descriptions are generated correctly
   - Test image searchability via descriptions
   - Test vision model integration

4. **Folder Summaries**:
   - Test folder description generation for various folder structures
   - Verify folder metadata is stored correctly
   - Test querying folder descriptions

5. **Project Summaries**:
   - Test project description generation
   - Verify all project information is included (languages, build system, etc.)
   - Test coding standards detection
   - Verify project metadata is stored correctly

6. **End-to-End**:
   - Test complete indexing flow: files → folders → project
   - Test incremental updates with MD5 caching
   - Test search results include all metadata types
   - Performance testing (indexing speed, search speed)

### Phase 6 Deliverables

- [ ] Comprehensive test suite for all phases
- [ ] Performance benchmarks
- [ ] Documentation updates
- [ ] Integration testing
- [ ] Bug fixes and optimizations
- [ ] User documentation

## Implementation Notes

When searching or providing context:
- Query `vector_metadata` WHERE `element_type = 'folder'` to get folder descriptions
- Query `vector_metadata` WHERE `element_type = 'project'` to get project description
- These are NOT in FAISS, so they're retrieved directly from SQL database
- Can be included in agent context or codebase search results

### Phase 4 Deliverables

- [ ] Create `resources/ocvector/folder-prompt.txt` prompt template (includes child folder summaries)
- [ ] Implement `Analysis.analyze_folder()` method:
  - Gets child folders from `folder.children.items` (filter for Folder objects)
  - Queries child folder descriptions from `vector_metadata` (if already analyzed)
  - Queries file descriptions for files in folder
- [ ] Add `Indexer.index_folders()` method with bottom-up traversal
- [ ] Implement folder depth calculation and sorting (deepest first)
- [ ] Integrate folder analysis into indexing flow (after file scanning)
- [ ] Store folder metadata with `element_type='folder'`
- [ ] Test folder description generation (verify child folders included in parent analysis)
- [ ] Verify folder metadata is queryable
- [ ] Test bottom-up processing order (child folders before parents)

---

## Phase 5: Project-Level Summaries

**Status**: ⏳ PENDING

**Goal**: Generate detailed project descriptions including languages, build system, documentation locations, coding standards, and folder structure.

### Problem Statement

There's no high-level overview of what the project is about, what technologies it uses, or where important files are located.

### Project Metadata

- This would enhance the existing codebase search/scanning infrastructure
- Vector metadata could store:
  - Project summary (what the project is about)
  - Language breakdown
  - Directory structure summaries
  - Key architectural patterns
- MD5 hashing ensures we can detect when summaries need regeneration
- Element-level caching dramatically reduces LLM API calls for unchanged code
- Line number updates handle code that moved but didn't change (e.g., added comments above)
- Folder and project metadata are NOT vectorized (stored only in SQL metadata table)
- Folder and project metadata provide context without requiring vector search
- Can be queried directly for agent context or project overview features

## Documentation File Vectorization

### Current Status

Documentation files (markdown, text files) are already vectorized if they pass the `is_text` check in `Indexer.index_file()`. However, we should ensure:

1. **All documentation is indexed**:
   - README.md, CONTRIBUTING.md, LICENSE, etc. in project root
   - All files in `docs/` directory
   - Documentation in subdirectories
   - Any markdown or text documentation files

2. **Documentation is searchable**:
   - Documentation content should appear in codebase search results
   - File-level summaries should describe documentation files
   - Documentation should be treated like code files for vectorization

3. **No special handling needed**:
   - Documentation files are text files, so they go through normal indexing pipeline
   - Tree parsing may not extract "elements" from markdown (depends on tree-sitter support)
   - If no elements extracted, file-level summary is still created
   - File content can still be vectorized as a single document

### Implementation Notes

- Documentation files should already work with current system
- May need to verify markdown files are being indexed
- If tree-sitter doesn't parse markdown, files will have `tree.elements.size == 0`
- File-level analysis still creates a summary via `Analysis.analyze_file()`
- This summary gets vectorized, making documentation searchable

### Phase 2 Deliverables

- [ ] Verify markdown files are being indexed (check `is_text` detection)
- [ ] Test documentation file indexing (README.md, CONTRIBUTING.md, docs/*.md)
- [ ] Verify documentation appears in codebase search results
- [ ] Ensure file-level summaries are created for documentation files
- [ ] Document any issues found and fix if needed

---

## Phase 3: Binary File Handling - Image Analysis

**Status**: ⏳ PENDING

**Goal**: Enable searching for images by their content through text descriptions generated by vision models.

### Problem Statement

Binary files (images, PDFs, etc.) are currently skipped entirely. There's no way to search for images or understand their content.

## Binary File Handling - Image Analysis

### Current Status

Binary files are currently skipped:
- `Indexer.index_file()` checks `if (!file.is_text)` and skips binary files
- Images, PDFs, and other binary files are not indexed
- No way to search for images or understand their content

### Proposed Image Analysis

**Goal**: Enable searching for images by their content through text descriptions.

**Approach**:
1. Detect image files (common extensions: `.png`, `.jpg`, `.jpeg`, `.gif`, `.webp`, `.svg`, etc.)
2. Use vision model to analyze image and generate text description
3. Vectorize the text description (not the image itself)
4. Store image metadata with description in `vector_metadata`
5. Enable codebase search to find images by their content descriptions

### Implementation Details

**Image Detection**:
- Check file extension against known image formats
- Could use `file.is_text == false` and check MIME type or extension
- Common formats: PNG, JPEG, GIF, WebP, SVG, BMP, TIFF

**Image Analysis Process**:
1. Load image file (read binary data)
2. Send image to vision model (e.g., via Ollama vision API or similar)
3. Generate text description of image content
4. Store description in `vector_metadata.description`
5. Vectorize description text (create embedding)
6. Store in FAISS with `vector_id` and metadata

**Storage**:
- `element_type = 'image'` (or `'binary'` for other binary files)
- `file_id` references the image file
- `start_line = 0, end_line = 0` (no line numbers for images)
- `description` contains text description of image content
- `vector_id` points to vectorized description embedding
- `ast_path = ''` (images don't have AST paths)

**Vision Model Integration**:
- Use Ollama vision models (e.g., `llava`, `bakllava`) or similar
- Send image as base64 or file path to vision API
- Prompt: "Describe this image in detail. Include any text, objects, diagrams, screenshots, or visual elements."
- Get text response describing the image
- This text description is what gets vectorized

**Vectorization**:
- The text description is treated like any other document
- Create embedding from description text
- Store in FAISS index
- Searchable via codebase search tool

**Example Flow**:
```
1. Detect: image.png (binary file, extension .png)
2. Load image data
3. Call vision model: "Describe this image"
4. Get response: "Screenshot of a login form with username and password fields, submit button, and company logo in header"
5. Create VectorMetadata:
   - element_type = 'image'
   - element_name = 'image.png'
   - description = "Screenshot of a login form..."
6. Vectorize description text
7. Store in FAISS and metadata table
```

**Search Results**:
- When searching for "login form", image description matches
- Search result includes image file path
- User can see what images contain without opening them

### Other Binary Files

**PDFs**:
- Could use PDF text extraction
- Extract text content and vectorize
- Similar to image analysis but extract text instead of describing

**Other Formats**:
- Consider on case-by-case basis
- Priority: Images (most common, most useful)
- Future: PDFs, Office documents, etc.

### Implementation Requirements

**New Components**:
1. `ImageAnalyzer` class:
   - Detects image files
   - Loads image data
   - Calls vision model API
   - Returns text description

2. `Indexer.index_binary_file()` method:
   - Handles binary files (images, PDFs, etc.)
   - Routes to appropriate analyzer (image, PDF, etc.)
   - Creates VectorMetadata entries
   - Vectorizes descriptions

3. Vision model integration:
   - Add vision API support to `OLLMchat.Client`
   - Or use existing image analysis tools if available
   - Configure vision model in tool config

**Configuration**:
- Add `vision_model` to `CodebaseSearchToolConfig`
- Specify which vision model to use for image analysis
- Enable/disable binary file indexing

**Modified Flow**:
```
Indexer.index_file():
  if (file.is_text):
    → Normal text file indexing (existing)
  else if (is_image_file(file)):
    → Image analysis and description vectorization (NEW)
  else:
    → Skip or handle other binary types (future)
```

### Benefits

- Images become searchable by content
- Screenshots, diagrams, UI mockups can be found via description
- Documentation images (screenshots, diagrams) are discoverable
- Better project understanding through visual content

### Phase 3 Deliverables

- [ ] Create `ImageAnalyzer` class for vision model integration
- [ ] Add vision API support to `OLLMchat.Client` (or use existing tools)
- [ ] Implement `Indexer.index_binary_file()` method
- [ ] Add image file detection (extension/MIME type checking)
- [ ] Add `vision_model` configuration to `CodebaseSearchToolConfig`
- [ ] Implement image analysis flow (load → analyze → describe → vectorize)
- [ ] Store image metadata with `element_type='image'`
- [ ] Test image indexing and searchability
- [ ] Document supported image formats

---

## Phase 4: Folder-Level Summaries

**Status**: ⏳ PENDING

**Goal**: Generate one-line descriptions for each directory/folder to help LLM understand code organization.

### Problem Statement

Folders provide important organizational context, but currently there's no high-level description of what each folder contains.

## Folder and Project Metadata Workflow

### Scanning Order

The indexing process follows a three-phase approach:

1. **Phase 1: File Scanning** (existing)
   - Scan all files in the project
   - Extract code elements, generate descriptions, create vector embeddings
   - Store file-level metadata in `vector_metadata` table

2. **Phase 2: Folder Scanning** (new)
   - After all files are scanned
   - Process folders in **bottom-up order** (child folders first, then parent folders)
   - Recursively traverse folder tree to identify all folders
   - Sort folders by depth (deepest first)
   - For each folder (in depth order):
     - Query existing file metadata for files in that folder
     - Query child folder descriptions (if any) from `vector_metadata` table
     - Collect file descriptions and child folder summaries
     - Send to Analysis layer with folder context (includes child folder summaries)
     - Generate one-line folder description
     - Store folder metadata in `vector_metadata` table (NOT vectorized)

3. **Phase 3: Project Analysis** (new)
   - After all folders are scanned
   - Collect all folder descriptions and project structure
   - Send to Analysis layer with project context
   - Generate detailed project description
   - Store project metadata in `vector_metadata` table (NOT vectorized)

### Folder Metadata

## Related Plans

- 2.10-codebase-search-tool.md - Existing codebase search infrastructure
- 2.18-DONE-agent-tool.md - Agent tools that could benefit from project context
- 4.7-rules-system.md - Rules system for coding standards (may integrate with project analysis)
