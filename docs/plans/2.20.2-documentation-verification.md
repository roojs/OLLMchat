# 2.20.2. Documentation File Verification

## Status

⏳ **PENDING**

## Coding Standards

This plan follows the coding standards defined in `.cursor/rules/CODING_STANDARDS.md`. All code examples in this document should:
- Use `this.` prefix for instance members
- Use `GLib.` prefix for GLib namespace (no `using` statements)
- Use early returns for negative tests
- Avoid nullable types where possible
- Use `string[]` arrays instead of `Gee.ArrayList<string>` when building arrays just to join
- Use `string.joinv()` or string concatenation instead of `GLib.StringBuilder` (except for hundreds of iterations)
- Use `.get()` and `.set()` for Gee collections
- Avoid unnecessary null checks
- Use brace placement: line breaks for classes/methods, inline for control structures

## Related Changes

### Default Model Updates

As part of this implementation, update default models in `CodebaseSearchToolConfig.setup_defaults()`:
- **Embed model** (`embed`): Keep as `"bge-m3:latest"` (unified for both code and documentation vectorization)
- **Analysis model** (`analysis`): Change from `"qwen3-coder:30b"` to `"qwen3:1.7b"` (smaller, faster default)
  - **Note**: For better analysis quality, users can change this to `"qwen3-coder:30b"` in settings (larger model with better code understanding)

**Configuration Structure:**
- Use unified `embed` model for both code and documentation files
- Both code and documentation use `analysis` model for LLM analysis

### Model Bootstrapping Configuration

Add interface-based model bootstrapping system:
- **Create `RequiresModels` interface**: Tool configs implement this interface to declare required models via `required_models()` method
- **Decouple main code from specific models**: Main startup code uses `RequiresModels` interface, not hardcoded model names or introspection
- **Implement interface in CodebaseSearchToolConfig**: Return both embed and analysis models from `required_models()` method (both required for app startup)
- **Generic bootstrapping logic**: Startup code iterates through all tool configs implementing `RequiresModels`, calls `required_models()`, and waits for those models

Implementation approach:
- **Block app startup**: Prevent the application from fully initializing until all required models are available
- **Keep settings dialog open**: Similar to how `Window.initialize_unverified_client()` shows settings dialog until default model is available, keep settings dialog open until required models are downloaded
- **Auto-pull if missing**: If required models are not available, automatically pull them (using `PullManager`)
- **Show progress**: Display pull progress in UI (similar to existing pull banner in settings dialog)
- **Loop until available**: Similar to connection/model checking loop, keep checking and pulling until all required models are available
- **Both models required**: Both embed and analysis models are included in `required_models()` and must be available at startup

## Purpose

Implement specialized indexing for documentation files (plain text and markdown) with hierarchical section breakdown, categorization, and context-aware vectorization. Unlike code files which are parsed bottom-up (node → parent → document), documentation files are parsed top-down (document → section → subsection).

## Problem Statement

Documentation files need specialized handling:
- They should be broken down by sections (especially markdown)
- They need categorization (plans, documentation, rules, configuration, data files, etc.)
- They require hierarchical context when vectorizing subsections
- They need a different parsing approach than code (top-down vs bottom-up)
- Only plain text files should be processed (not HTML, XML, JSON)

## File Type Restrictions

### Included
- Plain text files (`.txt`, `.md`, `.markdown`)
- Files that pass `is_text` check but are NOT:
  - HTML files (`.html`, `.htm`)
  - XML files (`.xml`, `.xsd`, `.xsl`, etc.)
  - JSON files (`.json`, `.jsonc`)
  - Other structured formats

### Handling
- Plain text files: Generate one-line description via LLM
- Markdown files: Break down by sections (unless trivial)
- Small sections (< 10 words): Ignore and skip

## Document Categorization

Documents are categorized using LLM analysis during Level A processing:
- **LLM-based categorization**: Send document path, filename, and content to LLM for categorization
- **No hardcoded rules**: LLM analyzes the document and returns category + description
- **Format**: LLM response format is "CATEGORY: description" (e.g., "plan: This document outlines...")
- **Fallback**: If LLM doesn't provide category prefix, default to "other"

Categories (returned by LLM):
- `plan` - Planning documents
- `documentation` - General documentation
- `rule` - Rules and standards
- `configuration` - Configuration files
- `data` - Data files
- `license` - License files
- `changelog` - Changelog files
- `other` - Uncategorized text files

## Processing Approach

### Three-Level Processing

Unlike code files which process bottom-up, documentation files use **reverse parsing** (top-down):

1. **Level A: Document Summary** (whole document)
   - Generate general summary about entire document via LLM
   - One-line description of document purpose and content
   - Store as document-level metadata

2. **Level B: Section Chunks** (with child data)
   - Break document into sections (for markdown, use heading hierarchy)
   - For each section, send to LLM with:
     - Section content
     - Information about child sections (if any)
   - Generate section-level summaries

3. **Level C: Final Smallest Chunks** (leaf sections)
   - Process the smallest subsections
   - Add context headers at the top:
     ```
     DOCUMENT: filename
     DOCUMENT SUMMARY: {one line doc summary from Level A}
     SECTION CONTEXT: {description of parent section context}
     ```
   - **Exception**: If only 1 nesting level exists, skip SECTION CONTEXT (document summary is sufficient)

### Parsing Order

**Code files** (current): `node → parent → document` (bottom-up)
- Process leaf nodes first, then parents, then document

**Documentation files** (new): `document → section → subsection` (top-down)
- Process document first, then sections, then subsections
- Build context as we go deeper

### Section Filtering

- Ignore sections with < 10 words
- Trivial markdown files (single section, no subsections) can be treated as simple text files

## Implementation Details

### Code Reuse and Extension Strategy

#### DocumentationTree extends TreeBase

**Can reuse from TreeBase:**
- `file` property - File reference
- `lines` property - File content split into lines
- `load_file_content()` - Async file loading and line splitting
- `lines_to_string(start_line, end_line)` - Extract content by line numbers (perfect for section content)
- File handling infrastructure

**Cannot reuse (code-specific):**
- Tree-sitter parser initialization (documentation doesn't use tree-sitter)
- `ast_path()` - Code AST traversal (documentation uses markdown heading hierarchy)
- `element_name()` - Code element extraction (documentation extracts markdown headings)
- `is_unsupported_language()` - Need different logic (check for HTML/XML/JSON instead)

**New implementation needed:**
- Markdown heading parser (regex or markdown library)
- Section hierarchy builder (top-down, not AST-based)
- Section content extractor (text between headings)

#### DocumentationVectorBuilder extends VectorBuilder

**Can reuse from VectorBuilder:**
- `config`, `database`, `sql_db` properties
- `embed_to_floats()` - Embedding conversion
- FAISS vector storage (`database.add_vectors_batch()`)
- SQL metadata storage (`element.saveToDB()`)
- Incremental processing logic (unchanged/changed/new detection)
- MD5 hash matching for unchanged elements
- Vector ID management

**Cannot reuse (code-specific):**
- `format_element_document()` - Code formatting (documentation needs context headers)
- `get_truncated_code_snippet()` - Code snippet extraction (documentation needs section content with context)
- Element document structure (type/name/file/lines/signature/code vs DOCUMENT/SECTION CONTEXT/content)

**New implementation needed:**
- `format_documentation_chunk()` - Format with context headers
- `get_section_content()` - Extract section content with context
- Chunking logic for leaf sections (different from code element chunking)
- Context header building (traverse parent chain)
- Use unified `tool_config.embed` model (same as code files)

#### Summary

- **DocumentationTree**: Extend `TreeBase` for file handling, implement markdown parsing separately
- **DocumentationVectorBuilder**: Extend `VectorBuilder` for vector storage/incremental logic, override formatting methods
- **DocumentationAnalysis**: New class (similar structure to `Analysis` but different processing flow)

### File Detection

Add `File.is_documentation()` in libocfiles. It uses `file.is_text` and `file.language` (from `BufferProvider.detect_language()`): returns true for markdown/plain text/unknown text, false for code and structured formats (HTML, XML, JSON, etc.). In `Indexer.index_file()`, route using `file.is_documentation()`.

### Markdown Section Parsing

Create new class `DocumentationTree : TreeBase` (extends TreeBase for file handling):
- Reuse `load_file_content()` and `lines_to_string()` from TreeBase
- Parse markdown headings to build section hierarchy (new implementation)
- Extract section content (text between headings) using `lines_to_string()`
- Build tree structure: Document → Section → Subsection → Subsubsection
- Similar structure to `Tree` but uses markdown parsing instead of tree-sitter AST

### Analysis Pipeline

Create `DocumentationAnalysis` class (similar to `Analysis` but for docs):

1. **analyze_document()** - Level A
   - Send entire document to LLM
   - Prompt: "Generate a one-line summary of this document"
   - Store as `document_summary`

2. **analyze_sections()** - Level B
   - For each section (top-down traversal):
     - Collect child section summaries (if already processed)
     - Send section + child info to LLM
     - Generate section summary
     - Store section metadata

3. **analyze_leaf_sections()** - Level C
   - For each leaf section:
     - Extract section content
     - Create VectorMetadata for the leaf section
     - Store section metadata (will be used during vectorization)

### Vectorization (Level C - Final Chunking)

Create `DocumentationVectorBuilder : VectorBuilder` (extends VectorBuilder for vector storage):
- Reuse FAISS storage, SQL metadata, incremental processing from VectorBuilder
- **Use unified `embed` model** from `CodebaseSearchToolConfig` (same as code files)
- Override `format_element_document()` → `format_documentation_chunk()` for context headers
- Override `get_truncated_code_snippet()` → `get_section_content()` for section content extraction
- Add chunking logic for leaf sections

In `DocumentationVectorBuilder.process_file()`:
- For each leaf section from Level C:
  - **Chunk the section content** into appropriate-sized chunks (similar to code chunking)
  - For **each chunk**:
    - **Build context using methods**:
      - Get document-level summary (from root VectorMetadata.description)
      - If nesting level > 1: call `get_section_context()` method to build SECTION CONTEXT from parent chain
    - **Prepend context headers** to the chunk content:
      ```
      DOCUMENT: filename
      DOCUMENT SUMMARY: {from root VectorMetadata.description}
      SECTION CONTEXT: {from metadata.get_section_context()} (only if > 1 nesting level)
      ```
    - Vectorize the chunk with prepended context
    - Create vector embedding for the chunk
  - Store vectors with metadata linking back to document and section

### VectorMetadata Extensions for Documentation

Extend existing `VectorMetadata` class (no separate class needed):
- **New fields to add**:
  - `parent` - Reference to parent VectorMetadata (for section hierarchy traversal)
  - `category` - Document category (plan, documentation, rule, configuration, data, license, changelog, other)
  - `children` - ArrayList<VectorMetadata> of child sections (for top-down traversal)
  
- **Reuse existing fields**:
  - `description` - Reuse for document/section summary (from Level A and Level B analysis)
  - `ast_path` - Reuse for section path through hierarchy (e.g., "Introduction-Overview")
  
- **Context building methods**:
  - `get_section_context()` - Method that traverses up `parent` chain to collect and return parent descriptions

### Parent-Child Relationships

- Each section VectorMetadata has a `parent` reference (null for document root)
- Each section VectorMetadata has a `children` ArrayList
- Traverse up via `parent` to build context chain
- Traverse down via `children` for top-down processing

### Integration with Existing Pipeline

- In `Indexer.index_file()`:
  - Call `file.is_documentation()` to detect documentation (plain text / markdown, not code)
  - Route to `DocumentationTree` + `DocumentationAnalysis` instead of `Tree` + `Analysis`
  - Use `DocumentationVectorBuilder` (extends VectorBuilder):
    - Reuses FAISS storage and incremental processing from VectorBuilder
    - Handles chunking of leaf sections with context headers
    - Stores vectors with proper metadata

## Deliverables

### Phase 1: PromptTemplate Refactoring
- [x] Create `PromptTemplate` class in `libocvector/Indexing/PromptTemplate.vala`
- [x] Refactor `Analysis.vala` to use PromptTemplate class:
  - [x] Remove `PromptTemplate` struct
  - [x] Replace cached template fields with `static PromptTemplate?` objects
  - [x] Add `static construct` block to load templates at class initialization
  - [x] Remove `load_prompt_template()` and `load_file_prompt_template()` methods
  - [x] Remove `get_prompt_template()` and `get_file_prompt_template()` methods (access static fields directly)
  - [x] Replace `.replace()` chains with `fill()` method calls
- [x] Update `libocvector/meson.build` to include `PromptTemplate.vala`
- [x] Update `docs/meson.build` to include `PromptTemplate.vala` (must come before Analysis.vala)

### Phase 2: Model Bootstrapping
- [ ] Create `RequiresModels` interface in `libollmchat/Settings/RequiresModels.vala`:
  - [ ] Interface with `required_models()` method returning `Gee.ArrayList<ModelUsage>`
  - [ ] Tool configs implement this interface to declare required models
- [ ] Update `CodebaseSearchToolConfig`:
  - [ ] Implement `RequiresModels` interface
  - [ ] Implement `required_models()` method returning both embed and analysis models (both required for app startup)
  - [ ] Update `setup_defaults()` to change analysis model from `"qwen3-coder:30b"` to `"qwen3:1.7b"` (smaller, faster default)
- [ ] Implement generic model bootstrapping in app startup (in `Window.initialize_unverified_client()` or similar):
  - [ ] Add `wait_for_pull()` helper method
  - [ ] Add `ensure_required_models()` method
  - [ ] Iterate through all tool configs that implement `RequiresModels` interface
  - [ ] Call `required_models()` on each to get list of required ModelUsage objects
  - [ ] For each required model:
    - [ ] Check if model is available (using `verify_model()`)
    - [ ] If not available, auto-pull using `PullManager`
    - [ ] Show pull progress in settings dialog (using existing pull banner)
    - [ ] Loop until model is available (similar to connection checking loop)
- [ ] Block app startup until all required models are available:
  - [ ] Keep settings dialog open (similar to connection/model checking loop)
  - [ ] Prevent app from fully initializing until all required models are available
  - [ ] Generic approach: works for any tool config implementing `RequiresModels`, not hardcoded to specific models

### Phase 3: Documentation Indexing Classes
- [ ] Extend VectorMetadata for documentation:
  - [ ] Add `parent` field (reference to parent VectorMetadata)
  - [ ] Add `category` field (document category)
  - [ ] Add `children` ArrayList<VectorMetadata> field
  - [ ] Reuse `description` for document/section summaries
  - [ ] Reuse `ast_path` for section hierarchy paths
  - [ ] Implement `get_section_context()` method (traverses up parent chain)
- [ ] Create prompt template files for documentation analysis:
  - [ ] Create `resources/ocvector/analysis-documentation-prompt.txt` (Level B - section analysis)
  - [ ] Create `resources/ocvector/analysis-documentation-document-prompt.txt` (Level A - document categorization and summary)
  - [ ] Add both files to `resources/gresources.xml` under `/ocvector` prefix
- [ ] Create `DocumentationTree : TreeBase` class:
  - [ ] Extend TreeBase for file handling (`load_file_content()`, `lines_to_string()`)
  - [ ] Implement markdown heading parser
  - [ ] Implement section hierarchy building (document → section → subsection)
- [ ] Create `DocumentationAnalysis` class with three-level processing:
  - [ ] Level A: Document summary generation
  - [ ] Level B: Section chunk analysis with child data
  - [ ] Level C: Leaf section extraction and metadata creation
  - [ ] Implement section filtering (< 10 words)
  - [ ] Implement document categorization system
- [ ] Create `DocumentationVectorBuilder : VectorBuilder` class:
  - [ ] Extend VectorBuilder for FAISS storage, SQL metadata, incremental processing
  - [ ] Use unified `tool_config.embed` model (same as code files)
  - [ ] Override `format_element_document()` → `format_documentation_chunk()` with context headers
  - [ ] Override `get_truncated_code_snippet()` → `get_section_content()` for section extraction
  - [ ] Implement chunking logic for leaf sections
  - [ ] Prepend DOCUMENT and SECTION context headers to each chunk
  - [ ] Add context header logic (skip if only 1 nesting level)
  - [ ] Vectorize chunks with context headers included using unified `embed` model

### Phase 4: Documentation Indexing Integration
- [ ] Add `File.is_documentation()` in libocfiles (uses `is_text` and `language`)
- [ ] Update `Indexer.index_file()` routing:
  - [ ] Use `file.is_documentation()` to detect documentation files
  - [ ] Route to `index_documentation_file()` for documentation files
  - [ ] Route to `index_code_file()` for code files
- [ ] Implement `index_documentation_file()` method in Indexer:
  - [ ] Create DocumentationTree and parse file
  - [ ] Create DocumentationAnalysis and analyze tree
  - [ ] Create DocumentationVectorBuilder and process file
  - [ ] Handle incremental processing (unchanged/changed/new detection)
- [ ] Test with various documentation files:
  - [ ] Simple text files (one-line description)
  - [ ] Trivial markdown (single section)
  - [ ] Multi-section markdown (nested hierarchy)
  - [ ] Plans (`docs/plans/*.md`)
  - [ ] Documentation (`docs/*.md`, `README.md`)
  - [ ] Rules (`.cursor/rules/*.md`)
- [ ] Verify documentation appears in codebase search results
- [ ] Ensure proper context propagation in search results

## Implementation Phases

This plan is divided into four phases:

1. **Phase 1: PromptTemplate Refactoring** - Refactor existing code to use PromptTemplate class
2. **Phase 2: Model Bootstrapping** - Add model initialization at app startup
3. **Phase 3: Documentation Indexing Classes** - Create new classes for documentation processing
4. **Phase 4: Documentation Indexing Integration** - Integrate documentation processing with existing code

## Code Changes

### Phase 1: PromptTemplate Refactoring

This phase refactors existing code to use a reusable PromptTemplate class instead of the current struct-based approach.

#### 1.1. Create PromptTemplate Class

**File**: `libocvector/Indexing/PromptTemplate.vala` (NEW FILE)

Create a reusable PromptTemplate class that can be used by both Analysis and DocumentationAnalysis:

```vala
namespace OLLMvector.Indexing
{
	/**
	 * Prompt template class for loading and filling templates.
	 */
	public class PromptTemplate : Object
	{
		/**
		 * Base path for ocvector resources.
		 */
		private const string RESOURCE_BASE_PREFIX = "/ocvector";
		
		public string system_message = "";
		public string user_template = "";
		
		private string resource_path;
		
		/**
		 * Constructor.
		 * 
		 * @param resource_path Relative path from RESOURCE_BASE_PREFIX (e.g., "analysis-prompt.txt")
		 */
		public PromptTemplate(string resource_path)
		{
			this.resource_path = resource_path;
		}
		
		/**
		 * Loads template from resources.
		 * 
		 * Template should use `---` separator between system and user messages.
		 */
		public void load() throws GLib.Error
		{
			var file = GLib.File.new_for_uri("resource://" + GLib.Path.build_filename(
				RESOURCE_BASE_PREFIX,
				this.resource_path
			));
			
			uint8[] data;
			string etag;
			file.load_contents(null, out data, out etag);
			
			var parts = ((string)data).split("---", 2);
			if (parts.length != 2) {
				throw new GLib.IOError.FAILED("Prompt template must contain '---' separator between system and user messages");
			}
			
			this.system_message = parts[0].strip();
			this.user_template = parts[1].strip();
		}
		
		/**
		 * Fills template placeholders with values.
		 * 
		 * Takes varargs of key-value pairs: fill("key1", value1, "key2", value2, ...)
		 * Replaces {key1} with value1, {key2} with value2, etc.
		 * Vala automatically passes null at the end of varargs to signal termination.
		 * 
		 * @param ... Varargs of string key-value pairs
		 * @return Filled user template
		 */
		public string fill(...)
		{
			var result = this.user_template;
			var args = va_list();
			
			// Process key-value pairs (Vala passes null at end automatically)
			while (true) {
				unowned string? key = args.arg<string?>();
				if (key == null) {
					break;
				}
				unowned string? value = args.arg<string?>();
				if (value == null) {
					break;
				}
				
				// Replace {key} with value
				var placeholder = "{" + key + "}";
				result = result.replace(placeholder, value);
			}
			
			return result;
		}
	}
}
```

#### 1.2. Refactor Analysis.vala to Use PromptTemplate Class

**File**: `libocvector/Indexing/Analysis.vala`

Replace the struct-based PromptTemplate with the new class using static fields loaded in a static constructor:

1. Remove `PromptTemplate` struct definition
2. Replace `cached_template` and `cached_file_template` with `static PromptTemplate?` objects
3. Add `static construct` block to load templates at class initialization
4. Remove `load_prompt_template()` and `load_file_prompt_template()` methods
5. Remove `get_prompt_template()` and `get_file_prompt_template()` methods (access static fields directly)
6. Replace manual `.replace()` calls with `fill()` method

**Changes:**
- Remove `private struct PromptTemplate` (lines 63-67)
- Change `private PromptTemplate? cached_template = null;` to `private static PromptTemplate? cached_template = null;`
- Change `private PromptTemplate? cached_file_template = null;` to `private static PromptTemplate? cached_file_template = null;`
- Add `static construct` block:
  ```vala
  static construct
  {
      try {
          cached_template = new PromptTemplate("analysis-prompt.txt");
          cached_template.load();
          
          cached_file_template = new PromptTemplate("analysis-prompt-file.txt");
          cached_file_template.load();
      } catch (GLib.Error e) {
          GLib.critical("Failed to load prompt templates in static constructor: %s", e.message);
      }
  }
  ```
- Remove `load_prompt_template()` and `load_file_prompt_template()` methods
- Remove `get_prompt_template()` and `get_file_prompt_template()` methods
- Access templates directly as `cached_template` and `cached_file_template` (no null checks needed)
- Replace `.replace()` chains with `fill()` calls:
  ```vala
  // Old:
  var user_message = this.cached_file_template.user_template.replace(
      "{file_basename}", file_basename
  ).replace(
      "{file_path}", file_path
  ).replace(
      "{elements_summary}", elements_summary
  );
  
  // New:
  var user_message = this.cached_file_template.fill(
      "file_basename", file_basename,
      "file_path", file_path,
      "elements_summary", elements_summary
  );
  ```

#### 1.3. Update meson.build

**File**: `libocvector/meson.build`

Add PromptTemplate.vala to the source files list.

### Phase 2: Model Bootstrapping

This phase adds model initialization at app startup to ensure required models are available before the app proceeds.

#### 2.1. RequiresModels Interface

**File**: `libollmchat/Settings/RequiresModels.vala` (NEW FILE)

Create interface for tool configs that require models to be available:

```vala
namespace OLLMchat.Settings
{
	/**
	 * Interface for tool configs that require models to be available at startup.
	 * 
	 * Tool configs that implement this interface must provide a list of
	 * ModelUsage objects that need to be available before the app can proceed.
	 */
	public interface RequiresModels : Object
	{
		/**
		 * Returns a list of ModelUsage objects that must be available at startup.
		 * 
		 * Any ModelUsage objects returned from this method will be checked and
		 * auto-pulled if missing. Return an empty list if no models are required.
		 * 
		 * @return List of required ModelUsage objects
		 */
		public abstract Gee.ArrayList<ModelUsage> required_models();
	}
}
```

#### 2.2. CodebaseSearchToolConfig Implementation

**File**: `libocvector/Tool/CodebaseSearchToolConfig.vala`

Update CodebaseSearchToolConfig to implement RequiresModels:

```vala
public class CodebaseSearchToolConfig : OLLMchat.Settings.BaseToolConfig, OLLMchat.Settings.RequiresModels
{
	// ... existing properties ...
	
	/**
	 * Returns list of required models (embed and analysis models are required for app startup).
	 */
	public Gee.ArrayList<OLLMchat.Settings.ModelUsage> required_models()
	{
		var required = new Gee.ArrayList<OLLMchat.Settings.ModelUsage>();
		
		// Embed model is required for codebase search functionality
		required.add(this.embed);
		
		// Analysis model is required for code analysis during indexing
		required.add(this.analysis);
		
		return required;
	}
	
	/**
	 * Sets up default values for embed and analysis model configurations.
	 */
	public void setup_defaults(string connection_url)
	{
		this.embed = new OLLMchat.Settings.ModelUsage() {
			connection = connection_url,
			model = "bge-m3:latest"
		};
		this.embed.options = new OLLMchat.Call.Options() {
			temperature = 0.0,
			num_ctx = 2048
		};
		
		this.analysis = new OLLMchat.Settings.ModelUsage() {
			connection = connection_url,
			model = "qwen3:1.7b"  // CHANGED: from "qwen3-coder:30b" to smaller default
		};
		this.analysis.options = new OLLMchat.Call.Options() {
			temperature = 0.0
		};
	}
}
```

#### 2.3. Model Bootstrapping in Window.vala

**File**: `ollmapp/Window.vala`

Add helper method to wait for pull completion, then add method to check and pull required models:

```vala
/**
 * Waits for a model pull to complete (success or failure).
 * 
 * Connects to PullManager signals and waits until model_complete or model_failed
 * is emitted for the specified model.
 * 
 * @param pull_manager The PullManager instance
 * @param model_name The model name to wait for
 * @return true if pull succeeded, false if failed
 */
private async bool wait_for_pull(SettingsDialog.PullManager pull_manager, string model_name)
{
	GLib.SourceFunc callback = wait_for_pull.callback;
	bool pull_success = false;
	bool completed = false;
	
	ulong complete_id = pull_manager.model_complete.connect((name) => {
		if (name == model_name && !completed) {
			completed = true;
			pull_success = true;
			pull_manager.disconnect(complete_id);
			pull_manager.disconnect(failed_id);
			callback();
		}
	});
	
	ulong failed_id = pull_manager.model_failed.connect((name) => {
		if (name == model_name && !completed) {
			completed = true;
			pull_success = false;
			pull_manager.disconnect(complete_id);
			pull_manager.disconnect(failed_id);
			callback();
		}
	});
	
	// Wait for signal
	yield;
	
	return pull_success;
}

/**
 * Checks and ensures all required models are available.
 * 
 * Iterates through all tool configs that implement RequiresModels interface,
 * collects required models via required_models() method, verifies they're available,
 * and auto-pulls them if missing. Shows progress in settings dialog.
 * 
 * @param config The Config2 instance
 * @return true if all required models are available, false if user cancelled
 */
private async bool ensure_required_models(OLLMchat.Settings.Config2 config) throws GLib.Error
{
	var required_models = new Gee.ArrayList<OLLMchat.Settings.ModelUsage>();
	
	// Collect required models from all tool configs that implement RequiresModels
	foreach (var tool_config_entry in config.tools.entries) {
		var tool_config = tool_config_entry.value;
		
		if (tool_config is OLLMchat.Settings.RequiresModels) {
			var requires_models = tool_config as OLLMchat.Settings.RequiresModels;
			required_models.add_all(requires_models.required_models());
		}
	}
	
	if (required_models.size == 0) {
		return true;  // No required models
	}
	
	// Check each required model
	foreach (var model_usage in required_models) {
		// Verify model is available
		if (yield model_usage.verify_model(config)) {
			continue;  // Model is available
		}
		
		// Model not available - need to pull it
		// Show settings dialog if not already shown
		if (!this.settings_dialog.visible) {
			this.show_settings_dialog("tools");
		}
		
		// Get connection (early return on failure)
		if (!config.connections.has_key(model_usage.connection)) {
			GLib.warning("Connection not found for model: %s", model_usage.model);
			return false;
		}
		var connection = config.connections.get(model_usage.connection);
		
		// Start background pull operation
		if (!this.settings_dialog.pull_manager.start_pull(model_usage.model, connection)) {
			// Pull already in progress - wait for it to complete
			GLib.debug("Pull already in progress for model: %s", model_usage.model);
		}
		
		// Wait for pull to complete using async wait pattern (early return on failure)
		// Wait for pull to complete (early return on failure)
		if (!(yield this.wait_for_pull(this.settings_dialog.pull_manager, model_usage.model))) {
			GLib.warning("Model pull failed: %s", model_usage.model);
			return false;
		}
		
		// Verify model is now available (early return on failure)
		if (!(yield model_usage.verify_model(config))) {
			GLib.warning("Model %s still not available after pull", model_usage.model);
			return false;
		}
	}
	
	return true;  // All required models are available
}
```

Update `initialize_unverified_client()` to call this method:

```vala
private async void initialize_unverified_client(OLLMchat.Settings.Config2 config)
{
	// Loop until both connection and model succeed
	while (true) {
		// ... existing connection checking code ...
		
		// Found a working connection - now ensure default model is set
		yield this.initialize_model(config, working_conn);
		
		// NEW: Ensure all required models are available (early return on failure)
		if (!(yield this.ensure_required_models(config))) {
			// User cancelled or pull failed - show error and allow retry
			var response = yield this.show_connection_error_dialog(
				"Required models are not available. Please ensure models are downloaded."
			);
			
			// Early return: user cancelled
			if (response != "settings") {
				(this.app as Gtk.Application).quit();
				return;
			}
			
			// Show settings dialog and loop again (early return to restart loop)
			ulong signal_id = 0;
			signal_id = this.settings_dialog.closed.connect(() => {
				this.settings_dialog.disconnect(signal_id);
				this.initialize_unverified_client.begin(this.app.config);
			});
			this.show_settings_dialog("tools");
			return;
		}
		
		// All required models are available - continue with initialization
		// ... rest of existing initialization code ...
	}
}
```

### Phase 3: Documentation Indexing Classes

This phase creates the new classes needed for documentation file processing.

#### 3.1. VectorMetadata Extensions

**File**: `libocvector/VectorMetadata.vala`

Add new properties and methods to VectorMetadata class:

```vala
/**
 * Parent VectorMetadata reference (for documentation section hierarchy).
 * 
 * Null for document root or code elements (which use parent_class instead).
 * Used to traverse up the hierarchy to build context headers.
 */
public VectorMetadata? parent { get; set; default = null; }

/**
 * Document category (for documentation files only).
 * 
 * Values: "plan", "documentation", "rule", "configuration", "data", "license", "changelog", "other"
 * Empty string for code elements.
 */
public string category { get; set; default = ""; }

/**
 * Child sections (for documentation files only).
 * 
 * Used for top-down traversal during analysis. Empty for leaf sections and code elements.
 */
public Gee.ArrayList<VectorMetadata> children { get; private set; default = new Gee.ArrayList<VectorMetadata>(); }

/**
 * Gets section context by traversing up the parent chain.
 * 
 * Collects descriptions from all parent sections and returns them as a formatted string.
 * Used to build SECTION CONTEXT headers for documentation chunks.
 * 
 * @return Formatted context string, or empty string if no parents
 */
public string get_section_context()
{
	// Early return: no parent
	if (this.parent == null) {
		return "";
	}
	
	string[] context_parts = {};
	var current = this.parent;
	
	// Traverse up parent chain
	while (current != null) {
		var desc = current.description.strip();
		if (desc != "") {
			// Insert at beginning to maintain order (prepend to array)
			string[] new_parts = { desc };
			context_parts = new_parts + context_parts;
		}
		current = current.parent;
	}
	
	if (context_parts.length == 0) {
		return "";
	}
	
	return string.joinv(" > ", context_parts);
}

```

Update database schema in `initDB()`:

```vala
public static void initDB(SQ.Database db)
{
	// ... existing table creation ...
	
	// Migrate: add category column if it doesn't exist
	var migrate_category = "ALTER TABLE vector_metadata ADD COLUMN category TEXT NOT NULL DEFAULT ''";
	if (Sqlite.OK != db.db.exec(migrate_category, null, out errmsg)) {
		if (!errmsg.contains("duplicate column name")) {
			GLib.debug("Migration note (may be expected): %s", errmsg);
		}
	}
	
	// Note: parent and children are not stored in database (runtime-only references)
	// They are reconstructed from ast_path hierarchy during analysis
}
```

#### 3.2. Prompt Template Files

**Files**: 
- `resources/ocvector/analysis-documentation-prompt.txt` (NEW FILE)
- `resources/ocvector/analysis-documentation-document-prompt.txt` (NEW FILE)
- `resources/gresources.xml` (UPDATE)

Create two new prompt template files for documentation analysis:

**1. `analysis-documentation-prompt.txt`** - For Level B (section analysis)

This prompt is used to analyze individual sections with their full content (including all subsections) and parent context. Format should follow the existing `analysis-prompt.txt` pattern with `---` separator.

**Note**: The code implementation needs to:
- Include full section content with all subsections in `{section_content}` (not just direct content)
- Add `{parent_sections}` placeholder with information about parent sections that contain this section

```
You are a documentation analysis assistant. Your task is to generate a SHORT, ONE-LINE description of documentation sections for semantic search indexing.

CRITICAL REQUIREMENTS:
- ONE LINE ONLY - do not write multiple sentences or paragraphs
- SHORT - keep it brief and concise (ideally under 20 words)
- NO DETAIL - do not go into implementation details, just state what the section covers
- NO MARKDOWN - no formatting, no code blocks, no special characters
- NO EXPLANATIONS - just the description text, nothing else

Generate a single, short sentence that describes what the section covers or discusses. Focus only on:
- The section's primary topic or purpose
- What information or concepts it presents

Return ONLY the one-line description text - no explanations, no markdown, no JSON, no additional text, just the plain single-line description.
---
Analyze the following documentation section and generate a SHORT ONE-LINE description:

IMPORTANT: Do not extract or infer any additional context beyond what is provided here. Only summarize based on the information given in this prompt - the section content (which includes all subsections and their content) and parent context provided below.

You are analyzing the section '{section_title}'.

This section is contained within the following parent sections:

{parent_sections}

Here is the complete content of this section (including all subsections and their full content):

{section_content}

Generate a single, short one-line description that summarizes the content in concrete detail (not the heading rephrased, and not generic). Include all subsections. Do not use "this section" in your output.
```

**2. `analysis-documentation-document-prompt.txt`** - For Level A (document categorization and summary)

This prompt is used to categorize and summarize entire documents. Should request format "CATEGORY: description" where category is one of: plan, documentation, rule, configuration, data, license, changelog, other.

```
You are a documentation analysis assistant. Your task is to categorize a documentation file and generate a SHORT, ONE-LINE summary for semantic search indexing.

CRITICAL REQUIREMENTS:
- Format your response as "CATEGORY: description" where CATEGORY MUST be exactly one of the categories listed below
- DO NOT CREATE OR INVENT NEW CATEGORIES - you must use ONLY one of the 8 categories defined below
- DO NOT USE VARIATIONS OR SIMILAR WORDS - use the exact category name as listed (lowercase, no modifications)
- If the document doesn't clearly fit any category, use "other" - do not create a new category
- ONE LINE ONLY for the description - do not write multiple sentences or paragraphs
- SHORT - keep the description brief and concise (ideally under 20 words)
- NO DETAIL - do not go into implementation details, just state what the document provides or covers
- NO MARKDOWN - no formatting, no code blocks, no special characters
- NO EXPLANATIONS - just the "CATEGORY: description" text, nothing else

VALID CATEGORIES (you MUST use one of these exactly as written):
- plan: Planning documents, roadmaps, feature specifications, project plans
- documentation: General documentation, guides, tutorials, API docs, README files
- rule: Rules, standards, coding standards, style guides, conventions
- configuration: Configuration files, settings, setup instructions
- data: Data files, sample data, test data
- license: License files, legal text
- changelog: Changelog files, release notes, version history
- other: Uncategorized text files that don't fit any of the above 7 categories

Generate a single line in the format "CATEGORY: description" where CATEGORY is exactly one of the 8 categories listed above (use lowercase, no variations), and the description is a short sentence that describes what the document provides or covers.

Return ONLY the "CATEGORY: description" text - no explanations, no markdown, no JSON, no additional text, just the category and description separated by a colon.
---
Analyze the following documentation file and generate a categorization and SHORT ONE-LINE description:

IMPORTANT: Do not extract or infer any additional context beyond what is provided here. Only categorize and summarize based on the information given in this prompt - the file path, filename, and content provided below.

You are analyzing the file '{filename}' located at '{filepath}'.

Here is the complete content of the document:

{document_content}

Generate a single line in the format "CATEGORY: description" that categorizes this document and provides a short one-line summary of what it provides or covers.

REMEMBER: You MUST use exactly one of the 8 categories listed above (plan, documentation, rule, configuration, data, license, changelog, or other). Do NOT create new categories or use variations of the category names.
```

**3. Update `resources/gresources.xml`**

Add both new files to the `/ocvector` gresource:

```xml
<gresource prefix="/ocvector">
  <file>ocvector-prompt.txt</file>
  <file>analysis-prompt.txt</file>
  <file>analysis-prompt-file.txt</file>
  <file>analysis-documentation-prompt.txt</file>
  <file>analysis-documentation-document-prompt.txt</file>
</gresource>
```

#### 3.3. DocumentationTree Class

**File**: `libocvector/Indexing/DocumentationTree.vala` (NEW FILE)

```vala
namespace OLLMvector.Indexing
{
	/**
	 * Markdown/documentation parsing and VectorMetadata creation.
	 * 
	 * Parses documentation files (markdown, plain text) to extract sections
	 * and create VectorMetadata objects with hierarchical structure.
	 */
	public class DocumentationTree : OLLMfiles.TreeBase
	{
		/**
		 * Array of VectorMetadata objects extracted from document sections.
		 */
		public Gee.ArrayList<VectorMetadata> elements { get; private set; default = new Gee.ArrayList<VectorMetadata>(); }
		
		/**
		 * Root element (document-level metadata).
		 */
		public VectorMetadata? root_element { get; private set; default = null; }
		
		/**
		 * Document category (plan, documentation, rule, etc.).
		 */
		public string category { get; private set; default = ""; }
		
		/**
		 * Constructor.
		 */
		public DocumentationTree(OLLMfiles.File file)
		{
			base(file);
		}
		
		/**
		 * Main entry point: parse file and populate elements array.
		 */
		public async void parse() throws GLib.Error
		{
			// Load file content using base class method
			yield this.load_file_content();
			
			// Category will be determined during analysis (Level A)
			this.category = "";
			
			// Check if file is markdown
			var is_markdown = this.file.path.has_suffix(".md") || 
			                  this.file.path.has_suffix(".markdown");
			
			if (is_markdown) {
				// Parse markdown sections
				this.parse_markdown_sections();
				return;
			} 
				// Plain text file - create single element
			this.parse_plain_text();
			
		}
		
		
		/**
		 * Parses markdown file into hierarchical sections using VectorMetadata directly.
		 */
		private void parse_markdown_sections()
		{
			// Parse headings using regex: ^#{1,6}\s+(.+)$
			var heading_regex = new GLib.Regex("^#{1,6}\\s+(.+)$", GLib.RegexCompileFlags.MULTILINE);
			
			var root_sections = new Gee.ArrayList<VectorMetadata>();
			VectorMetadata? current_section = null;
			int current_level = 0;
			
			for (int i = 0; i < this.lines.length; i++) {
				var line = this.lines[i];
				GLib.MatchInfo match_info;
				
				// Skip non-heading lines (use continue to reduce nesting)
				if (!heading_regex.match(line, 0, out match_info)) {
					// Content line - update current section end line
					if (current_section != null) {
						current_section.end_line = i + 1;
					}
					continue;
				}
				
				// Process heading
				var heading_text = match_info.fetch(1).strip();
				var level = this.get_heading_level(line);
				
				// Create new VectorMetadata section
				var section = new VectorMetadata() {
					element_type = "section",
					element_name = heading_text,
					file_id = this.file.id,
					start_line = i + 1,
					end_line = i + 1,
					category = this.category,
					ast_path = ""
				};
				
				// Build hierarchy
				if (current_section == null) {
					// Root section
					root_sections.add(section);
					this.root_element = section;
				} else if (level > current_level) {
					// Child of current section
					current_section.children.add(section);
					section.parent = current_section;
					section.ast_path = current_section.ast_path + "-" + heading_text;
				} else {
					// Sibling or ancestor - find appropriate parent
					var parent = this.find_parent_section(root_sections, current_section, level);
					if (parent != null) {
						parent.children.add(section);
						section.parent = parent;
						section.ast_path = parent.ast_path + "-" + heading_text;
					} else {
						// New root-level section
						root_sections.add(section);
						section.ast_path = heading_text;
					}
				}
				
				current_section = section;
				current_level = level;
				this.elements.add(section);
			}
		}
		
		/**
		 * Parses plain text file (creates single element).
		 */
		private void parse_plain_text()
		{
			// Create single element for entire file
			var metadata = new VectorMetadata() {
				element_type = "document",
				element_name = GLib.Path.get_basename(this.file.path),
				file_id = this.file.id,
				start_line = 1,
				end_line = this.lines.length,
				category = this.category,
				ast_path = ""
			};
			
			this.root_element = metadata;
			this.elements.add(metadata);
		}
		
		/**
		 * Gets heading level (number of # characters).
		 * 
		 * Uses string methods instead of character looping.
		 */
		private int get_heading_level(string line)
		{
			// Extract leading # characters using string methods
			var trimmed = line.strip();
			if (!trimmed.has_prefix("#")) {
				return 0;
			}
			
			// Split on space to get the # prefix (e.g., "### Title" -> ["###", "Title"])
			var parts = trimmed.split(" ", 2);
			if (parts.length == 0) {
				return 0;
			}
			
			var hash_prefix = parts[0];
			// Verify prefix is all # characters: remove all # and check if result is empty
			var without_hash = hash_prefix.replace("#", "");
			if (without_hash != "") {
				// Prefix contains non-# characters, not a valid heading
				return 0;
			}
			
			// Prefix is all # characters, length is the level
			var hash_count = hash_prefix.length;
			return hash_count > 6 ? 6 : hash_count;
		}
		
		/**
		 * Finds parent section for given level by traversing up from current section.
		 * 
		 * Walks up the parent chain from current_section until finding a section
		 * with level < target_level, or returns null if no suitable parent exists.
		 */
		private VectorMetadata? find_parent_section(
			Gee.ArrayList<VectorMetadata> root_sections,
			VectorMetadata current_section,
			int target_level)
		{
			// Walk up parent chain to find section with level < target_level
			var candidate = current_section.parent;
			while (candidate != null) {
				// Calculate candidate's level from ast_path depth
				var candidate_level = candidate.ast_path.split("-").length;
				if (candidate_level < target_level) {
					return candidate;
				}
				candidate = candidate.parent;
			}
			
			// No suitable parent found in chain - check root sections
			// Find root section that could be parent (would have level < target_level)
			// Since we don't store level in VectorMetadata, we infer from ast_path
			// Root sections have ast_path without dashes (level 1)
			foreach (var root in root_sections) {
				if (root.ast_path.split("-").length < target_level) {
					return root;
				}
			}
			
			return null;
		}
		
	}
}
```

4. **Phase 4: Documentation Indexing Integration** - Integrate documentation processing with existing code

#### 3.4. DocumentationAnalysis Class

**File**: `libocvector/Indexing/DocumentationAnalysis.vala` (NEW FILE)

**Note**: This class uses the PromptTemplate class created in Phase 1.

```vala
namespace OLLMvector.Indexing
{
	public class DocumentationAnalysis : VectorBase
	{
		private SQ.Database sql_db;
		private static PromptTemplate? cached_template = null;
		private static PromptTemplate? cached_document_template = null;
		
		/**
		 * Static constructor - loads templates at class initialization.
		 */
		static construct
		{
			try {
				cached_template = new PromptTemplate("analysis-documentation-prompt.txt");
				cached_template.load();
				
				cached_document_template = new PromptTemplate("analysis-documentation-document-prompt.txt");
				cached_document_template.load();
			} catch (GLib.Error e) {
				GLib.critical("Failed to load prompt templates in static constructor: %s", e.message);
			}
		}
		
		/**
		 * Emitted when an element is finished being analyzed.
		 */
		public signal void element_analyzed(string element_name, int element_number, int total_elements);
		
		public DocumentationAnalysis(OLLMchat.Settings.Config2 config, SQ.Database sql_db)
		{
			base(config);
			this.sql_db = sql_db;
		}
		
		/**
		 * Analyzes a DocumentationTree and generates descriptions.
		 * 
		 * Three-level processing:
		 * 1. Level A: Document summary (root element)
		 * 2. Level B: Section summaries (with child context)
		 * 3. Level C: Leaf section extraction
		 */
		public async DocumentationTree analyze_tree(DocumentationTree tree) throws GLib.Error
		{
			// Level A: Analyze document (root element)
			if (tree.root_element != null) {
				yield this.analyze_document(tree.root_element, tree);
			}
			
			// Level B: Analyze sections (top-down traversal)
			yield this.analyze_sections(tree);
			
			// Level C: Extract leaf sections (already done during parsing)
			// Leaf sections are ready for vectorization
			
			// Sync database after processing
			this.sql_db.backupDB();
			
			return tree;
		}
		
		/**
		 * Level A: Analyzes entire document and generates summary and category.
		 */
		private async void analyze_document(VectorMetadata root_element, DocumentationTree tree) throws GLib.Error
		{
			// Skip if already has description
			if (root_element.description != "") {
				return;
			}
			
			// Get full document content
			var document_content = tree.lines_to_string(1, tree.lines.length);
			var filename = GLib.Path.get_basename(tree.file.path);
			var filepath = tree.file.path;
			
			// Build user message (includes path and filename for LLM categorization)
			var user_message = cached_document_template.fill(
				"document_content", document_content,
				"filename", filename,
				"filepath", filepath
			);
			
			// Call LLM
			var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
			var analysis_conn = yield this.connection("analysis");
			
			var chat = new OLLMchat.Call.Chat(
				analysis_conn,
				tool_config.analysis.model) {
				stream = false,
				options = tool_config.analysis.options
			};
			
			var messages = new Gee.ArrayList<OLLMchat.Message>();
			if (cached_document_template.system_message != "") {
				messages.add(new OLLMchat.Message("system", cached_document_template.system_message));
			}
			messages.add(new OLLMchat.Message("user", user_message));
			
			var response = yield chat.send(messages, null);
			if (response != null && response.message != null && response.message.content != null) {
				var result = response.message.content.strip();
				// Parse result: format is "CATEGORY: description" or just "description"
				var parts = result.split(":", 2);
				if (parts.length == 2) {
					var category_candidate = parts[0].strip().down();
					// Validate category is one of the known categories
					switch (category_candidate) {
						case "plan":
						case "documentation":
						case "rule":
						case "configuration":
						case "data":
						case "license":
						case "changelog":
						case "other":
							root_element.category = category_candidate;
							root_element.description = parts[1].strip();
							break;
						default:
							// Invalid category prefix - treat as description only
							root_element.category = "other";
							root_element.description = result;
							break;
					}
				} else {
					// No category prefix - LLM didn't provide one, use "other"
					root_element.category = "other";
					root_element.description = result;
				}
				// Update tree category
				tree.category = root_element.category;
			}
		}
		
		/**
		 * Level B: Analyzes sections with child context.
		 */
		private async void analyze_sections(DocumentationTree tree) throws GLib.Error
		{
			// Top-down traversal: process parent sections before children
			var sections_to_process = new Gee.ArrayList<VectorMetadata>();
			
			// Collect all sections (excluding root)
			foreach (var element in tree.elements) {
				if (element.element_type == "section") {
					sections_to_process.add(element);
				}
			}
			
			// Sort by level (shallow to deep)
			sections_to_process.sort((a, b) => {
				var a_level = a.ast_path.split("-").length;
				var b_level = b.ast_path.split("-").length;
				return a_level - b_level;
			});
			
			// Process each section
			int element_number = 0;
			foreach (var section in sections_to_process) {
				element_number++;
				
				// Skip if already has description
				if (section.description != "") {
					this.element_analyzed(section.element_name, element_number, sections_to_process.size);
					continue;
				}
				
				// Skip small sections (< 10 words)
				var content = tree.lines_to_string(section.start_line, section.end_line);
				if (content.split(" ").length < 10) {
					section.description = "";
					this.element_analyzed(section.element_name, element_number, sections_to_process.size);
					continue;
				}
				
				// Get full section content including all subsections
				// Note: content should include all text from this section through all its child sections
				var full_content = tree.lines_to_string(section.start_line, section.end_line);
				
				// Collect parent context (sections that contain this section)
				// Format: "description1 > description2 > description3" (using same format as get_section_context())
				string[] parent_descriptions = {};
				var current_parent = section.parent;
				while (current_parent != null) {
					var parent_desc = current_parent.description.strip();
					if (parent_desc != "") {
						// Insert at beginning to maintain order (prepend to array)
						string[] new_parts = { parent_desc };
						parent_descriptions = new_parts + parent_descriptions;
					}
					current_parent = current_parent.parent;
				}
				var parent_context_text = parent_descriptions.length > 0 
					? string.joinv(" > ", parent_descriptions)
					: "(top-level section)";
				
				// Build user message
				// Note: full_content already includes all text from section.start_line to section.end_line,
				// which includes all subsections and their content
				var user_message = cached_template.fill(
					"section_content", full_content,
					"section_title", section.element_name,
					"parent_sections", parent_context_text
				);
				
				// Call LLM
				var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
				var analysis_conn = yield this.connection("analysis");
				
				var chat = new OLLMchat.Call.Chat(
					analysis_conn,
					tool_config.analysis.model) {
					stream = false,
					options = tool_config.analysis.options
				};
				
			var messages = new Gee.ArrayList<OLLMchat.Message>();
			if (cached_template.system_message != "") {
				messages.add(new OLLMchat.Message("system", cached_template.system_message));
			}
				messages.add(new OLLMchat.Message("user", user_message));
				
				var response = yield chat.send(messages, null);
				if (response != null && response.message != null && response.message.content != null) {
					section.description = response.message.content.strip();
				}
				
				this.element_analyzed(section.element_name, element_number, sections_to_process.size);
			}
		}
		
	}
}
```

#### 3.5. DocumentationVectorBuilder Class

**File**: `libocvector/Indexing/DocumentationVectorBuilder.vala` (NEW FILE)

```vala
namespace OLLMvector.Indexing
{
	/**
	 * Vector building layer for documentation file processing.
	 * 
	 * Extends VectorBuilder to handle documentation-specific chunking
	 * with context headers.
	 */
	public class DocumentationVectorBuilder : VectorBuilder
	{
		/**
		 * Constructor.
		 */
		public DocumentationVectorBuilder(
			OLLMchat.Settings.Config2 config,
			OLLMvector.Database database,
			SQ.Database sql_db)
		{
			base(config, database, sql_db);
		}
		
		/**
		 * Processes a DocumentationTree and generates vectors for leaf sections.
		 * 
		 * Overrides VectorBuilder.process_file() to handle documentation chunking.
		 */
		public async void process_file(DocumentationTree tree) throws GLib.Error
		{
			if (tree.elements.size == 0) {
				return;
			}
			
			// Find all leaf sections (sections with no children)
			var leaf_sections = new Gee.ArrayList<VectorMetadata>();
			foreach (var element in tree.elements) {
				if (element.children.size == 0) {
					leaf_sections.add(element);
				}
			}
			
			// Separate into unchanged and changed (similar to VectorBuilder)
			var unchanged_elements = new Gee.ArrayList<VectorMetadata>();
			var changed_elements = new Gee.ArrayList<VectorMetadata>();
			// ... incremental processing logic (similar to VectorBuilder) ...
			
			// Process changed/new leaf sections
			if (changed_elements.size == 0) {
				return;
			}
			
			// Chunk each leaf section and create documents
			var documents = new Gee.ArrayList<string>();
			var chunk_metadata = new Gee.ArrayList<VectorMetadata>();
			
			foreach (var section in changed_elements) {
				// Get section content
				var section_content = tree.lines_to_string(section.start_line, section.end_line);
				
				// Chunk section content (split into appropriate-sized chunks)
				var chunks = this.chunk_section_content(section_content);
				
				// Create document for each chunk with context headers
				foreach (var chunk in chunks) {
					var document = this.format_documentation_chunk(
						section,
						chunk,
						tree
					);
					documents.add(document);
					
					// Create metadata for this chunk (copy of section with chunk info)
					var chunk_meta = this.create_chunk_metadata(section, chunk);
					chunk_metadata.add(chunk_meta);
				}
			}
			
			// Get embed model (unified for code and documentation)
			var tool_config = this.config.tools.get("codebase_search") as OLLMvector.Tool.CodebaseSearchToolConfig;
			
			// Create embeddings
			var embed_response = yield new OLLMchat.Client(
				this.config.connections.get(tool_config.embed.connection)
			).embed_array(
				tool_config.embed.model,
				documents,
				-1,
				false,
				tool_config.embed.options
			);
			
			// Store vectors in FAISS and metadata in SQL
			// ... similar to VectorBuilder.process_file() ...
		}
		
		/**
		 * Formats a documentation chunk with context headers.
		 * 
		 * Overrides VectorBuilder.format_element_document().
		 */
		private string format_documentation_chunk(
			VectorMetadata section,
			string chunk_content,
			DocumentationTree tree)
		{
			string doc = "DOCUMENT: " + GLib.Path.get_basename(tree.file.path) + "\n";
			
			// DOCUMENT SUMMARY header (from root element)
			if (tree.root_element != null && tree.root_element.description != "") {
				doc += "DOCUMENT SUMMARY: " + tree.root_element.description.strip() + "\n";
			}
			
			// SECTION CONTEXT header (only if nesting level > 1)
			var nesting_level = section.ast_path.split("-").length;
			if (nesting_level > 1) {
				var section_context = section.get_section_context();
				if (section_context != "") {
					doc += "SECTION CONTEXT: " + section_context + "\n";
				}
			}
			
			// Section content
			doc += "\n" + chunk_content;
			
			return doc;
		}
		
		/**
		 * Chunks section content into appropriate-sized pieces.
		 */
		private Gee.ArrayList<string> chunk_section_content(string content)
		{
			var chunks = new Gee.ArrayList<string>();
			
			// Simple chunking: split by paragraphs, then combine into ~500 word chunks
			var paragraphs = content.split("\n\n");
			var current_chunk = new GLib.StringBuilder();
			int current_word_count = 0;
			const int TARGET_WORDS = 500;
			
			foreach (var paragraph in paragraphs) {
				var word_count = paragraph.split(" ").length;
				
				if (current_word_count + word_count > TARGET_WORDS && current_chunk.len > 0) {
					// Save current chunk and start new one
					chunks.add(current_chunk.str.strip());
					current_chunk = new GLib.StringBuilder();
					current_word_count = 0;
				}
				
				if (current_chunk.len > 0) {
					current_chunk.append("\n\n");
				}
				current_chunk.append(paragraph);
				current_word_count += word_count;
			}
			
			// Add final chunk
			if (current_chunk.len > 0) {
				chunks.add(current_chunk.str.strip());
			}
			
			return chunks;
		}
		
		/**
		 * Creates metadata for a chunk (copy of section metadata).
		 */
		private VectorMetadata create_chunk_metadata(VectorMetadata section, string chunk)
		{
			// Create copy of section metadata
			var chunk_meta = new VectorMetadata() {
				file_id = section.file_id,
				element_type = section.element_type,
				element_name = section.element_name,
				category = section.category,
				ast_path = section.ast_path,
				parent = section.parent,
				description = section.description
			};
			
			// Note: start_line and end_line would need to be calculated based on chunk position
			// This is simplified - actual implementation would track line numbers per chunk
			
			return chunk_meta;
		}
	}
}
```

### Phase 4: Documentation Indexing Integration

This phase integrates the documentation processing classes with the existing indexing pipeline.

#### 4.1. File.is_documentation Method

**File**: `libocfiles/File.vala` (or `libocfiles/FileBase.vala` if shared with other file types)

Add a method on File so callers can ask whether the file is documentation (plain text / markdown, not code or structured data). Uses existing `is_text` and `language` (from `BufferProvider.detect_language()`).

```vala
/**
 * Whether this file is a documentation file (plain text or markdown, not code).
 * 
 * Uses is_text and language (from BufferProvider.detect_language()).
 * Returns true for markdown, plain text, and unknown text; false for code and structured formats.
 */
public bool is_documentation()
{
	if (!this.is_text) {
		return false;
	}
	
	var lang = this.language;
	if (lang == null || lang == "") {
		return true;
	}
	
	var lang_lower = lang.down();
	
	switch (lang_lower) {
		case "markdown":
		case "txt":
		case "text":
		case "plaintext":
			return true;
		default:
			// Code languages (vala, python, c, etc.) and structured formats (html, xml, json, yaml, css, etc.)
			return false;
	}
}
```

#### 4.2. Indexer Routing Updates

**File**: `libocvector/Indexing/Indexer.vala`

Update `index_file()` to route using `file.is_documentation()`:

```vala
private async bool index_file(OLLMfiles.File file, bool force = false) throws GLib.Error
{
	// ... existing checks (is_ignored, is_text) ...
	
	if (file.is_documentation()) {
		return yield this.index_documentation_file(file, force);
	} else {
		return yield this.index_code_file(file, force);
	}
}

/**
 * Indexes a documentation file using documentation pipeline.
 */
private async bool index_documentation_file(OLLMfiles.File file, bool force = false) throws GLib.Error
{
	// ... incremental check (similar to code pipeline) ...
	
	// Create DocumentationTree
	var tree = new DocumentationTree(file);
	yield tree.parse();
	
	if (tree.elements.size == 0) {
		// ... handle empty file ...
		return true;
	}
	
	// Create DocumentationAnalysis
	var analysis = new DocumentationAnalysis(this.config, this.sql_db);
	
	// Connect to element_analyzed signal
	analysis.element_analyzed.connect((element_name, element_number, total_elements) => {
		this.element_scanned(element_name, element_number, total_elements);
	});
	
	// Analyze tree
	tree = yield analysis.analyze_tree(tree);
	
	// Create DocumentationVectorBuilder
	var vector_builder = new DocumentationVectorBuilder(
		this.config, this.vector_db, this.sql_db);
	
	// Process file
	yield vector_builder.process_file(tree);
	
	// ... update last_vector_scan and save ...
	
	return true;
}

/**
 * Indexes a code file using code pipeline (existing logic).
 */
private async bool index_code_file(OLLMfiles.File file, bool force = false) throws GLib.Error
{
	// Existing code pipeline logic (unchanged)
	// ...
}
```

## Related Plans

- 2.20-codebase-scanner-improvements.md - Parent plan
- 2.20.1-md5-incremental-analysis.md - MD5 caching (may affect documentation indexing)
- 2.10-codebase-search-tool.md - Existing codebase search infrastructure
