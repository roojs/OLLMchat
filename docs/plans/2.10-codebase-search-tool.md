# 2.10. Codebase Search Tool

## Overview

Implementation of a semantic codebase search tool using vector embeddings and FAISS. The code will be migrated from the existing `VectorSearch/` directory into a new `libocvector` library with two main components: **Indexing** and **Search**. The library includes its own FileBase structure (copied from `liboccoder`) to track indexed files and support incremental updates, avoiding dependency issues.

## Status

ðŸš§ **IN PROGRESS** - Foundation and FAISS migration completed. Core indexing and search components pending.

## Todo List

### Phase 1: Foundation & Structure
- [x] Create `libocvector/` directory structure
- [ ] Create `libocvector/Files/` directory for FileBase structure
- [x] Set up `libocvector/meson.build` for the library
- [x] Create namespace structure (`OLLMvector.Files`, `OLLMvector.Indexing`, `OLLMvector.Search`, `OLLMvector.Tools.Search`, `OLLMvector.Tool`) - *Partially done: OLLMvector namespace exists, subdirectories not yet created*

### Phase 2: FAISS Migration
- [x] Migrate `VectorSearch/Index.vala` â†’ `libocvector/Index.vala` (FAISS namespace)
- [x] Migrate `VectorSearch/Database.vala` â†’ `libocvector/Database.vala`
- [x] Update Database namespace to `OLLMvector`
- [x] Create FAISS C++ wrapper (`faiss_c_wrapper.cpp`) - *Additional work not in original plan*
- [ ] Integrate Database with `OLLMvector.Files.File` for metadata tracking - *TODOs in code indicate this is pending*

### Phase 3: Analysis Layer Implementation
- [ ] Create `libocvector/Indexing/Analysis.vala`
- [ ] Create `resources/ocvector/result-schema.json` with JSON schema for structured outputs
- [ ] Add `format_obj` property to `Call.Chat` class (type: `Json.Object?`)
- [ ] Update `Call.Chat.serialize_property()` to use `format_obj` when set (output JSON object instead of string `format`)
- [ ] Implement helper method to load JSON schema from `resources/ocvector/result-schema.json`
- [ ] Implement LLM API integration with structured outputs (using `format_obj`)
- [ ] Implement prompt generation for code analysis
- [ ] Implement JSON response parsing and validation
- [ ] Integrate with `OLLMvector.Files` for file tracking
- [ ] Add error handling and fallback strategies

### Phase 4: Vector Building Implementation
- [ ] Create `libocvector/Indexing/VectorBuilder.vala`
- [ ] Implement document formatting for vectorization (type, name, file, lines, description, parameters, return type, dependencies, code snippet)
- [ ] Implement Ollama embeddings API integration (`/api/embeddings`)
- [ ] Implement batch processing for vector generation
- [ ] Implement FAISS vector storage
- [ ] Implement metadata storage (file path, element info, line numbers, vector ID, timestamp)

### Phase 5: Indexer Orchestration
- [ ] Create `libocvector/Indexing/Indexer.vala`
- [ ] Integrate Analysis and VectorBuilder components
- [ ] Implement incremental update logic
- [ ] Implement file change detection using `OLLMvector.Files.File.mtime`
- [ ] Implement project-wide indexing using `OLLMvector.Files.Folder`

### Phase 6: Search Layer Implementation
- [ ] Create `libocvector/Search/Searcher.vala`
- [ ] Migrate and refactor `VectorSearch/CodeSearch.vala`
- [ ] Implement query vectorization (convert queries to embeddings)
- [ ] Implement FAISS similarity search
- [ ] Add GPU support detection and usage
- [ ] Implement CPU fallback
- [ ] Create `libocvector/Search/QueryProcessor.vala`
- [ ] Implement query type detection and routing
- [ ] Implement result ranking and formatting

### Phase 7: FileBase Structure Copy
- [ ] Copy `liboccoder/Files/FileBase.vala` â†’ `libocvector/Files/FileBase.vala`
- [ ] Copy `liboccoder/Files/File.vala` â†’ `libocvector/Files/File.vala`
- [ ] Copy `liboccoder/Files/Folder.vala` â†’ `libocvector/Files/Folder.vala`
- [ ] Update namespaces from `OLLMcoder.Files` to `OLLMvector.Files`
- [ ] Update database table names (e.g., `vector_filebase` instead of `filebase`)
- [ ] Remove or simplify ProjectManager dependency
- [ ] Add vector indexing metadata fields to `OLLMvector.Files.File`
- [ ] Implement metadata storage in database
- [ ] Store vector position, indexing status, and timestamps
- [ ] Implement change detection using `mtime`

### Phase 8: Command-Line Tools
- [x] Create `examples/oc-vector-index.vala` (single file indexing tool)
- [ ] Create `libocvector/Tools/SearchTool.vala`
- [ ] Create `examples/oc-vector-search.vala` (search tool executable)
- [x] Implement command-line argument parsing - *Basic version implemented in oc-vector-index*
- [ ] Add progress display for indexing
- [ ] Add result formatting for search output

### Phase 9: OLLM Tool Integration
- [ ] Create `libocvector/Tool/CodebaseSearchTool.vala`
- [ ] Extend `OLLMchat.Tool.Interface`
- [ ] Implement `execute()` method for tool calling
- [ ] Implement structured result format for LLM agents
- [ ] Add to `libollmchat` tool registry
- [ ] Integrate with permission system

### Phase 10: Build System & Dependencies
- [x] Update root `meson.build` to include `libocvector` subdirectory
- [x] Add dependencies to `libocvector/meson.build` (libocsqlite, libollmchat, FAISS)
- [x] Note: liboccoder is NOT a dependency (FileBase structure is copied)
- [x] Generate VAPI files for `libocvector` - *Generated via meson.build*
- [ ] Generate GIR files for `libocvector` - *Commented out in meson.build, pending implementation*
- [x] Update library dependencies in consuming projects - *examples/meson.build updated*
- [x] Update VAPI dependencies

### Phase 11: Testing
- [ ] Test indexing with various codebases (Vala, Python, JavaScript, etc.)
- [ ] Test search functionality with different query types
- [ ] Test file change detection and re-indexing
- [ ] Test command-line tools (`oc-vector-index`, `oc-vector-search`)
- [ ] Test OLLM tool integration
- [ ] Test GPU vs CPU search performance
- [ ] Test incremental updates
- [ ] Test structured outputs reliability
- [ ] Test batch processing for vectorization
- [ ] Test error handling and fallbacks

### Phase 12: Cleanup & Documentation
- [ ] Remove or deprecate `VectorSearch/` directory
- [ ] Update project documentation
- [ ] Add code comments and documentation
- [ ] Create usage examples
- [ ] Document API for library consumers

## Architecture

### Library Structure: `libocvector`

The vector search functionality will be organized as a new library `libocvector` with the following structure:

```
libocvector/
â”œâ”€â”€ meson.build
â”œâ”€â”€ Files/                      # FileBase structure (copied from liboccoder, no dependency)
â”‚   â”œâ”€â”€ FileBase.vala          # Base class for File and Folder
â”‚   â”œâ”€â”€ File.vala              # File class
â”‚   â””â”€â”€ Folder.vala            # Folder class
â”œâ”€â”€ Indexing/
â”‚   â”œâ”€â”€ Analysis.vala          # Code and documentation analysis
â”‚   â”œâ”€â”€ VectorBuilder.vala      # Vector generation and FAISS storage
â”‚   â””â”€â”€ Indexer.vala            # Main indexing orchestrator
â”œâ”€â”€ Search/
â”‚   â”œâ”€â”€ Searcher.vala           # Search functionality with GPU support
â”‚   â””â”€â”€ QueryProcessor.vala     # Query processing and result formatting
â””â”€â”€ Tool/
    â””â”€â”€ CodebaseSearchTool.vala  # OLLM tool integration
```

**Note**: The `Files/` directory contains a copy of the FileBase structure from `liboccoder`. This avoids dependency issues while maintaining the same file/folder tracking capabilities. The code is based on `liboccoder.Files` but exists independently in `libocvector.Files` namespace.

### Component 1: Indexing

The indexing component consists of two layers:

#### Layer 1: Analysis
- **Purpose**: Process code files and send them to LLM to generate structured JSON data for vector generation
- **Input**: 
  - Code files (via `OLLMvector.Files.File` and `OLLMvector.Files.Folder`)
  - Code documentation (markdown, comments, etc.) - *Note: Documentation handling to be determined (may include downloaded content)*
- **Output**: Structured JSON data from LLM that can be used for vector generation
- **Responsibilities**:
  - Read source files from the codebase
  - Send source file content to LLM with a prompt requesting structured JSON output
  - LLM analyzes the code and returns JSON containing semantic elements (classes, functions, methods, etc.)
  - Extract and process the JSON response from LLM
  - Format the JSON data into vector-friendly text representations
  - Handle multiple programming languages
  - Track file metadata (path, modification time via OLLMvector.Files.File)

##### Analysis Prompt Details

The Analysis layer sends code files to the LLM with a specific prompt designed to extract structured semantic information. The prompt format is:

```
Analyze the following {language} code and provide a structured summary. For each class, function, method, struct, interface, and significant code block, include:

1. Class/Method/Function name
2. Starting line number
3. Ending line number
4. Purpose and functionality description
5. Key parameters and return values
6. Dependencies and relationships

Format the response as JSON matching this schema:
{
    "file_path": "string",
    "language": "string",
    "summary": "overall file summary",
    "elements": [
        {
            "type": "class|function|method|struct|interface|enum|namespace",
            "name": "element_name",
            "start_line": number,
            "end_line": number,
            "description": "detailed description of purpose and functionality",
            "parameters": ["param1", "param2"],
            "return_type": "return_type",
            "dependencies": ["dep1", "dep2"]
        }
    ]
}

Code:
{code_content}
```

**Note**: The prompt includes the JSON schema as a string to ground the model's response. The actual schema enforcement is handled by Ollama's `format` parameter (see LLM API Call section below), which ensures reliable structured output even if the model doesn't perfectly follow the prompt.

**LLM API Call**:
- **Endpoint**: `/api/chat` (preferred for structured outputs) or `/api/generate`
- **Model**: Code-aware model (e.g., `codellama`, `deepseek-coder`, `qwen2.5-coder`)
- **Messages**: Single user message with the formatted analysis prompt above
- **Format Parameter**: JSON schema object to enforce structured output. The schema will be:
  - Stored in `resources/ocvector/result-schema.json` as a JSON file
  - Loaded as a `Json.Object?` when needed
  - Passed to `Call.Chat` via a `format_obj` property
  - When `Call.Chat` serializes the request, if `format_obj != null`, it will serialize that `Json.Object` instead of the string `format` property
  - This will be handled in the serialization process (likely in `serialize_property` method or in the method that calls it, outside of the serialize method itself)
  
  The JSON schema structure:
  ```json
  {
    "type": "object",
    "properties": {
      "file_path": {"type": "string"},
      "language": {"type": "string"},
      "summary": {"type": "string"},
      "elements": {
        "type": "array",
        "items": {
          "type": "object",
          "properties": {
            "type": {
              "type": "string",
              "enum": ["class", "function", "method", "struct", "interface", "enum", "namespace"]
            },
            "name": {"type": "string"},
            "start_line": {"type": "integer"},
            "end_line": {"type": "integer"},
            "description": {"type": "string"},
            "parameters": {
              "type": "array",
              "items": {"type": "string"}
            },
            "return_type": {"type": "string"},
            "dependencies": {
              "type": "array",
              "items": {"type": "string"}
            }
          },
          "required": ["type", "name", "start_line", "end_line", "description"]
        }
      }
    },
    "required": ["file_path", "language", "summary", "elements"]
  }
  ```
- **Options**: Set `temperature: 0` for more deterministic structured outputs
- **Response Format**: JSON string containing structured analysis (enforced by format schema)

**Implementation Notes**:
- The JSON schema file will be created at `resources/ocvector/result-schema.json`
- `Call.Chat` will have a new `format_obj` property of type `Json.Object?`
- When serializing the chat call, if `format_obj != null`, the serialization will output the JSON object instead of the string `format` property
- This modification will likely be done in `serialize_property` method or in the method that calls serialization (e.g., `get_request_body()`)

**Analysis Output Processing**:
1. Parse JSON response from LLM
2. Validate structure (file_path, language, elements array)
3. For each element in the JSON:
   - Extract code snippet from original file using line numbers
   - Validate element data (type, name, line numbers)
   - Create `CodeElement` structure with all fields
4. Create `CodeFile` structure containing all elements
5. Pass structured data to Vector Building layer

**Error Handling**:
- Structured outputs via `format` parameter significantly reduce JSON parsing errors
- If LLM still returns invalid JSON (rare with structured outputs), validate and retry once
- If schema validation fails, check for partial matches and extract valid elements
- If analysis fails completely, fall back to basic element extraction (function/class names only) using AST parsing
- Log analysis errors for debugging but continue indexing other files
- Set `temperature: 0` in API options for more deterministic structured outputs

#### Layer 2: Vector Building
- **Purpose**: Convert structured JSON data (from Analysis layer) into vectors and store in FAISS
- **Input**: Structured JSON data from Analysis layer (LLM output)
- **Output**: Vectors stored in FAISS index
- **Responsibilities**:
  - Generate embeddings using LLM embedding API
  - Store vectors in FAISS index
  - Maintain mapping between vectors and source files/elements
  - Handle batch processing for efficiency
  - Support incremental updates (add/update/remove vectors)

##### Vectorization Process Details

The vectorization process converts structured code analysis into searchable text documents that are sent to Ollama's vectorize API. Each code element (class, function, method, struct, interface, etc.) becomes a separate document for vectorization.

**Document Format for Vectorization**:

For each code element extracted by the Analysis layer, a searchable text document is created with the following structure:

```
{type}: {name}
File: {file_path}
Lines: {start_line}-{end_line}
Description: {detailed_description}
Parameters: {comma_separated_parameters}
Returns: {return_type}
Dependencies: {comma_separated_dependencies}
Code:
{code_snippet}
```

**Specific Fields Sent to Ollama Vectorize API**:

1. **Element Type and Name** (`{type}: {name}`)
   - Examples: `class: DatabaseManager`, `function: parse_json`, `method: initialize_connection`
   - Provides semantic context about what kind of code element this is

2. **File Location** (`File: {file_path}`)
   - Full path to the source file
   - Enables location-based filtering and result display

3. **Line Range** (`Lines: {start_line}-{end_line}`)
   - Exact line numbers where the element is defined
   - Critical for precise code navigation

4. **Description** (`Description: {detailed_description}`)
   - Natural language description of the element's purpose and functionality
   - Generated by LLM analysis, includes semantic understanding of what the code does
   - This is the primary semantic content for search matching

5. **Parameters** (`Parameters: {param1}, {param2}, ...`)
   - List of function/method parameters
   - Only included for functions and methods
   - Helps match queries about specific APIs or signatures

6. **Return Type** (`Returns: {return_type}`)
   - Return type information
   - Only included for functions and methods
   - Aids in finding code that returns specific types

7. **Dependencies** (`Dependencies: {dep1}, {dep2}, ...`)
   - Other code elements, libraries, or modules this element depends on
   - Enables relationship-based searches

8. **Code Snippet** (`Code:\n{code_snippet}`)
   - The actual source code for the element (lines start_line through end_line)
   - Provides concrete implementation details for context
   - Helps match queries that reference specific code patterns or implementations

**Example Document for Vectorization**:

```
class: DatabaseManager
File: /path/to/project/src/database.vala
Lines: 45-120
Description: Manages database connections and provides methods for executing queries. Handles connection pooling, transaction management, and error recovery. Automatically retries failed connections and maintains connection health status.
Parameters: connection_string, pool_size, timeout
Returns: DatabaseManager
Dependencies: libocsqlite.Database, OLLMvector.Files.File
Code:
public class DatabaseManager : Object {
    private string connection_string;
    private int pool_size;
    private int timeout;
    
    public DatabaseManager(string connection_string, int pool_size = 10, int timeout = 30) {
        this.connection_string = connection_string;
        this.pool_size = pool_size;
        this.timeout = timeout;
    }
    
    public async bool initialize() throws Error {
        // Connection initialization logic
    }
}
```

**Vectorization API Call**:

Each document (as formatted above) is sent to Ollama's `/api/embeddings` endpoint (or equivalent vectorize API) with:

- **Model**: The embedding model to use (e.g., `nomic-embed-text`, `all-minilm`, or model-specific embedding endpoint)
- **Input**: The complete formatted document string
- **Response**: Vector embedding (array of floats)

**Batch Processing**:

- Multiple documents can be vectorized in a single batch request if the API supports it
- Documents are grouped by file to maintain locality
- Batch size is configurable (default: 10-20 documents per batch)

**Document Metadata Storage**:

Alongside each vector, the following metadata is stored in the database (NOT in FAISS, which only stores vectors):
- Original file path
- Element type and name
- Line range (start_line, end_line)
- Vector ID in FAISS index
- Timestamp of vectorization

**Important**: Original document text is NOT stored. Code snippets are read from the filesystem when needed for result display. This approach:
- Avoids duplication (files already exist in filesystem or are referenced in SQL database)
- Reduces storage requirements
- Ensures results always show current file content
- Enables precise result location (file + line numbers)
- Supports incremental updates (re-vectorizing only changed elements)

### Component 2: Search

The search component provides semantic search capabilities:

- **Purpose**: Perform semantic search on indexed codebase
- **Features**:
  - Use local GPU for search operations when available
  - Fallback to CPU if GPU unavailable
  - Return ranked results with similarity scores
  - Support various query types (functionality search, pattern search, etc.)
- **Responsibilities**:
  - Convert search queries to embedding vectors
  - Perform similarity search in FAISS index
  - Rank and filter results
  - Return formatted results with context

### File Tracking System

The vector indexing system uses its own FileBase structure (copied from `liboccoder`) to:

- **Track Indexed Files**: Store vector metadata linked to `OLLMvector.Files.File` objects
- **Detect Changes**: Use file modification times (`mtime`) from `OLLMvector.Files.File` to detect when files need re-indexing
- **Incremental Updates**: Update vectors when code or documentation changes
- **File Management**: Maintain file/folder hierarchy for indexed codebases

**Note**: The `OLLMvector.Files` structure is based on `liboccoder.Files` but is copied into `libocvector` to avoid dependency issues. The code will never overlap in practice, as `libocvector` uses its own namespace and database tables.

Metadata storage (integrated with `OLLMvector.Files.File`):
- File path and ID (via `OLLMvector.Files.File`)
- File modification time (`mtime`) for change detection
- Last indexed timestamp
- Vector position in FAISS index
- Indexing status (indexed, needs update, error, etc.)

### Command-Line Tools

Command-line tools will be created in the `examples/` directory for testing:

#### Indexing Example (`examples/oc-vector-index.vala`)
- **Purpose**: Test single file indexing from command line
- **Features**:
  - Index a single file for testing
  - Show indexing progress
  - Simple interface for development/testing

#### Search Tool (`oc-vector-search`)
- **Namespace**: `OLLMvector.Tools.Search`
- **Purpose**: Search indexed codebases from command line
- **Features**:
  - Execute semantic search queries
  - Display ranked results
  - Filter by file type, language, etc.
  - Export results in various formats

### OLLM Library Tool Integration

A tool class will be created for use with the `libollmchat` tool system:

- **Class**: `OLLMvector.Tool.CodebaseSearchTool` (extends `OLLMchat.Tool.Interface`)
- **Purpose**: Provide semantic codebase search as a tool for LLM agents
- **Integration**: 
  - Implements `OLLMchat.Tool.Interface`
  - Provides `execute()` method for tool calling
  - Returns search results in structured format
  - Supports permission system integration

## Migration Plan: VectorSearch â†’ libocvector

### Phase 1: Create libocvector Structure
1. Create `libocvector/` directory
2. Set up `meson.build` for the library
3. Create namespace structure (`OLLMvector.Indexing`, `OLLMvector.Search`, `OLLMvector.Tools.Search`, `OLLMvector.Tool`)

### Phase 2: Migrate FAISS Components
1. **Migrate FAISS Integration**:
   - Move `VectorSearch/Index.vala` â†’ `libocvector/Index.vala`
   - Keep namespace as `Faiss` (or appropriate FAISS namespace)
   - Keep FAISS functionality intact

2. **Migrate Database**:
   - Move `VectorSearch/Database.vala` â†’ `libocvector/Database.vala`
   - Update namespace to `OLLMvector`
   - Integrate with `OLLMvector.Files.File` for metadata tracking

### Phase 3: Refactor Analysis Layer
1. **Create Analysis Component**:
   - Extract analysis logic from `VectorSearch/CodeImport.vala`
   - Create `libocvector/Indexing/Analysis.vala`
   - Implement LLM-based analysis: send source files to LLM, receive JSON output
   - Process JSON response to extract semantic elements
   - Integrate with `OLLMvector.Files` for file tracking
   - *Note: Documentation handling (markdown, downloaded content) to be determined*

2. **Create Vector Builder**:
   - Extract vector generation from `VectorSearch/Database.vala`
   - Create `libocvector/Indexing/VectorBuilder.vala`
   - Separate embedding generation from storage
   - Add batch processing support

3. **Create Indexer**:
   - Refactor `VectorSearch/CodeIndexer.vala` â†’ `libocvector/Indexing/Indexer.vala`
   - Combine Analysis and VectorBuilder
   - Add integration with `OLLMvector.Files` for file tracking
   - Implement incremental update logic

### Phase 4: Refactor Search Layer
1. **Create Searcher**:
   - Refactor `VectorSearch/CodeSearch.vala` â†’ `libocvector/Search/Searcher.vala`
   - Add GPU support detection and usage
   - Enhance result formatting

2. **Create Query Processor**:
   - Extract query processing logic
   - Create `libocvector/Search/QueryProcessor.vala`
   - Add query type detection and routing

### Phase 5: Copy FileBase Structure
1. **Copy FileBase structure from liboccoder**:
   - Copy `liboccoder/Files/FileBase.vala` â†’ `libocvector/Files/FileBase.vala`
   - Copy `liboccoder/Files/File.vala` â†’ `libocvector/Files/File.vala`
   - Copy `liboccoder/Files/Folder.vala` â†’ `libocvector/Files/Folder.vala`
   - Update namespaces from `OLLMcoder.Files` to `OLLMvector.Files`
   - Update database table names to avoid conflicts (e.g., `vector_filebase` instead of `filebase`)
   - Remove ProjectManager dependency (use simpler manager or remove if not needed)
   - Add vector indexing metadata fields to `OLLMvector.Files.File`
   - Store vector position and indexing status in database

### Phase 6: Create Command-Line Tools
1. **Create Indexing Example**:
   - Create `examples/oc-vector-index.vala`
   - Simple command-line tool for testing single file indexing
   - Focus on development/testing use cases

2. **Create Search Tool**:
   - Create `libocvector/Tools/SearchTool.vala`
   - Implement command-line interface
   - Add to `examples/` directory as `oc-vector-search.vala`
   - Create `oc-vector-search` executable

### Phase 7: Create OLLM Tool Integration
1. **Create CodebaseSearchTool**:
   - Create `libocvector/Tool/CodebaseSearchTool.vala`
   - Extend `OLLMchat.Tool.Interface`
   - Implement tool interface methods
   - Add to `libollmchat` tool registry

### Phase 8: Update Dependencies and Build
1. Update `meson.build`:
   - Add `libocvector` subdirectory
   - Add dependencies (libocsqlite, libollmchat, FAISS)
   - Note: liboccoder is NOT a dependency (FileBase structure is copied)
   - Create library, VAPI, and GIR files

2. Update library dependencies:
   - Add `libocvector` to libraries that need it
   - Update VAPI dependencies

### Phase 9: Testing and Cleanup
1. Test indexing with various codebases
2. Test search functionality
3. Test command-line tools
4. Test OLLM tool integration
5. Remove or deprecate `VectorSearch/` directory
6. Update documentation

## Existing Infrastructure

The project already has a `VectorSearch/` directory with:
- **FAISS Integration** (`Index.vala`, `Database.vala`) - Vector search using FAISS
- **Code Analysis** (`CodeImport.vala`, `CodeIndexer.vala`) - Code file analysis
- **Code Search** (`CodeSearch.vala`) - Search functionality
- **Code Elements** (`CodeElement.vala`, `CodeFile.vala`) - Code structure representation

This code will be migrated and refactored into the new `libocvector` structure.

## Dependencies

- **FAISS library** (already integrated via `vapi/fiass.vapi`)
- **libocsqlite** - For metadata storage
- **libollmchat** - For LLM client access and tool integration
- **libocagent** - For agent integration (if needed)

**Note**: `libocvector` does not depend on `liboccoder`. Instead, it includes its own copy of the FileBase structure (`OLLMvector.Files`) which is based on `liboccoder.Files` but exists independently to avoid dependency issues.

## Files to Create

### Library Files
- `libocvector/meson.build`
- `libocvector/Files/FileBase.vala` (copied from liboccoder, namespace: OLLMvector.Files)
- `libocvector/Files/File.vala` (copied from liboccoder, namespace: OLLMvector.Files)
- `libocvector/Files/Folder.vala` (copied from liboccoder, namespace: OLLMvector.Files)
- `libocvector/Index.vala` (FAISS namespace)
- `libocvector/Database.vala`
- `libocvector/Indexing/Analysis.vala`
- `libocvector/Indexing/VectorBuilder.vala`
- `libocvector/Indexing/Indexer.vala`
- `libocvector/Search/Searcher.vala`
- `libocvector/Search/QueryProcessor.vala`
- `libocvector/Tool/CodebaseSearchTool.vala`
- `libocvector/Tools/SearchTool.vala`

### Example/Test Tools
- `examples/oc-vector-index.vala` (simple single-file indexing for testing)
- `examples/oc-vector-search.vala` (uses `libocvector/Tools/SearchTool.vala`)

## Features

- Semantic code search using FAISS vectors
- Support for both code and documentation indexing
- GPU-accelerated search (with CPU fallback)
- Incremental indexing with change detection
- Independent file tracking system (FileBase structure copied from liboccoder)
- Command-line tools for testing and manual operations
- OLLM library tool integration for LLM agents
- Permission system integration

## Testing

- Test indexing with various codebases and languages
- Test search functionality with different query types
- Test file change detection and re-indexing
- Test command-line tools
- Test OLLM tool integration
- Test GPU vs CPU search performance
- Test incremental updates
- Add to meson.build
